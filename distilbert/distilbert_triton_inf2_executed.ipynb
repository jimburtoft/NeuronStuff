{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Inference on AWS Inferentia2 with Triton Inference Server\n",
    "\n",
    "This notebook deploys DistilBERT (`distilbert-base-uncased`) on an AWS **inf2.xlarge** instance using\n",
    "NVIDIA Triton Inference Server with the AWS Neuron SDK, then benchmarks throughput and latency.\n",
    "\n",
    "Everything is self-contained: the notebook writes all required files (Dockerfile, Triton config,\n",
    "Python backend, compile script, benchmark client), builds the Docker image, compiles the models,\n",
    "starts the server, and runs the benchmark.\n",
    "\n",
    "**Instance**: inf2.xlarge (1 Neuron device, 2 NeuronCores, NeuronCore V2)  \n",
    "**Time to complete**: ~35 minutes (Docker build dominates)  \n",
    "**Prerequisites**: Deep Learning AMI Neuron (Ubuntu 24.04), Docker installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We measure throughput (inferences/second) and latency (P50, P95, P99) under multiple load patterns,\n",
    "varying both client-side batch size and request concurrency to exercise Triton's dynamic batching.\n",
    "\n",
    "- **Input**: Fixed \"hello world\" text, tokenized and padded to 128 tokens\n",
    "- **Output**: 768-dimensional CLS token embedding (FP32 output, BF16 internal matmult via `--auto-cast matmult`)\n",
    "- **Duration**: 10 seconds per test configuration\n",
    "- **Warmup**: 5 requests before timing\n",
    "- **Concurrency**: Python threads, one HTTP client per thread\n",
    "\n",
    "### Inferentia2 vs Trainium2\n",
    "\n",
    "| Property | inf2.xlarge | trn2.3xlarge (LNC=2) |\n",
    "|----------|------------|---------------------|\n",
    "| NeuronCore generation | V2 | V3 |\n",
    "| NeuronCores | **2** | 4 logical (8 physical) |\n",
    "| HBM per core | 16 GB | 24 GB |\n",
    "| vCPUs / RAM | 4 / 16 GB | 12 / 128 GB |\n",
    "| LNC support | No | Yes |\n",
    "| FP8 support | No | Yes |\n",
    "\n",
    "### Neuron-Specific: Per-Batch-Size Compilation\n",
    "\n",
    "Unlike GPUs, Neuron models are compiled for a **fixed batch size**. Running a BS=1 request through\n",
    "a BS=16 compiled model wastes 15/16 of the compute. We compile separate models for BS=1, 2, 4, 8, 16\n",
    "and the Triton Python backend dispatches each request to the smallest model that fits.\n",
    "\n",
    "**Note**: Models compiled for Inferentia2 are **not interchangeable** with Trainium2 models.\n",
    "The underlying NeuronCore hardware differs (V2 vs V3).\n",
    "\n",
    "### Triton Configuration\n",
    "\n",
    "- **2 model instances** (one per NeuronCore), each pinned via `NEURON_RT_VISIBLE_CORES`\n",
    "- **Dynamic batching**: preferred sizes [4, 8, 16], max queue delay 5ms\n",
    "- **Python backend**: Triton has no native Neuron backend, so we use the Python backend with `torch_neuronx`\n",
    "\n",
    "### Test Matrix\n",
    "\n",
    "| Test | Concurrency | Client Batch Sizes |\n",
    "|------|-------------|--------------------|\n",
    "| Baseline | 1 (no batching) | 1 |\n",
    "| Dynamic batching | 4, 8, 16 | 1, 2, 4, 8, 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Instance Specifications\n",
    "\n",
    "| Spec | Value |\n",
    "|------|-------|\n",
    "| Model | DistilBERT base uncased (67M params) |\n",
    "| Architecture | 6-layer transformer, hidden size 768 |\n",
    "| Input | 128 tokens (padded) |\n",
    "| Output | 768-dim CLS embedding (FP32 output, BF16 matmult) |\n",
    "| Compiled model size | ~148 MB per batch size variant |\n",
    "| Instance type | inf2.xlarge |\n",
    "| Neuron device | 1 device, 2 NeuronCores (V2) |\n",
    "| HBM per core | 16 GB (32 GB total) |\n",
    "| vCPUs / RAM | 4 / 16 GB |\n",
    "\n",
    "### Software Stack\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|--------|\n",
    "| Neuron SDK | 2.27.1 |\n",
    "| PyTorch / torch-neuronx | 2.9.0 / 2.9.0.2.11 |\n",
    "| neuronx-cc (compiler) | 2.22.12471 |\n",
    "| Transformers | 4.48.0 |\n",
    "| Triton Inference Server | 2.65.0 (r26.01, built from source) |\n",
    "| Python | 3.12 |\n",
    "| OS / AMI | Ubuntu 24.04 / Deep Learning AMI Neuron 20260126 |\n",
    "\n",
    "> **Important**: Transformers versions 4.54.0+ have a confirmed 31% performance regression for\n",
    "> DistilBERT on Neuron. Use versions 4.48.0 through 4.53.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Setup & Environment Check\n",
    "\n",
    "After you deploy the instance using the Ubuntu Neuron Deep Learning AMI (with all the Neuron drivers installed), run this notebook inside the pre-installed PyTorch 2.9 Neuron virtual environment:\n",
    "\n",
    "```bash\n",
    "source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser\n",
    "```\n",
    "\n",
    "Alternatively, if you are running from withing a remote vscode instance, you can use ```ln -s /opt/aws_neuronx_venv_pytorch_2_9/bin/activate ~/.venv``` to help vscode find your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:36:21.527232Z",
     "iopub.status.busy": "2026-02-24T23:36:21.527068Z",
     "iopub.status.idle": "2026-02-24T23:36:28.650260Z",
     "shell.execute_reply": "2026-02-24T23:36:28.649438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu128\n",
      "torch-neuronx: 2.9.0.2.11.19912+e48cd891\n",
      "transformers: 4.48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instance-type: inf2.xlarge\n",
      "instance-id: i-008658b9f6f8a6768\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 2      | 0-1      | 32 GB  | 0000:00:1f.0 | 0-3      | -1   |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os, time, shutil\n",
    "\n",
    "# Verify Neuron environment\n",
    "import torch, torch_neuronx\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'torch-neuronx: {torch_neuronx.__version__}')\n",
    "\n",
    "# Ensure correct transformers version\n",
    "try:\n",
    "    import transformers\n",
    "    ver = transformers.__version__\n",
    "    print(f'transformers: {ver}')\n",
    "    if ver >= '4.54.0':\n",
    "        print('WARNING: transformers >= 4.54.0 has 31% regression. Downgrading...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                               'transformers==4.48.0', '-q'])\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           'transformers==4.48.0', '-q'])\n",
    "\n",
    "# Install Triton client\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                       'tritonclient[http]', '-q'])\n",
    "\n",
    "# Show Neuron devices\n",
    "r = subprocess.run(['neuron-ls'], capture_output=True, text=True)\n",
    "print(f'\\n{r.stdout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Compile DistilBERT for All Batch Sizes\n",
    "\n",
    "Compile 5 model variants (BS=1, 2, 4, 8, 16) with sequence length 128 for Inferentia2.\n",
    "Takes ~5 minutes per variant (~25 min total--time may vary for other models). Skips already-compiled models, so if you are running this multiple times you will see faster compilation.  For production, you would deploy with the pre-compiled models.\n",
    "\n",
    "**Note**: No `--lnc` flag is used. LNC is a Trainium2-only feature. Inferentia2 has fixed\n",
    "1:1 physical-to-logical core mapping.\n",
    "\n",
    "The `--auto-cast matmult` flag casts matrix multiplications to BF16, yielding ~2.8x throughput\n",
    "with negligible accuracy loss (cosine similarity > 0.99999 vs FP32). This matches the\n",
    "compilation settings used in the [AWS Neuron SDK benchmarks](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/about-neuron/benchmarks/inf2/inf2-performance.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:36:28.652415Z",
     "iopub.status.busy": "2026-02-24T23:36:28.652084Z",
     "iopub.status.idle": "2026-02-24T23:38:14.302043Z",
     "shell.execute_reply": "2026-02-24T23:38:14.301170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling DistilBERT for batch sizes: [1, 2, 4, 8, 16]\n",
      "Sequence length: 128, Target: Inferentia2 (no LNC)\n",
      "\n",
      "  Compiling BS=1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs1.pt (148.1 MB)\n",
      "  Compiling BS=2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs2.pt (148.1 MB)\n",
      "  Compiling BS=4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs4.pt (148.2 MB)\n",
      "  Compiling BS=8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs8.pt (148.4 MB)\n",
      "  Compiling BS=16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs16.pt (149.1 MB)\n",
      "\n",
      "Compiled models:\n",
      "  model_bs1.pt: 148.1 MB\n",
      "  model_bs16.pt: 149.1 MB\n",
      "  model_bs2.pt: 148.1 MB\n",
      "  model_bs4.pt: 148.2 MB\n",
      "  model_bs8.pt: 148.4 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "os.environ['NEURON_RT_LOG_LEVEL'] = 'ERROR'\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "MODEL_DIR = os.path.expanduser('~/triton_repo/distilbert/1')\n",
    "BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "print(f'Compiling DistilBERT for batch sizes: {BATCH_SIZES}')\n",
    "print(f'Sequence length: {SEQ_LENGTH}, Target: Inferentia2 (no LNC)\\n')\n",
    "\n",
    "for bs in BATCH_SIZES:\n",
    "    output_path = os.path.join(MODEL_DIR, f'model_bs{bs}.pt')\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'  BS={bs}: already compiled, skipping')\n",
    "        continue\n",
    "    print(f'  Compiling BS={bs}...')\n",
    "    texts = ['Test sentence.'] * bs\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=SEQ_LENGTH,\n",
    "                       padding='max_length', truncation=True)\n",
    "    model_neuron = torch_neuronx.trace(\n",
    "        model, (inputs['input_ids'], inputs['attention_mask']),\n",
    "        compiler_args=['--model-type', 'transformer', '--optlevel', '2',\n",
    "                       '--auto-cast', 'matmult'])\n",
    "    torch.jit.save(model_neuron, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f'    Saved: {output_path} ({size_mb:.1f} MB)')\n",
    "\n",
    "print('\\nCompiled models:')\n",
    "for f in sorted(os.listdir(MODEL_DIR)):\n",
    "    if f.endswith('.pt'):\n",
    "        size = os.path.getsize(os.path.join(MODEL_DIR, f)) / (1024 * 1024)\n",
    "        print(f'  {f}: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Write Triton Model Repository Files\n",
    "\n",
    "Write `config.pbtxt` and `model.py` into the model repository alongside the compiled `.pt` files.\n",
    "\n",
    "Key differences from Trainium2:\n",
    "- `instance_group count: 2` (2 NeuronCores on inf2.xlarge)\n",
    "- No `NEURON_LOGICAL_NC_CONFIG` environment variable\n",
    "- Core IDs 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:38:14.304067Z",
     "iopub.status.busy": "2026-02-24T23:38:14.303673Z",
     "iopub.status.idle": "2026-02-24T23:38:14.311076Z",
     "shell.execute_reply": "2026-02-24T23:38:14.310401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config.pbtxt (instance_group count=2 for inf2.xlarge)\n",
      "Wrote model.py (cores 0-1, no LNC)\n",
      "\n",
      "Model repository:\n",
      "triton_repo/\n",
      "  distilbert/\n",
      "    config.pbtxt\n",
      "    1/\n",
      "      model.py\n",
      "      model_bs1.pt (148.1 MB)\n",
      "      model_bs16.pt (149.1 MB)\n",
      "      model_bs2.pt (148.1 MB)\n",
      "      model_bs4.pt (148.2 MB)\n",
      "      model_bs8.pt (148.4 MB)\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = os.path.expanduser('~/triton_repo/distilbert')\n",
    "\n",
    "# ── config.pbtxt (inf2.xlarge: 2 instances) ──────────────────────────────────\n",
    "config_pbtxt = r\"\"\"name: \"distilbert\"\n",
    "platform: \"python\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"embeddings\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 5000\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"model_dir\"\n",
    "  value: { string_value: \"/models/distilbert/1\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"tokenizer_name\"\n",
    "  value: { string_value: \"distilbert-base-uncased\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"max_seq_length\"\n",
    "  value: { string_value: \"128\" }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(REPO_DIR, 'config.pbtxt'), 'w') as f:\n",
    "    f.write(config_pbtxt)\n",
    "print('Wrote config.pbtxt (instance_group count=2 for inf2.xlarge)')\n",
    "\n",
    "# ── model.py (Triton Python backend, Inferentia2) ─────────────────────────────\n",
    "model_py = r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Triton Python Backend for DistilBERT on AWS Neuron (Inferentia2)\n",
    "\n",
    "Loads compiled models for each batch size (1, 2, 4, 8, 16) and\n",
    "dispatches to the best-fit model to avoid padding waste.\n",
    "Each Triton instance is pinned to a separate Neuron core (cores 0-1).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import triton_python_backend_utils as pb_utils\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\"model_config\"])\n",
    "        params = self.model_config.get(\"parameters\", {})\n",
    "        model_dir = params.get(\"model_dir\", {}).get(\n",
    "            \"string_value\", \"/models/distilbert/1\")\n",
    "        tokenizer_name = params.get(\"tokenizer_name\", {}).get(\n",
    "            \"string_value\", \"distilbert-base-uncased\")\n",
    "        self.max_seq_length = int(\n",
    "            params.get(\"max_seq_length\", {}).get(\"string_value\", \"128\"))\n",
    "\n",
    "        # Pin this instance to a specific Neuron core (0-1 for inf2.xlarge)\n",
    "        instance_name = args.get(\"model_instance_name\", \"distilbert_0_0\")\n",
    "        core_id = int(instance_name.split(\"_\")[-1])\n",
    "        os.environ[\"NEURON_RT_VISIBLE_CORES\"] = str(core_id)\n",
    "        print(f\"Instance {instance_name}: pinned to Neuron core {core_id}\")\n",
    "\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n",
    "        print(\"Tokenizer loaded\")\n",
    "\n",
    "        # Load all compiled models\n",
    "        self.models = {}\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            model_path = os.path.join(model_dir, f\"model_bs{bs}.pt\")\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[bs] = torch.jit.load(model_path)\n",
    "                self.models[bs].eval()\n",
    "                print(f\"  Loaded model for batch_size={bs}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Model not found: {model_path}\")\n",
    "\n",
    "        if not self.models:\n",
    "            raise RuntimeError(\"No compiled models found!\")\n",
    "\n",
    "        # Warmup all models\n",
    "        print(\"Warming up models...\")\n",
    "        for bs, mdl in self.models.items():\n",
    "            dummy_ids = torch.zeros((bs, self.max_seq_length), dtype=torch.long)\n",
    "            dummy_mask = torch.ones((bs, self.max_seq_length), dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = mdl(dummy_ids, dummy_mask)\n",
    "            print(f\"  Warmed up BS={bs}\")\n",
    "        print(f\"Model initialization complete! \"\n",
    "              f\"Available batch sizes: {sorted(self.models.keys())}\")\n",
    "\n",
    "    def _get_best_model(self, actual_batch_size):\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            if bs >= actual_batch_size and bs in self.models:\n",
    "                return bs, self.models[bs]\n",
    "        largest = max(self.models.keys())\n",
    "        return largest, self.models[largest]\n",
    "\n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        for request in requests:\n",
    "            input_ids = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            ).long()\n",
    "            attention_mask = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            ).long()\n",
    "\n",
    "            actual_bs = input_ids.shape[0]\n",
    "            target_bs, model = self._get_best_model(actual_bs)\n",
    "\n",
    "            # Pad to compiled batch size if needed\n",
    "            if actual_bs < target_bs:\n",
    "                pad = target_bs - actual_bs\n",
    "                input_ids = torch.cat([input_ids,\n",
    "                    torch.zeros((pad, input_ids.shape[1]), dtype=torch.long)], dim=0)\n",
    "                attention_mask = torch.cat([attention_mask,\n",
    "                    torch.zeros((pad, attention_mask.shape[1]), dtype=torch.long)], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            embeddings = outputs[\"last_hidden_state\"][:, 0, :].cpu().numpy()\n",
    "            embeddings = embeddings[:actual_bs, :]\n",
    "\n",
    "            output_tensor = pb_utils.Tensor(\"embeddings\",\n",
    "                                            embeddings.astype(np.float32))\n",
    "            responses.append(\n",
    "                pb_utils.InferenceResponse(output_tensors=[output_tensor]))\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(\"Finalizing DistilBERT model...\")\n",
    "        if hasattr(self, \"models\"):\n",
    "            for bs, mdl in self.models.items():\n",
    "                del mdl\n",
    "            self.models.clear()\n",
    "'''\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model.py'), 'w') as f:\n",
    "    f.write(model_py)\n",
    "print('Wrote model.py (cores 0-1, no LNC)')\n",
    "\n",
    "# Verify\n",
    "print('\\nModel repository:')\n",
    "for root, dirs, files in os.walk(os.path.expanduser('~/triton_repo')):\n",
    "    level = root.replace(os.path.expanduser('~/triton_repo'), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, f)) / (1024 * 1024)\n",
    "        label = f'{f} ({size:.1f} MB)' if size > 1 else f\n",
    "        print(f'{indent}  {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Build Triton + Neuron Docker Image\n",
    "\n",
    "Triton has no native Neuron backend, so we build Triton from source inside the AWS Neuron\n",
    "PyTorch inference base image. This takes **15-25 minutes** and produces a ~15.8 GB image.\n",
    "\n",
    "**Note**: On inf2.xlarge (4 vCPUs, 16 GB RAM), the build is slower than on larger instances.\n",
    "If the build fails due to memory, use an inf2.8xlarge for the build step, save the image\n",
    "with `docker save`, transfer it, and load with `docker load`.\n",
    "\n",
    "The cell skips the build if the image already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:38:14.312721Z",
     "iopub.status.busy": "2026-02-24T23:38:14.312541Z",
     "iopub.status.idle": "2026-02-24T23:38:14.463898Z",
     "shell.execute_reply": "2026-02-24T23:38:14.462948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker image already exists: 9ce9c9a1d000\n",
      "Delete with: docker rmi triton-neuron-distilbert:latest\n",
      "IMAGE                             ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
      "triton-neuron-distilbert:latest   9ce9c9a1d000       24.1GB         7.82GB        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOCKER_IMAGE = 'triton-neuron-distilbert:latest'\n",
    "\n",
    "# Check if already built\n",
    "r = subprocess.run(['docker', 'images', '-q', DOCKER_IMAGE],\n",
    "                   capture_output=True, text=True)\n",
    "if r.stdout.strip():\n",
    "    print(f'Docker image already exists: {r.stdout.strip()}')\n",
    "    print('Delete with: docker rmi ' + DOCKER_IMAGE)\n",
    "else:\n",
    "    # Write Dockerfile\n",
    "    dockerfile = r\"\"\"ARG BASE_IMAGE=public.ecr.aws/neuron/pytorch-inference-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04\n",
    "FROM $BASE_IMAGE\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PJRT_DEVICE=NEURON \\\n",
    "    LD_LIBRARY_PATH=\"/opt/conda/lib:/opt/aws/neuron/lib:/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}\" \\\n",
    "    PATH=\"/opt/program:/opt/aws/neuron/bin:/opt/tritonserver/bin:${PATH}\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    wget gnupg2 build-essential git nginx pkg-config unzip \\\n",
    "    libssl-dev libcurl4-openssl-dev libgoogle-perftools-dev \\\n",
    "    libnuma-dev libarchive-dev libxml2-dev zlib1g-dev \\\n",
    "    autoconf automake libtool gperf scons patchelf \\\n",
    "    libre2-dev libb64-dev rapidjson-dev libboost-dev \\\n",
    "    cmake cmake-data \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel virtualenv build cmake==3.31.10\n",
    "RUN pip3 install transformers==4.48.0\n",
    "\n",
    "RUN git clone --depth=1 --branch=r26.01 https://github.com/triton-inference-server/server.git /server && \\\n",
    "    cd /server && \\\n",
    "    Python3_ROOT_DIR=/opt/conda \\\n",
    "    Python3_EXECUTABLE=/opt/conda/bin/python3 \\\n",
    "    Python3_INCLUDE_DIR=/opt/conda/include/python3.12 \\\n",
    "    Python3_LIBRARY=/opt/conda/lib/libpython3.12.so \\\n",
    "    ./build.py -v --no-container-build --build-dir=/server/build --backend=python \\\n",
    "    --enable-metrics --enable-logging --enable-stats --endpoint=\"http\" --endpoint=\"grpc\" && \\\n",
    "    cp -r /server/build/opt/* /opt/ && \\\n",
    "    cd / && rm -rf /server\n",
    "\n",
    "EXPOSE 8000 8001 8002\n",
    "CMD [\"tritonserver\", \"--model-repository=/models\"]\n",
    "\"\"\"\n",
    "    dockerfile_path = os.path.expanduser('~/Dockerfile.triton-neuron')\n",
    "    with open(dockerfile_path, 'w') as f:\n",
    "        f.write(dockerfile)\n",
    "\n",
    "    print('Building Triton + Neuron Docker image (15-25 minutes)...')\n",
    "    r = subprocess.run(\n",
    "        ['docker', 'build', '-f', dockerfile_path, '-t', DOCKER_IMAGE, '.'],\n",
    "        cwd=os.path.expanduser('~'),\n",
    "        capture_output=True, text=True, timeout=3000)\n",
    "    if r.returncode == 0:\n",
    "        print('Build complete!')\n",
    "    else:\n",
    "        print(f'Build FAILED (rc={r.returncode})')\n",
    "        print(r.stderr[-3000:])\n",
    "\n",
    "# Show image\n",
    "r = subprocess.run(['docker', 'images', DOCKER_IMAGE], capture_output=True, text=True)\n",
    "print(r.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Start Triton Server\n",
    "\n",
    "Launch the Docker container with the Neuron device mounted and the model repository bind-mounted.\n",
    "With 2 instances on inf2.xlarge, model loading takes **5-8 minutes** (each instance loads 5 compiled\\n\",\n",
    "     \"models sequentially into NeuronCore V2 memory). The cell polls the health endpoint and waits\\n\",\n",
    "     \"up to 10 minutes for the server to become ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:38:14.465719Z",
     "iopub.status.busy": "2026-02-24T23:38:14.465514Z",
     "iopub.status.idle": "2026-02-24T23:39:02.771406Z",
     "shell.execute_reply": "2026-02-24T23:39:02.770715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container started. Waiting for 2 model instances to load (~5-8 min on inf2.xlarge)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server ready after ~42s\n",
      "Model instances initialized: 2/2\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "NUM_INSTANCES = 2  # inf2.xlarge: 2 NeuronCores\n",
    "\n",
    "# Stop any previous run\n",
    "subprocess.run(['docker', 'rm', '-f', 'triton-distilbert'],\n",
    "               capture_output=True)\n",
    "time.sleep(3)\n",
    "\n",
    "cmd = [\n",
    "    'docker', 'run', '-d',\n",
    "    '--name', 'triton-distilbert',\n",
    "    '--device=/dev/neuron0',\n",
    "    '-v', os.path.expanduser('~/triton_repo') + ':/models:ro',\n",
    "    '-p', '8000:8000', '-p', '8001:8001', '-p', '8002:8002',\n",
    "    DOCKER_IMAGE,\n",
    "    'tritonserver', '--model-repository=/models', '--log-verbose=0',\n",
    "]\n",
    "r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "if r.returncode != 0:\n",
    "    print(f'Failed to start container: {r.stderr}')\n",
    "else:\n",
    "    print(f'Container started. Waiting for {NUM_INSTANCES} model instances to load (~5-8 min on inf2.xlarge)...')\n",
    "\n",
    "# Poll for readiness\n",
    "for i in range(300):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen('http://localhost:8000/v2/health/ready', timeout=2)\n",
    "        if resp.status == 200:\n",
    "            elapsed = (i + 1) * 2\n",
    "            print(f'Server ready after ~{elapsed}s')\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('Timeout waiting for server (10 min)!')\n",
    "    r = subprocess.run(['docker', 'logs', '--tail', '30', 'triton-distilbert'],\n",
    "                       capture_output=True, text=True)\n",
    "    print(r.stdout)\n",
    "\n",
    "# Verify instances loaded\n",
    "r = subprocess.run(['docker', 'logs', 'triton-distilbert'],\n",
    "                   capture_output=True, text=True)\n",
    "n = r.stdout.count('Model initialization complete')\n",
    "print(f'Model instances initialized: {n}/{NUM_INSTANCES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Run Benchmark\n",
    "\n",
    "Runs the full test matrix: 1 baseline + 15 dynamic-batching configurations (3 concurrency levels\n",
    "x 5 batch sizes), 10 seconds each. Total runtime ~3 minutes.\n",
    "\n",
    "Concurrency levels are 4/8/16 (scaled for 2 cores: 2 workers/core = 4 baseline).\n",
    "\n",
    "**Note**: One worker per test will print a harmless greenlet thread-switch error. This is a\n",
    "known cosmetic issue in `tritonclient` and does not affect results -- the remaining workers run fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:39:02.773387Z",
     "iopub.status.busy": "2026-02-24T23:39:02.773225Z",
     "iopub.status.idle": "2026-02-24T23:41:43.902890Z",
     "shell.execute_reply": "2026-02-24T23:41:43.902119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTILBERT TRITON BENCHMARK - Neuron (inf2.xlarge)\n",
      "==========================================================================================\n",
      "\n",
      "Baseline: single request, no concurrency...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  P50: 6.49ms  Throughput: 153 inf/sec\n",
      "\n",
      "Concurrency=4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   7.97ms  P95=   8.38ms  P99=   8.56ms  Throughput=     376 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   4.23ms  P95=   5.26ms  P99=   5.39ms  Throughput=    1496 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=   2.99ms  P95=   3.28ms  P99=   3.42ms  Throughput=    3996 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=   5.37ms  P95=   6.41ms  P99=   6.52ms  Throughput=    4498 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=   9.65ms  P95=  11.61ms  P99=  11.86ms  Throughput=    4986 inf/sec\n",
      "\n",
      "Concurrency=8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   6.85ms  P95=   7.56ms  P99=   7.78ms  Throughput=    1117 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   4.48ms  P95=   4.90ms  P99=   5.08ms  Throughput=    3280 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=   7.33ms  P95=   7.94ms  P99=   8.08ms  Throughput=    4056 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  13.08ms  P95=  14.25ms  P99=  14.54ms  Throughput=    4512 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  22.66ms  P95=  24.61ms  P99=  24.93ms  Throughput=    4935 inf/sec\n",
      "\n",
      "Concurrency=16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   6.71ms  P95=   7.59ms  P99=   8.09ms  Throughput=    2264 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   9.31ms  P95=  10.90ms  P99=  14.23ms  Throughput=    3289 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  15.16ms  P95=  16.69ms  P99=  19.06ms  Throughput=    4043 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  27.92ms  P95=  28.75ms  P99=  29.06ms  Throughput=    4484 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  49.26ms  P95=  52.89ms  P99=  53.89ms  Throughput=    4844 inf/sec\n",
      "\n",
      "==========================================================================================\n",
      "Batch   Workers   Requests    P50 (ms)    P95 (ms)    P99 (ms)    Throughput     \n",
      "------------------------------------------------------------------------------------------\n",
      "1       1         1528        6.49        6.95        7.01        153            \n",
      "1       4         3759        7.97        8.38        8.56        376            \n",
      "2       4         7484        4.23        5.26        5.39        1496           \n",
      "4       4         9996        2.99        3.28        3.42        3996           \n",
      "8       4         5627        5.37        6.41        6.52        4498           \n",
      "16      4         3120        9.65        11.61       11.86       4986           \n",
      "1       8         11181       6.85        7.56        7.78        1117           \n",
      "2       8         16424       4.48        4.90        5.08        3280           \n",
      "4       8         10149       7.33        7.94        8.08        4056           \n",
      "8       8         5648        13.08       14.25       14.54       4512           \n",
      "16      8         3092        22.66       24.61       24.93       4935           \n",
      "1       16        22683       6.71        7.59        8.09        2264           \n",
      "2       16        16486       9.31        10.90       14.23       3289           \n",
      "4       16        10136       15.16       16.69       19.06       4043           \n",
      "8       16        5625        27.92       28.75       29.06       4484           \n",
      "16      16        3046        49.26       52.89       53.89       4844           \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'distilbert'\n",
    "DURATION = 10.0  # seconds per test\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def _worker(client, batch_size, duration_s, latencies_q, stop_evt):\n",
    "    sentences = ['hello world'] * batch_size\n",
    "    tokens = tok(sentences, max_length=128, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    t_end = time.time() + duration_s\n",
    "    while time.time() < t_end and not stop_evt.is_set():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "            latencies_q.put((time.time() - t0) * 1000)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "\n",
    "def run_concurrent(batch_size, num_workers, duration_s=DURATION):\n",
    "    clients = [httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "               for _ in range(num_workers)]\n",
    "    # Warmup\n",
    "    tokens = tok(['hello world'] * batch_size, max_length=128,\n",
    "                 padding='max_length', truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(5):\n",
    "        clients[0].infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "\n",
    "    q = Queue()\n",
    "    stop = threading.Event()\n",
    "    threads = []\n",
    "    t_start = time.time()\n",
    "    for i in range(num_workers):\n",
    "        t = threading.Thread(target=_worker,\n",
    "                             args=(clients[i], batch_size, duration_s, q, stop))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    total_time = time.time() - t_start\n",
    "\n",
    "    latencies = []\n",
    "    while not q.empty():\n",
    "        latencies.append(q.get())\n",
    "    if not latencies:\n",
    "        return None\n",
    "    return {\n",
    "        'batch_size': batch_size, 'num_workers': num_workers,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': (len(latencies) * batch_size) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_baseline(duration_s=DURATION):\n",
    "    client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    tokens = tok(['hello world'], max_length=128, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(10):\n",
    "        client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "    latencies = []\n",
    "    t_start = time.time()\n",
    "    while time.time() - t_start < duration_s:\n",
    "        t0 = time.time()\n",
    "        client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "        latencies.append((time.time() - t0) * 1000)\n",
    "    total_time = time.time() - t_start\n",
    "    return {\n",
    "        'batch_size': 1, 'num_workers': 1,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': len(latencies) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Run all tests ─────────────────────────────────────────────────────────────\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "assert client.is_server_ready(), 'Triton server not ready!'\n",
    "\n",
    "print('DISTILBERT TRITON BENCHMARK - Neuron (inf2.xlarge)')\n",
    "print('=' * 90)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline\n",
    "print('\\nBaseline: single request, no concurrency...')\n",
    "r = run_baseline()\n",
    "results.append(r)\n",
    "print(f'  P50: {r[\"p50\"]:.2f}ms  Throughput: {r[\"throughput\"]:.0f} inf/sec')\n",
    "\n",
    "# Dynamic batching -- concurrency levels scaled for 2 cores\n",
    "for conc in [4, 8, 16]:\n",
    "    print(f'\\nConcurrency={conc}:')\n",
    "    for bs in [1, 2, 4, 8, 16]:\n",
    "        r = run_concurrent(bs, conc)\n",
    "        if r:\n",
    "            results.append(r)\n",
    "            print(f'  BS={bs:<3} P50={r[\"p50\"]:>7.2f}ms  '\n",
    "                  f'P95={r[\"p95\"]:>7.2f}ms  '\n",
    "                  f'P99={r[\"p99\"]:>7.2f}ms  '\n",
    "                  f'Throughput={r[\"throughput\"]:>8.0f} inf/sec')\n",
    "\n",
    "# Summary table\n",
    "print(f'\\n{\"=\" * 90}')\n",
    "print(f'{\"Batch\":<8}{\"Workers\":<10}{\"Requests\":<12}'\n",
    "      f'{\"P50 (ms)\":<12}{\"P95 (ms)\":<12}{\"P99 (ms)\":<12}{\"Throughput\":<15}')\n",
    "print('-' * 90)\n",
    "for r in results:\n",
    "    print(f'{r[\"batch_size\"]:<8}{r[\"num_workers\"]:<10}{r[\"total_requests\"]:<12}'\n",
    "          f'{r[\"p50\"]:<12.2f}{r[\"p95\"]:<12.2f}{r[\"p99\"]:<12.2f}'\n",
    "          f'{r[\"throughput\"]:<15.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:41:43.904647Z",
     "iopub.status.busy": "2026-02-24T23:41:43.904477Z",
     "iopub.status.idle": "2026-02-24T23:41:44.393566Z",
     "shell.execute_reply": "2026-02-24T23:41:44.392589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton server stopped and removed.\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(['docker', 'rm', '-f', 'triton-distilbert'], capture_output=True)\n",
    "print('Triton server stopped and removed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (aws_neuronx_venv_pytorch_2_9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
