{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Inference on AWS Trainium2 with Triton Inference Server (LNC=1)\n",
    "\n",
    "This notebook deploys DistilBERT (`distilbert-base-uncased`) on an AWS **trn2.3xlarge** instance using\n",
    "NVIDIA Triton Inference Server with the AWS Neuron SDK, then benchmarks throughput and latency.\n",
    "\n",
    "**This is the LNC=1 variant.** With LNC=1, each physical NeuronCore becomes one logical core,\n",
    "giving **8 logical cores** (vs 4 with LNC=2). Each core has half the memory (12 GB vs 24 GB) but\n",
    "there are twice as many independent execution units.\n",
    "\n",
    "Everything is self-contained: the notebook writes all required files (Dockerfile, Triton config,\n",
    "Python backend, compile script, benchmark client), builds the Docker image, compiles the models,\n",
    "starts the server, and runs the benchmark.\n",
    "\n",
    "**Instance**: trn2.3xlarge (1 Neuron device, 8 logical NeuronCores at LNC=1)  \n",
    "**Time to complete**: ~30 minutes (Docker build dominates)  \n",
    "**Prerequisites**: Deep Learning AMI Neuron (Ubuntu 24.04), Docker installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We measure throughput (inferences/second) and latency (P50, P95, P99) under multiple load patterns,\n",
    "varying both client-side batch size and request concurrency to exercise Triton's dynamic batching.\n",
    "\n",
    "- **Input**: Fixed \"hello world\" text, tokenized and padded to 128 tokens\n",
    "- **Output**: 768-dimensional CLS token embedding (FP32 output, BF16 internal matmult via `--auto-cast matmult`)\n",
    "- **Duration**: 10 seconds per test configuration\n",
    "- **Warmup**: 5 requests before timing\n",
    "- **Concurrency**: Python threads, one HTTP client per thread\n",
    "\n",
    "### LNC=1 vs LNC=2\n",
    "\n",
    "| Property | LNC=2 | LNC=1 |\n",
    "|----------|-------|-------|\n",
    "| Logical cores | 4 | **8** |\n",
    "| Physical cores per logical core | 2 | **1** |\n",
    "| Memory per logical core | 24 GB | **12 GB** |\n",
    "| Triton model instances | 4 | **8** |\n",
    "| Single-core throughput | ~4,329 inf/s | ~2,661 inf/s |\n",
    "| Total throughput (peak) | ~17,300 inf/s | ~21,300 inf/s |\n",
    "| Per-request latency | Lower | Higher (~2x) |\n",
    "\n",
    "### Neuron-Specific: Per-Batch-Size Compilation\n",
    "\n",
    "Unlike GPUs, Neuron models are compiled for a **fixed batch size**. Running a BS=1 request through\n",
    "a BS=16 compiled model wastes 15/16 of the compute. We compile separate models for BS=1, 2, 4, 8, 16\n",
    "and the Triton Python backend dispatches each request to the smallest model that fits.\n",
    "\n",
    "**Critical**: Models compiled with `--lnc 1` are **not interchangeable** with `--lnc 2` models.\n",
    "The `--lnc` compiler flag and `NEURON_LOGICAL_NC_CONFIG` environment variable must always match.\n",
    "\n",
    "### Triton Configuration\n",
    "\n",
    "- **8 model instances** (one per logical NeuronCore), each pinned via `NEURON_RT_VISIBLE_CORES`\n",
    "- **Dynamic batching**: preferred sizes [4, 8, 16], max queue delay 5ms\n",
    "- **Python backend**: Triton has no native Neuron backend, so we use the Python backend with `torch_neuronx`\n",
    "\n",
    "### Test Matrix\n",
    "\n",
    "| Test | Concurrency | Client Batch Sizes |\n",
    "|------|-------------|--------------------|\n",
    "| Baseline | 1 (no batching) | 1 |\n",
    "| Dynamic batching | 16, 32, 64 | 1, 2, 4, 8, 16 |\n",
    "\n",
    "Concurrency levels are doubled vs LNC=2 (16/32/64 instead of 8/16/32) to maintain\n",
    "2 workers per core as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Instance Specifications\n",
    "\n",
    "| Spec | Value |\n",
    "|------|-------|\n",
    "| Model | DistilBERT base uncased (67M params) |\n",
    "| Architecture | 6-layer transformer, hidden size 768 |\n",
    "| Input | 128 tokens (padded) |\n",
    "| Output | 768-dim CLS embedding (FP32 output, BF16 matmult) |\n",
    "| Compiled model size | ~148 MB per batch size variant |\n",
    "| Instance type | trn2.3xlarge |\n",
    "| Neuron device | 1 device, 8 physical NeuronCores |\n",
    "| LNC config | **1 (= 8 logical cores, 12 GB each)** |\n",
    "| vCPUs / RAM | 12 / 128 GB |\n",
    "\n",
    "### Software Stack\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|--------|\n",
    "| Neuron SDK | 2.28 |\n",
     "| PyTorch / torch-neuronx | 2.9.0 / 2.9.0.2.12 |\n",
     "| neuronx-cc (compiler) | 2.22.22436 |\n",
     "| Transformers | 4.48.0 |\n",
     "| Triton Inference Server | 2.65.0 (r26.01, built from source) |\n",
     "| Python | 3.12 |\n",
     "| OS / AMI | Ubuntu 24.04 / Deep Learning AMI Neuron 20260227 |\n",
    "\n",
    "> **Important**: Transformers versions 4.54.0+ have a confirmed 31% performance regression for\n",
    "> DistilBERT on Neuron. Use versions 4.48.0 through 4.53.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Setup & Environment Check\n",
    "\n",
    "After you deploy the instance using the Ubuntu Neuron Deep Learning AMI (with all the Neuron drivers installed), run this notebook inside the pre-installed PyTorch 2.9 Neuron virtual environment:\n",
    "\n",
    "```bash\n",
    "source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser\n",
    "```\n",
    "\n",
    "Alternatively, if you are running from withing a remote vscode instance, you can use ```ln -s /opt/aws_neuronx_venv_pytorch_2_9/bin/activate ~/.venv``` to help vscode find your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:43:08.965263Z",
     "iopub.status.busy": "2026-02-24T23:43:08.965125Z",
     "iopub.status.idle": "2026-02-24T23:43:14.193734Z",
     "shell.execute_reply": "2026-02-24T23:43:14.193150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu128\n",
       "torch-neuronx: 2.9.0.2.12.22436+0f1dac25\n",
      "transformers: 4.48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instance-type: trn2.3xlarge\n",
      "instance-id: i-09dbb4802167c2239\n",
      "logical-neuroncore-config: 2\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 4      | 0-3      | 96 GB  | 0000:33:00.0 | 0-11     | 0    |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os, time, shutil\n",
    "\n",
    "# Verify Neuron environment\n",
    "import torch, torch_neuronx\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'torch-neuronx: {torch_neuronx.__version__}')\n",
    "\n",
    "# Ensure correct transformers version\n",
    "try:\n",
    "    import transformers\n",
    "    ver = transformers.__version__\n",
    "    print(f'transformers: {ver}')\n",
    "    if ver >= '4.54.0':\n",
    "        print('WARNING: transformers >= 4.54.0 has 31% regression. Downgrading...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                               'transformers==4.48.0', '-q'])\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           'transformers==4.48.0', '-q'])\n",
    "\n",
    "# Install Triton client\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                       'tritonclient[http]', '-q'])\n",
    "\n",
    "# Show Neuron devices\n",
    "r = subprocess.run(['neuron-ls'], capture_output=True, text=True)\n",
    "print(f'\\n{r.stdout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Compile DistilBERT for All Batch Sizes (LNC=1)\n",
    "\n",
    "Compile 5 model variants (BS=1, 2, 4, 8, 16) with sequence length 128 and **LNC=1**.\n",
    "Takes ~5 minutes per variant (~25 min total--time may vary for other models). Skips already-compiled models, so if you are running this multiple times you will see faster compilation.  For production, you would deploy with the pre-compiled models.\n",
    "\n",
    "**Critical**: The `--lnc 1` compiler flag produces models that are **incompatible** with LNC=2.\n",
    "Never mix LNC=1 and LNC=2 compiled models.\n",
    "\n",
    "The `--auto-cast matmult` flag casts matrix multiplications to BF16, yielding ~2.8x throughput\n",
    "with negligible accuracy loss (cosine similarity > 0.99999 vs FP32). This matches the\n",
    "compilation settings used in the [AWS Neuron SDK benchmarks](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/about-neuron/benchmarks/inf2/inf2-performance.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:43:14.195618Z",
     "iopub.status.busy": "2026-02-24T23:43:14.195250Z",
     "iopub.status.idle": "2026-02-24T23:44:42.451772Z",
     "shell.execute_reply": "2026-02-24T23:44:42.450871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling DistilBERT for batch sizes: [1, 2, 4, 8, 16]\n",
      "Sequence length: 128, LNC: 1\n",
      "\n",
      "  Compiling BS=1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs1.pt (148.1 MB)\n",
      "  Compiling BS=2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs2.pt (148.1 MB)\n",
      "  Compiling BS=4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs4.pt (148.2 MB)\n",
      "  Compiling BS=8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs8.pt (148.4 MB)\n",
      "  Compiling BS=16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/distilbert/1/model_bs16.pt (149.1 MB)\n",
      "\n",
      "Compiled models:\n",
      "  model_bs1.pt: 148.1 MB\n",
      "  model_bs16.pt: 149.1 MB\n",
      "  model_bs2.pt: 148.1 MB\n",
      "  model_bs4.pt: 148.2 MB\n",
      "  model_bs8.pt: 148.4 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "os.environ['NEURON_RT_LOG_LEVEL'] = 'ERROR'\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "LNC = 1\n",
    "MODEL_DIR = os.path.expanduser('~/triton_repo/distilbert/1')\n",
    "BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "print(f'Compiling DistilBERT for batch sizes: {BATCH_SIZES}')\n",
    "print(f'Sequence length: {SEQ_LENGTH}, LNC: {LNC}\\n')\n",
    "\n",
    "for bs in BATCH_SIZES:\n",
    "    output_path = os.path.join(MODEL_DIR, f'model_bs{bs}.pt')\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'  BS={bs}: already compiled, skipping')\n",
    "        continue\n",
    "    print(f'  Compiling BS={bs}...')\n",
    "    texts = ['Test sentence.'] * bs\n",
    "    inputs = tokenizer(texts, return_tensors='pt', max_length=SEQ_LENGTH,\n",
    "                       padding='max_length', truncation=True)\n",
    "    model_neuron = torch_neuronx.trace(\n",
    "        model, (inputs['input_ids'], inputs['attention_mask']),\n",
    "        compiler_args=['--model-type', 'transformer', '--optlevel', '2',\n",
    "                       '--auto-cast', 'matmult', '--lnc', str(LNC)])\n",
    "    torch.jit.save(model_neuron, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f'    Saved: {output_path} ({size_mb:.1f} MB)')\n",
    "\n",
    "print('\\nCompiled models:')\n",
    "for f in sorted(os.listdir(MODEL_DIR)):\n",
    "    if f.endswith('.pt'):\n",
    "        size = os.path.getsize(os.path.join(MODEL_DIR, f)) / (1024 * 1024)\n",
    "        print(f'  {f}: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Write Triton Model Repository Files (LNC=1)\n",
    "\n",
    "Write `config.pbtxt` and `model.py` into the model repository alongside the compiled `.pt` files.\n",
    "\n",
    "Key differences from LNC=2:\n",
    "- `instance_group count: 8` (8 logical cores)\n",
    "- `NEURON_LOGICAL_NC_CONFIG = \"1\"` in model.py\n",
    "- Core IDs 0-7 (vs 0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:44:42.454200Z",
     "iopub.status.busy": "2026-02-24T23:44:42.453893Z",
     "iopub.status.idle": "2026-02-24T23:44:42.460670Z",
     "shell.execute_reply": "2026-02-24T23:44:42.460068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config.pbtxt (instance_group count=8 for LNC=1)\n",
      "Wrote model.py (NEURON_LOGICAL_NC_CONFIG=1, cores 0-7)\n",
      "\n",
      "Model repository:\n",
      "triton_repo/\n",
      "  distilbert/\n",
      "    config.pbtxt\n",
      "    1/\n",
      "      model.py\n",
      "      model_bs1.pt (148.1 MB)\n",
      "      model_bs16.pt (149.1 MB)\n",
      "      model_bs2.pt (148.1 MB)\n",
      "      model_bs4.pt (148.2 MB)\n",
      "      model_bs8.pt (148.4 MB)\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = os.path.expanduser('~/triton_repo/distilbert')\n",
    "\n",
    "# ── config.pbtxt (LNC=1: 8 instances) ─────────────────────────────────────────\n",
    "config_pbtxt = r\"\"\"name: \"distilbert\"\n",
    "platform: \"python\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [128]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [128]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"embeddings\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 8\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 5000\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"model_dir\"\n",
    "  value: { string_value: \"/models/distilbert/1\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"tokenizer_name\"\n",
    "  value: { string_value: \"distilbert-base-uncased\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"max_seq_length\"\n",
    "  value: { string_value: \"128\" }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(REPO_DIR, 'config.pbtxt'), 'w') as f:\n",
    "    f.write(config_pbtxt)\n",
    "print('Wrote config.pbtxt (instance_group count=8 for LNC=1)')\n",
    "\n",
    "# ── model.py (Triton Python backend, LNC=1) ──────────────────────────────────\n",
    "model_py = r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Triton Python Backend for DistilBERT on AWS Neuron - Multi-Model (LNC=1)\n",
    "\n",
    "Loads compiled models for each batch size (1, 2, 4, 8, 16) and\n",
    "dispatches to the best-fit model to avoid padding waste.\n",
    "Each Triton instance is pinned to a separate Neuron core (cores 0-7).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import triton_python_backend_utils as pb_utils\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\"model_config\"])\n",
    "        params = self.model_config.get(\"parameters\", {})\n",
    "        model_dir = params.get(\"model_dir\", {}).get(\n",
    "            \"string_value\", \"/models/distilbert/1\")\n",
    "        tokenizer_name = params.get(\"tokenizer_name\", {}).get(\n",
    "            \"string_value\", \"distilbert-base-uncased\")\n",
    "        self.max_seq_length = int(\n",
    "            params.get(\"max_seq_length\", {}).get(\"string_value\", \"128\"))\n",
    "\n",
    "        # Pin this instance to a specific Neuron core (0-7 for LNC=1)\n",
    "        instance_name = args.get(\"model_instance_name\", \"distilbert_0_0\")\n",
    "        core_id = int(instance_name.split(\"_\")[-1])\n",
    "        os.environ[\"NEURON_RT_VISIBLE_CORES\"] = str(core_id)\n",
    "        os.environ[\"NEURON_LOGICAL_NC_CONFIG\"] = \"1\"\n",
    "        print(f\"Instance {instance_name}: pinned to Neuron core {core_id}\")\n",
    "\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n",
    "        print(\"Tokenizer loaded\")\n",
    "\n",
    "        # Load all compiled models\n",
    "        self.models = {}\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            model_path = os.path.join(model_dir, f\"model_bs{bs}.pt\")\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[bs] = torch.jit.load(model_path)\n",
    "                self.models[bs].eval()\n",
    "                print(f\"  Loaded model for batch_size={bs}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Model not found: {model_path}\")\n",
    "\n",
    "        if not self.models:\n",
    "            raise RuntimeError(\"No compiled models found!\")\n",
    "\n",
    "        # Warmup all models\n",
    "        print(\"Warming up models...\")\n",
    "        for bs, mdl in self.models.items():\n",
    "            dummy_ids = torch.zeros((bs, self.max_seq_length), dtype=torch.long)\n",
    "            dummy_mask = torch.ones((bs, self.max_seq_length), dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = mdl(dummy_ids, dummy_mask)\n",
    "            print(f\"  Warmed up BS={bs}\")\n",
    "        print(f\"Model initialization complete! \"\n",
    "              f\"Available batch sizes: {sorted(self.models.keys())}\")\n",
    "\n",
    "    def _get_best_model(self, actual_batch_size):\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            if bs >= actual_batch_size and bs in self.models:\n",
    "                return bs, self.models[bs]\n",
    "        largest = max(self.models.keys())\n",
    "        return largest, self.models[largest]\n",
    "\n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        for request in requests:\n",
    "            input_ids = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            ).long()\n",
    "            attention_mask = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            ).long()\n",
    "\n",
    "            actual_bs = input_ids.shape[0]\n",
    "            target_bs, model = self._get_best_model(actual_bs)\n",
    "\n",
    "            # Pad to compiled batch size if needed\n",
    "            if actual_bs < target_bs:\n",
    "                pad = target_bs - actual_bs\n",
    "                input_ids = torch.cat([input_ids,\n",
    "                    torch.zeros((pad, input_ids.shape[1]), dtype=torch.long)], dim=0)\n",
    "                attention_mask = torch.cat([attention_mask,\n",
    "                    torch.zeros((pad, attention_mask.shape[1]), dtype=torch.long)], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            embeddings = outputs[\"last_hidden_state\"][:, 0, :].cpu().numpy()\n",
    "            embeddings = embeddings[:actual_bs, :]\n",
    "\n",
    "            output_tensor = pb_utils.Tensor(\"embeddings\",\n",
    "                                            embeddings.astype(np.float32))\n",
    "            responses.append(\n",
    "                pb_utils.InferenceResponse(output_tensors=[output_tensor]))\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(\"Finalizing DistilBERT model...\")\n",
    "        if hasattr(self, \"models\"):\n",
    "            for bs, mdl in self.models.items():\n",
    "                del mdl\n",
    "            self.models.clear()\n",
    "'''\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model.py'), 'w') as f:\n",
    "    f.write(model_py)\n",
    "print('Wrote model.py (NEURON_LOGICAL_NC_CONFIG=1, cores 0-7)')\n",
    "\n",
    "# Verify\n",
    "print('\\nModel repository:')\n",
    "for root, dirs, files in os.walk(os.path.expanduser('~/triton_repo')):\n",
    "    level = root.replace(os.path.expanduser('~/triton_repo'), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, f)) / (1024 * 1024)\n",
    "        label = f'{f} ({size:.1f} MB)' if size > 1 else f\n",
    "        print(f'{indent}  {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Build Triton + Neuron Docker Image\n",
    "\n",
    "Triton has no native Neuron backend, so we build Triton from source inside the AWS Neuron\n",
    "PyTorch inference base image. This takes **15-20 minutes** and produces a ~15.8 GB image.\n",
    "\n",
    "The cell skips the build if the image already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:44:42.462111Z",
     "iopub.status.busy": "2026-02-24T23:44:42.461994Z",
     "iopub.status.idle": "2026-02-24T23:44:42.551184Z",
     "shell.execute_reply": "2026-02-24T23:44:42.550572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker image already exists: 26b55f589d0c\n",
      "Delete with: docker rmi triton-neuron-distilbert:latest\n",
      "IMAGE                             ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
      "triton-neuron-distilbert:latest   26b55f589d0c       24.1GB         7.82GB        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOCKER_IMAGE = 'triton-neuron-distilbert:latest'\n",
    "\n",
    "# Check if already built\n",
    "r = subprocess.run(['docker', 'images', '-q', DOCKER_IMAGE],\n",
    "                   capture_output=True, text=True)\n",
    "if r.stdout.strip():\n",
    "    print(f'Docker image already exists: {r.stdout.strip()}')\n",
    "    print('Delete with: docker rmi ' + DOCKER_IMAGE)\n",
    "else:\n",
    "    # Write Dockerfile\n",
    "    dockerfile = r\"\"\"ARG BASE_IMAGE=public.ecr.aws/neuron/pytorch-inference-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04\n",
    "FROM $BASE_IMAGE\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PJRT_DEVICE=NEURON \\\n",
    "    LD_LIBRARY_PATH=\"/opt/conda/lib:/opt/aws/neuron/lib:/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}\" \\\n",
    "    PATH=\"/opt/program:/opt/aws/neuron/bin:/opt/tritonserver/bin:${PATH}\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    wget gnupg2 build-essential git nginx pkg-config unzip \\\n",
    "    libssl-dev libcurl4-openssl-dev libgoogle-perftools-dev \\\n",
    "    libnuma-dev libarchive-dev libxml2-dev zlib1g-dev \\\n",
    "    autoconf automake libtool gperf scons patchelf \\\n",
    "    libre2-dev libb64-dev rapidjson-dev libboost-dev \\\n",
    "    cmake cmake-data \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel virtualenv build cmake==3.31.10\n",
    "RUN pip3 install transformers==4.48.0\n",
    "\n",
    "RUN git clone --depth=1 --branch=r26.01 https://github.com/triton-inference-server/server.git /server && \\\n",
    "    cd /server && \\\n",
    "    Python3_ROOT_DIR=/opt/conda \\\n",
    "    Python3_EXECUTABLE=/opt/conda/bin/python3 \\\n",
    "    Python3_INCLUDE_DIR=/opt/conda/include/python3.12 \\\n",
    "    Python3_LIBRARY=/opt/conda/lib/libpython3.12.so \\\n",
    "    ./build.py -v --no-container-build --build-dir=/server/build --backend=python \\\n",
    "    --enable-metrics --enable-logging --enable-stats --endpoint=\"http\" --endpoint=\"grpc\" && \\\n",
    "    cp -r /server/build/opt/* /opt/ && \\\n",
    "    cd / && rm -rf /server\n",
    "\n",
    "EXPOSE 8000 8001 8002\n",
    "CMD [\"tritonserver\", \"--model-repository=/models\"]\n",
    "\"\"\"\n",
    "    dockerfile_path = os.path.expanduser('~/Dockerfile.triton-neuron')\n",
    "    with open(dockerfile_path, 'w') as f:\n",
    "        f.write(dockerfile)\n",
    "\n",
    "    print('Building Triton + Neuron Docker image (15-20 minutes)...')\n",
    "    r = subprocess.run(\n",
    "        ['docker', 'build', '-f', dockerfile_path, '-t', DOCKER_IMAGE, '.'],\n",
    "        cwd=os.path.expanduser('~'),\n",
    "        capture_output=True, text=True, timeout=2400)\n",
    "    if r.returncode == 0:\n",
    "        print('Build complete!')\n",
    "    else:\n",
    "        print(f'Build FAILED (rc={r.returncode})')\n",
    "        print(r.stderr[-3000:])\n",
    "\n",
    "# Show image\n",
    "r = subprocess.run(['docker', 'images', DOCKER_IMAGE], capture_output=True, text=True)\n",
    "print(r.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Start Triton Server\n",
    "\n",
    "Launch the Docker container with the Neuron device mounted and the model repository bind-mounted.\n",
    "With 8 instances (LNC=1), model loading takes longer (~90-120 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:44:42.552366Z",
     "iopub.status.busy": "2026-02-24T23:44:42.552250Z",
     "iopub.status.idle": "2026-02-24T23:45:24.393488Z",
     "shell.execute_reply": "2026-02-24T23:45:24.392732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container started. Waiting for 8 model instances to load (~90-120s)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server ready after ~36s\n",
      "Model instances initialized: 8/8\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "NUM_INSTANCES = 8  # LNC=1: 8 logical cores\n",
    "\n",
    "# Stop any previous run\n",
    "subprocess.run(['docker', 'rm', '-f', 'triton-distilbert'],\n",
    "               capture_output=True)\n",
    "time.sleep(3)\n",
    "\n",
    "cmd = [\n",
    "    'docker', 'run', '-d',\n",
    "    '--name', 'triton-distilbert',\n",
    "    '--device=/dev/neuron0',\n",
    "    '-v', os.path.expanduser('~/triton_repo') + ':/models:ro',\n",
    "    '-p', '8000:8000', '-p', '8001:8001', '-p', '8002:8002',\n",
    "    DOCKER_IMAGE,\n",
    "    'tritonserver', '--model-repository=/models', '--log-verbose=0',\n",
    "]\n",
    "r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "if r.returncode != 0:\n",
    "    print(f'Failed to start container: {r.stderr}')\n",
    "else:\n",
    "    print(f'Container started. Waiting for {NUM_INSTANCES} model instances to load (~90-120s)...')\n",
    "\n",
    "# Poll for readiness\n",
    "for i in range(180):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen('http://localhost:8000/v2/health/ready', timeout=2)\n",
    "        if resp.status == 200:\n",
    "            elapsed = (i + 1) * 2\n",
    "            print(f'Server ready after ~{elapsed}s')\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('Timeout waiting for server!')\n",
    "    r = subprocess.run(['docker', 'logs', '--tail', '30', 'triton-distilbert'],\n",
    "                       capture_output=True, text=True)\n",
    "    print(r.stdout)\n",
    "\n",
    "# Verify instances loaded\n",
    "r = subprocess.run(['docker', 'logs', 'triton-distilbert'],\n",
    "                   capture_output=True, text=True)\n",
    "n = r.stdout.count('Model initialization complete')\n",
    "print(f'Model instances initialized: {n}/{NUM_INSTANCES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Run Benchmark\n",
    "\n",
    "Runs the full test matrix: 1 baseline + 15 dynamic-batching configurations (3 concurrency levels\n",
    "x 5 batch sizes), 10 seconds each. Total runtime ~3 minutes.\n",
    "\n",
    "Concurrency levels are 16/32/64 (vs 8/16/32 for LNC=2) to maintain 2 workers per core as baseline.\n",
    "\n",
    "**Note**: One worker per test will print a harmless greenlet thread-switch error. This is a\n",
    "known cosmetic issue in `tritonclient` and does not affect results -- the remaining workers run fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:45:24.395038Z",
     "iopub.status.busy": "2026-02-24T23:45:24.394910Z",
     "iopub.status.idle": "2026-02-24T23:48:07.939901Z",
     "shell.execute_reply": "2026-02-24T23:48:07.939185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTILBERT TRITON BENCHMARK - Neuron (trn2.3xlarge, LNC=1)\n",
      "==========================================================================================\n",
      "\n",
      "Baseline: single request, no concurrency...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  P50: 7.10ms  Throughput: 141 inf/sec\n",
      "\n",
      "Concurrency=16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   4.37ms  P95=   5.26ms  P99=   5.63ms  Throughput=    3392 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   3.49ms  P95=   5.26ms  P99=   6.55ms  Throughput=    8048 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=   3.79ms  P95=   7.08ms  P99=   9.23ms  Throughput=   14328 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=   8.05ms  P95=   9.45ms  P99=  10.16ms  Throughput=   17310 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  11.18ms  P95=  12.86ms  P99=  13.95ms  Throughput=   21289 inf/sec\n",
      "\n",
      "Concurrency=32:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   6.15ms  P95=  11.82ms  P99=  15.88ms  Throughput=    4489 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   5.65ms  P95=  14.31ms  P99=  19.99ms  Throughput=    9062 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=   7.06ms  P95=  16.46ms  P99=  22.23ms  Throughput=   14898 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  14.31ms  P95=  15.74ms  P99=  16.75ms  Throughput=   17229 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  23.10ms  P95=  25.63ms  P99=  27.00ms  Throughput=   21176 inf/sec\n",
      "\n",
      "Concurrency=64:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   9.77ms  P95=  27.70ms  P99=  40.13ms  Throughput=    4909 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   8.90ms  P95=  32.58ms  P99=  48.74ms  Throughput=    9685 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  13.14ms  P95=  34.36ms  P99=  47.95ms  Throughput=   15641 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  29.03ms  P95=  31.04ms  P99=  32.49ms  Throughput=   16880 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  47.05ms  P95=  50.73ms  P99=  52.66ms  Throughput=   21151 inf/sec\n",
      "\n",
      "==========================================================================================\n",
      "Batch   Workers   Requests    P50 (ms)    P95 (ms)    P99 (ms)    Throughput     \n",
      "------------------------------------------------------------------------------------------\n",
      "1       1         1409        7.10        7.30        7.37        141            \n",
      "1       16        33973       4.37        5.26        5.63        3392           \n",
      "2       16        40417       3.49        5.26        6.55        8048           \n",
      "4       16        35894       3.79        7.08        9.23        14328          \n",
      "8       16        21687       8.05        9.45        10.16       17310          \n",
      "16      16        13338       11.18       12.86       13.95       21289          \n",
      "1       32        45128       6.15        11.82       15.88       4489           \n",
      "2       32        45712       5.65        14.31       19.99       9062           \n",
      "4       32        37701       7.06        16.46       22.23       14898          \n",
      "8       32        21687       14.31       15.74       16.75       17229          \n",
      "16      32        13339       23.10       25.63       27.00       21176          \n",
      "1       64        51157       9.77        27.70       40.13       4909           \n",
      "2       64        50924       8.90        32.58       48.74       9685           \n",
      "4       64        40735       13.14       34.36       47.95       15641          \n",
      "8       64        21598       29.03       31.04       32.49       16880          \n",
      "16      64        13457       47.05       50.73       52.66       21151          \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'distilbert'\n",
    "DURATION = 10.0  # seconds per test\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def _worker(client, batch_size, duration_s, latencies_q, stop_evt):\n",
    "    sentences = ['hello world'] * batch_size\n",
    "    tokens = tok(sentences, max_length=128, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    t_end = time.time() + duration_s\n",
    "    while time.time() < t_end and not stop_evt.is_set():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "            latencies_q.put((time.time() - t0) * 1000)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "\n",
    "def run_concurrent(batch_size, num_workers, duration_s=DURATION):\n",
    "    clients = [httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "               for _ in range(num_workers)]\n",
    "    # Warmup\n",
    "    tokens = tok(['hello world'] * batch_size, max_length=128,\n",
    "                 padding='max_length', truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(5):\n",
    "        clients[0].infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "\n",
    "    q = Queue()\n",
    "    stop = threading.Event()\n",
    "    threads = []\n",
    "    t_start = time.time()\n",
    "    for i in range(num_workers):\n",
    "        t = threading.Thread(target=_worker,\n",
    "                             args=(clients[i], batch_size, duration_s, q, stop))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    total_time = time.time() - t_start\n",
    "\n",
    "    latencies = []\n",
    "    while not q.empty():\n",
    "        latencies.append(q.get())\n",
    "    if not latencies:\n",
    "        return None\n",
    "    return {\n",
    "        'batch_size': batch_size, 'num_workers': num_workers,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': (len(latencies) * batch_size) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_baseline(duration_s=DURATION):\n",
    "    client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    tokens = tok(['hello world'], max_length=128, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(10):\n",
    "        client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "    latencies = []\n",
    "    t_start = time.time()\n",
    "    while time.time() - t_start < duration_s:\n",
    "        t0 = time.time()\n",
    "        client.infer(model_name=MODEL_NAME, inputs=[inp_ids, inp_mask])\n",
    "        latencies.append((time.time() - t0) * 1000)\n",
    "    total_time = time.time() - t_start\n",
    "    return {\n",
    "        'batch_size': 1, 'num_workers': 1,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': len(latencies) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Run all tests ─────────────────────────────────────────────────────────────\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "assert client.is_server_ready(), 'Triton server not ready!'\n",
    "\n",
    "print('DISTILBERT TRITON BENCHMARK - Neuron (trn2.3xlarge, LNC=1)')\n",
    "print('=' * 90)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline\n",
    "print('\\nBaseline: single request, no concurrency...')\n",
    "r = run_baseline()\n",
    "results.append(r)\n",
    "print(f'  P50: {r[\"p50\"]:.2f}ms  Throughput: {r[\"throughput\"]:.0f} inf/sec')\n",
    "\n",
    "# Dynamic batching -- concurrency levels scaled for 8 cores\n",
    "for conc in [16, 32, 64]:\n",
    "    print(f'\\nConcurrency={conc}:')\n",
    "    for bs in [1, 2, 4, 8, 16]:\n",
    "        r = run_concurrent(bs, conc)\n",
    "        if r:\n",
    "            results.append(r)\n",
    "            print(f'  BS={bs:<3} P50={r[\"p50\"]:>7.2f}ms  '\n",
    "                  f'P95={r[\"p95\"]:>7.2f}ms  '\n",
    "                  f'P99={r[\"p99\"]:>7.2f}ms  '\n",
    "                  f'Throughput={r[\"throughput\"]:>8.0f} inf/sec')\n",
    "\n",
    "# Summary table\n",
    "print(f'\\n{\"=\" * 90}')\n",
    "print(f'{\"Batch\":<8}{\"Workers\":<10}{\"Requests\":<12}'\n",
    "      f'{\"P50 (ms)\":<12}{\"P95 (ms)\":<12}{\"P99 (ms)\":<12}{\"Throughput\":<15}')\n",
    "print('-' * 90)\n",
    "for r in results:\n",
    "    print(f'{r[\"batch_size\"]:<8}{r[\"num_workers\"]:<10}{r[\"total_requests\"]:<12}'\n",
    "          f'{r[\"p50\"]:<12.2f}{r[\"p95\"]:<12.2f}{r[\"p99\"]:<12.2f}'\n",
    "          f'{r[\"throughput\"]:<15.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-24T23:48:07.941319Z",
     "iopub.status.busy": "2026-02-24T23:48:07.941170Z",
     "iopub.status.idle": "2026-02-24T23:48:08.502971Z",
     "shell.execute_reply": "2026-02-24T23:48:08.502297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton server stopped and removed.\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(['docker', 'rm', '-f', 'triton-distilbert'], capture_output=True)\n",
    "print('Triton server stopped and removed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (aws_neuronx_venv_pytorch_2_9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
