{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BERT Reranker Inference on AWS Neuron"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "This notebook demonstrates how to compile and run the [Alibaba-NLP/gte-multilingual-reranker-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base) BERT reranker model for accelerated inference on AWS Neuron (Inferentia2/Trainium).\n",
                "\n",
                "This notebook was tested on an **inf2.8xlarge** instance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "This notebook requires an Inferentia2 or Trainium instance with the following Neuron SDK packages:\n",
                "\n",
                "- `torch-neuronx`\n",
                "- `neuronx-cc`\n",
                "- `transformers` (4.48 - 4.53)\n",
                "\n",
                "For a step-by-step guide on launching an instance, see [Getting Started with Inferentia or Trainium](https://repost.aws/articles/ARgiH8VXXuQ22iSUmwX7ffiQ/getting-started-with-inferentia-or-trainium).\n",
                "\n",
                "**Important**: The version of `transformers` affects compilation performance. Versions 4.48 through 4.53\n",
                "produce an optimal TorchScript graph for `torch_neuronx.trace()`. Other versions (including the 4.56\n",
                "shipped with the Neuron DLAMI) produce a ~20% slower compiled model. Pin the version before compiling:\n",
                "\n",
                "```bash\n",
                "pip install \"transformers>=4.48,<=4.53.3\"\n",
                "```\n",
                "\n",
                "If you are using VS Code with Remote SSH on the AWS Neuron DLAMI, you can make the pre-installed environment available as a kernel:\n",
                "\n",
                "```bash\n",
                "ln -s /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13 ~/.venv\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch_neuronx\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"torch-neuronx version: {torch_neuronx.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "\n",
                "Set the model parameters. For best performance, we recommend:\n",
                "\n",
                "- `--autocast matmult`: Enables BF16 for matrix multiplications (2x faster, minimal accuracy impact)\n",
                "- `--optlevel 2`: Standard compiler optimizations\n",
                "\n",
                "**DataParallel**: If using an instance with multiple Neuron cores (e.g., inf2.8xlarge has 2 cores),\n",
                "you can use `torch_neuronx.DataParallel` to split batches across cores for higher throughput."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration\n",
                "MODEL_ID = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
                "SEQUENCE_LENGTH = 1024\n",
                "BATCH_SIZE = 16\n",
                "AUTOCAST = \"matmult\"  # Use 'matmult' for best performance, 'none' for full precision\n",
                "OPTLEVEL = 2\n",
                "\n",
                "print(f\"Model: {MODEL_ID}\")\n",
                "print(f\"Sequence Length: {SEQUENCE_LENGTH}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Autocast: {AUTOCAST}\")\n",
                "print(f\"Optlevel: {OPTLEVEL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer and model\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    torchscript=True,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model.eval()\n",
                "\n",
                "# Model info\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"Hidden size: {model.config.hidden_size}\")\n",
                "print(f\"Layers: {model.config.num_hidden_layers}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare Example Input\n",
                "\n",
                "Create example inputs for tracing. The input shape during compilation must match inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create example input\n",
                "queries = [\"What is machine learning?\"] * BATCH_SIZE\n",
                "docs = [\"Machine learning is a subset of artificial intelligence.\"] * BATCH_SIZE\n",
                "\n",
                "encoded = tokenizer(\n",
                "    queries,\n",
                "    docs,\n",
                "    padding=\"max_length\",\n",
                "    max_length=SEQUENCE_LENGTH,\n",
                "    truncation=True,\n",
                "    return_tensors=\"pt\"\n",
                ")\n",
                "\n",
                "example_inputs = (encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
                "print(f\"Input shape: {example_inputs[0].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compile Model\n",
                "\n",
                "Compile the model using `torch_neuronx.trace()` with optimized settings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "# Build compiler arguments\n",
                "compiler_args = [f\"--optlevel={OPTLEVEL}\"]\n",
                "if AUTOCAST == \"matmult\":\n",
                "    compiler_args.extend([\"--auto-cast\", \"matmult\"])\n",
                "\n",
                "print(f\"Compiler args: {compiler_args}\")\n",
                "\n",
                "# Compile\n",
                "print(\"\\nCompiling model...\")\n",
                "start = time.time()\n",
                "model_neuron = torch_neuronx.trace(\n",
                "    model,\n",
                "    example_inputs,\n",
                "    compiler_args=compiler_args\n",
                ")\n",
                "compile_time = time.time() - start\n",
                "\n",
                "# Save\n",
                "output_path = f\"bert_reranker_seq{SEQUENCE_LENGTH}_batch{BATCH_SIZE}.pt\"\n",
                "torch.jit.save(model_neuron, output_path)\n",
                "\n",
                "print(f\"Compilation time: {compile_time:.1f}s\")\n",
                "print(f\"Saved to: {output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Verify Compilation\n",
                "\n",
                "Run a quick test to verify the compiled model produces valid output.\n",
                "The model is saved to disk so it can be loaded in future sessions without recompiling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test inference with the compiled model\n",
                "with torch.no_grad():\n",
                "    outputs = model_neuron(*example_inputs)\n",
                "\n",
                "# Get score\n",
                "if isinstance(outputs, tuple):\n",
                "    logits = outputs[0]\n",
                "else:\n",
                "    logits = outputs\n",
                "\n",
                "score = torch.sigmoid(logits[0]).item()\n",
                "print(f\"Sample score: {score:.4f}\")\n",
                "print(\"Model compiled and running successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Benchmark Single-Core Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "WARMUP_ITERATIONS = 10\n",
                "BENCHMARK_ITERATIONS = 50\n",
                "\n",
                "print(\"Warming up...\")\n",
                "for _ in range(WARMUP_ITERATIONS):\n",
                "    with torch.no_grad():\n",
                "        _ = model_neuron(*example_inputs)\n",
                "\n",
                "print(f\"Benchmarking ({BENCHMARK_ITERATIONS} iterations)...\")\n",
                "latencies = []\n",
                "for _ in range(BENCHMARK_ITERATIONS):\n",
                "    start = time.perf_counter()\n",
                "    with torch.no_grad():\n",
                "        _ = model_neuron(*example_inputs)\n",
                "    end = time.perf_counter()\n",
                "    latencies.append((end - start) * 1000)\n",
                "\n",
                "latencies = np.array(latencies)\n",
                "p50 = np.percentile(latencies, 50)\n",
                "p90 = np.percentile(latencies, 90)\n",
                "p99 = np.percentile(latencies, 99)\n",
                "throughput = (BATCH_SIZE * 1000) / np.mean(latencies)\n",
                "\n",
                "print(\"\\n=== Single-Core Results ===\")\n",
                "print(f\"Latency p50: {p50:.2f} ms\")\n",
                "print(f\"Latency p90: {p90:.2f} ms\")\n",
                "print(f\"Latency p99: {p99:.2f} ms\")\n",
                "print(f\"Throughput: {throughput:.2f} queries/second\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Benchmark with DataParallel\n",
                "\n",
                "`torch_neuronx.DataParallel` loads the same compiled model onto each available Neuron core\n",
                "and runs them in parallel. Each core processes its own full batch independently, multiplying\n",
                "total throughput by the number of cores.\n",
                "\n",
                "On inf2.8xlarge (2 cores), this gives each core its own batch=16, doubling throughput."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import subprocess\n",
                "\n",
                "def get_neuron_core_count():\n",
                "    \"\"\"Detect the number of Neuron cores using neuron-ls.\"\"\"\n",
                "    try:\n",
                "        result = subprocess.run(\n",
                "            [\"neuron-ls\", \"--json-output\"],\n",
                "            capture_output=True, text=True, timeout=10\n",
                "        )\n",
                "        if result.returncode == 0:\n",
                "            devices = json.loads(result.stdout)\n",
                "            return sum(d[\"nc_count\"] for d in devices)\n",
                "    except Exception:\n",
                "        pass\n",
                "    return 1\n",
                "\n",
                "num_cores = get_neuron_core_count()\n",
                "print(f\"Neuron cores detected: {num_cores}\")\n",
                "\n",
                "if num_cores > 1:\n",
                "    # Load the same compiled model onto all cores\n",
                "    model_dp = torch_neuronx.DataParallel(model_neuron)\n",
                "    \n",
                "    # Each core gets a full batch, so total input is BATCH_SIZE * num_cores\n",
                "    dp_total_batch = BATCH_SIZE * num_cores\n",
                "    dp_queries = [\"What is machine learning?\"] * dp_total_batch\n",
                "    dp_docs = [\"Machine learning is a subset of artificial intelligence.\"] * dp_total_batch\n",
                "    dp_encoded = tokenizer(\n",
                "        dp_queries, dp_docs,\n",
                "        padding=\"max_length\", max_length=SEQUENCE_LENGTH,\n",
                "        truncation=True, return_tensors=\"pt\"\n",
                "    )\n",
                "    dp_inputs = (dp_encoded[\"input_ids\"], dp_encoded[\"attention_mask\"])\n",
                "    print(f\"DataParallel input shape: {dp_inputs[0].shape}  ({BATCH_SIZE} per core x {num_cores} cores)\")\n",
                "    \n",
                "    print(\"\\nWarming up DataParallel...\")\n",
                "    for _ in range(WARMUP_ITERATIONS):\n",
                "        with torch.no_grad():\n",
                "            _ = model_dp(*dp_inputs)\n",
                "    \n",
                "    print(f\"Benchmarking DataParallel ({BENCHMARK_ITERATIONS} iterations)...\")\n",
                "    latencies_dp = []\n",
                "    for _ in range(BENCHMARK_ITERATIONS):\n",
                "        start = time.perf_counter()\n",
                "        with torch.no_grad():\n",
                "            _ = model_dp(*dp_inputs)\n",
                "        end = time.perf_counter()\n",
                "        latencies_dp.append((end - start) * 1000)\n",
                "    \n",
                "    latencies_dp = np.array(latencies_dp)\n",
                "    p50_dp = np.percentile(latencies_dp, 50)\n",
                "    p90_dp = np.percentile(latencies_dp, 90)\n",
                "    p99_dp = np.percentile(latencies_dp, 99)\n",
                "    throughput_dp = (dp_total_batch * 1000) / np.mean(latencies_dp)\n",
                "    \n",
                "    print(f\"\\n=== DataParallel Results ({num_cores} cores) ===\")\n",
                "    print(f\"Latency p50: {p50_dp:.2f} ms\")\n",
                "    print(f\"Latency p90: {p90_dp:.2f} ms\")\n",
                "    print(f\"Latency p99: {p99_dp:.2f} ms\")\n",
                "    print(f\"Throughput: {throughput_dp:.2f} queries/second\")\n",
                "    print(f\"\\nSpeedup vs single-core: {throughput_dp/throughput:.2f}x\")\n",
                "else:\n",
                "    print(\"Only 1 Neuron core available - skipping DataParallel benchmark.\")\n",
                "    print(\"Use an instance with multiple cores (e.g., inf2.8xlarge) for DataParallel.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Results Summary\n",
                "\n",
                "Measured on **inf2.8xlarge** (torch-neuronx 2.9.0, neuronx-cc 2.22, transformers 4.53.0, sa-east-1):\n",
                "\n",
                "| Configuration | Batch Size | Seq Length | Latency (p50) | Throughput |\n",
                "|--------------|------------|------------|---------------|------------|\n",
                "| Single-Core | 16 | 1024 | 214.51 ms | 74.59 qps |\n",
                "| DataParallel (2 cores) | 16 per core | 1024 | 228.15 ms | 140.25 qps |\n",
                "\n",
                "**Key Findings**:\n",
                "- `--autocast matmult` provides 57-120% speedup vs `none` with negligible accuracy impact\n",
                "- DataParallel provides ~1.88x throughput by running each core independently\n",
                "- Model size with autocast=matmult: ~720 MB (vs ~830 MB without)\n",
                "- The `transformers` version used at compile time significantly affects performance (see Setup section)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}