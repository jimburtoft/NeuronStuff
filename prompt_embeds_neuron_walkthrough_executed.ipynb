{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Embeddings with vLLM on AWS Inferentia2\n",
    "\n",
    "This notebook demonstrates how to send **pre-computed embedding tensors** (instead of token IDs) to a vLLM server running on AWS Inferentia2 (Neuron). This enables use cases like:\n",
    "\n",
    "- Embedding manipulation (e.g., interpolation, arithmetic)\n",
    "- Custom embedding pipelines (e.g., from a different encoder)\n",
    "- Avoiding redundant tokenization when embeddings are already available\n",
    "\n",
    "**Requirements:** This notebook must be run on an **inf2.8xlarge** instance with the **Deep Learning AMI Neuron (Ubuntu 24.04) 20260126** and a Hugging Face token with access to `meta-llama/Llama-3.1-8B-Instruct`.  It is very version dependent and is meant as an example of how the capability can be added.  To deploy earlier version of the DLAMI see https://builder.aws.com/content/30beZEzf3XencHUcq7XDVLClkQ6/deploying-previous-versions-of-neuron-sdk-on-trainium-and-inferentia-ec2-instances\n",
    "\n",
    "If you are running in a remote vscode, consider running ```ln -s /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13 ~/.venv``` from a terminal before selecting your kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:22:09.290756Z",
     "iopub.status.busy": "2026-02-25T04:22:09.290626Z",
     "iopub.status.idle": "2026-02-25T04:22:09.295680Z",
     "shell.execute_reply": "2026-02-25T04:22:09.294966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "HF_TOKEN: set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your Hugging Face token (required for gated models like Llama)\n",
    "os.environ[\"HF_TOKEN\"] = os.environ.get(\"HF_TOKEN\", \"your-hf-token-here\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "TP_SIZE = 2\n",
    "MAX_MODEL_LEN = 512\n",
    "SERVER_PORT = 8000\n",
    "SERVER_URL = f\"http://localhost:{SERVER_PORT}\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"HF_TOKEN: {'set' if os.environ['HF_TOKEN'] != 'your-hf-token-here' else 'NOT SET -- update above'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone and Install Modified Branches\n",
    "\n",
    "The `prompt_embeds` feature requires modifications to two repositories:\n",
    "\n",
    "1. **vllm-neuron** -- the Neuron platform plugin for vLLM (passes embeddings from the API to the model)\n",
    "2. **neuronx-distributed-inference (NxDI)** -- the Neuron model compilation/execution library (handles embeddings in the traced model graph)\n",
    "\n",
    "We install these over the pre-installed DLAMI venv using editable installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:22:09.326538Z",
     "iopub.status.busy": "2026-02-25T04:22:09.326358Z",
     "iopub.status.idle": "2026-02-25T04:22:26.711999Z",
     "shell.execute_reply": "2026-02-25T04:22:26.711438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'neuronx-distributed-inference'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'embeddings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/embeddings'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NxDI installed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate\n",
    "\n",
    "cd /home/ubuntu\n",
    "if [ ! -d \"neuronx-distributed-inference\" ]; then\n",
    "    git clone -b embeddings https://github.com/jimburtoft/neuronx-distributed-inference.git\n",
    "fi\n",
    "cd neuronx-distributed-inference\n",
    "git checkout embeddings\n",
    "pip install -e . --quiet 2>&1 | tail -3\n",
    "echo \"NxDI installed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:22:26.713993Z",
     "iopub.status.busy": "2026-02-25T04:22:26.713806Z",
     "iopub.status.idle": "2026-02-25T04:24:10.266199Z",
     "shell.execute_reply": "2026-02-25T04:24:10.265387Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'vllm-neuron'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'embeddings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/embeddings'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm-neuron installed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate\n",
    "\n",
    "cd /home/ubuntu\n",
    "if [ ! -d \"vllm-neuron\" ]; then\n",
    "    git clone -b embeddings https://github.com/jimburtoft/vllm-neuron.git\n",
    "fi\n",
    "cd vllm-neuron\n",
    "git checkout embeddings\n",
    "pip install -e . --quiet 2>&1 | tail -3\n",
    "echo \"vllm-neuron installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Start the vLLM Server\n",
    "\n",
    "We launch the vLLM OpenAI-compatible server with `--enable-prompt-embeds`. The server will download model weights, compile the model for Neuron (first run takes ~15-25 minutes; cached NEFFs are used on subsequent runs), load it onto Neuron cores, and start serving.\n",
    "\n",
    "**Important flags:**\n",
    "- `--enable-prompt-embeds` -- enables the prompt embedding API\n",
    "- `--no-enable-prefix-caching` -- **required** (prefix caching is on by default in vLLM v0.13 and is not yet compatible with prompt_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:24:10.268005Z",
     "iopub.status.busy": "2026-02-25T04:24:10.267822Z",
     "iopub.status.idle": "2026-02-25T04:24:10.272078Z",
     "shell.execute_reply": "2026-02-25T04:24:10.271570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server starting in background. Log: /home/ubuntu/vllm_server.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "log_file = \"/home/ubuntu/vllm_server.log\"\n",
    "\n",
    "server_cmd = (\n",
    "    f\"source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate && \"\n",
    "    f\"HF_TOKEN={os.environ['HF_TOKEN']} \"\n",
    "    f\"python -m vllm.entrypoints.openai.api_server \"\n",
    "    f\"--model {MODEL_NAME} \"\n",
    "    f\"--tensor-parallel-size {TP_SIZE} \"\n",
    "    f\"--max-model-len {MAX_MODEL_LEN} \"\n",
    "    f\"--max-num-seqs 1 \"\n",
    "    f\"--enable-prompt-embeds \"\n",
    "    f\"--no-enable-prefix-caching \"\n",
    "    f\"--port {SERVER_PORT}\"\n",
    ")\n",
    "\n",
    "subprocess.Popen(\n",
    "    f\"bash -c '{server_cmd}' > {log_file} 2>&1 &\",\n",
    "    shell=True,\n",
    ")\n",
    "\n",
    "print(f\"Server starting in background. Log: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the Server to Be Ready\n",
    "\n",
    "This polls the server until it responds. First run (compilation) takes ~15-25 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:24:10.274041Z",
     "iopub.status.busy": "2026-02-25T04:24:10.273873Z",
     "iopub.status.idle": "2026-02-25T04:44:25.425551Z",
     "shell.execute_reply": "2026-02-25T04:44:25.424797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server to be ready...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server ready! Model: meta-llama/Llama-3.1-8B-Instruct (took 1215s)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Waiting for server to be ready...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        resp = requests.get(f\"{SERVER_URL}/v1/models\", timeout=2)\n",
    "        if resp.status_code == 200:\n",
    "            elapsed = time.time() - start_time\n",
    "            model_id = resp.json()[\"data\"][0][\"id\"]\n",
    "            print(f\"Server ready! Model: {model_id} (took {elapsed:.0f}s)\")\n",
    "            break\n",
    "    except (requests.ConnectionError, requests.Timeout):\n",
    "        pass\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed > 2400:  # 40 min timeout\n",
    "        raise TimeoutError(f\"Server did not start within {elapsed:.0f}s\")\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Regular Text Completion\n",
    "\n",
    "Verify the server works with a standard text prompt. Both regular text and prompt embeddings work on the same compiled model -- no recompilation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:25.427473Z",
     "iopub.status.busy": "2026-02-25T04:44:25.427171Z",
     "iopub.status.idle": "2026-02-25T04:44:27.089465Z",
     "shell.execute_reply": "2026-02-25T04:44:27.088821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"The capital of France is\"\n",
      "Output: \" a city of romance, art, fashion, and cuisine. Paris is a must-visit destination for anyone who loves history, architecture, and culture.\"\n"
     ]
    }
   ],
   "source": [
    "TEST_PROMPT = \"The capital of France is\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{SERVER_URL}/v1/completions\",\n",
    "    json={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": TEST_PROMPT,\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "assert response.status_code == 200, f\"Server error: {response.text}\"\n",
    "baseline_text = response.json()[\"choices\"][0][\"text\"]\n",
    "print(f\"Prompt: \\\"{TEST_PROMPT}\\\"\")\n",
    "print(f\"Output: \\\"{baseline_text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Embeddings from the Model's Embedding Layer\n",
    "\n",
    "To test `prompt_embeds`, we produce embedding vectors that match what the model's internal embedding layer would generate:\n",
    "\n",
    "1. Tokenize the prompt\n",
    "2. Load **only** the embedding weight matrix from safetensors (not the full model)\n",
    "3. Look up the token embeddings\n",
    "\n",
    "Since these are the same embeddings the model would compute internally, the output should match the baseline exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:27.091215Z",
     "iopub.status.busy": "2026-02-25T04:44:27.091065Z",
     "iopub.status.idle": "2026-02-25T04:44:31.825921Z",
     "shell.execute_reply": "2026-02-25T04:44:31.824823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:     \"The capital of France is\"\n",
      "Token IDs:  [128000, 791, 6864, 315, 9822, 374]\n",
      "Tokens:     ['<|begin_of_text|>', 'The', ' capital', ' of', ' France', ' is']\n",
      "Seq length: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from safetensors import safe_open\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokens = tokenizer(TEST_PROMPT, return_tensors=\"pt\")\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "print(f\"Prompt:     \\\"{TEST_PROMPT}\\\"\")\n",
    "print(f\"Token IDs:  {input_ids[0].tolist()}\")\n",
    "print(f\"Tokens:     {[tokenizer.decode(t) for t in input_ids[0]]}\")\n",
    "print(f\"Seq length: {input_ids.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:31.828429Z",
     "iopub.status.busy": "2026-02-25T04:44:31.828112Z",
     "iopub.status.idle": "2026-02-25T04:44:34.855189Z",
     "shell.execute_reply": "2026-02-25T04:44:34.853689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embed_tokens.weight from model-00001-of-00004.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: torch.Size([128256, 4096]) (torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Load only the embedding weight matrix (avoids loading the full 16GB model)\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "model_dirs = glob.glob(os.path.join(\n",
    "    cache_dir,\n",
    "    f\"models--{MODEL_NAME.replace('/', '--')}\",\n",
    "    \"snapshots\", \"*\"\n",
    "))\n",
    "\n",
    "if model_dirs:\n",
    "    model_dir = model_dirs[0]\n",
    "    st_files = sorted(glob.glob(os.path.join(model_dir, \"model-*.safetensors\")))\n",
    "else:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    st_files = [hf_hub_download(MODEL_NAME, \"model-00001-of-00004.safetensors\")]\n",
    "\n",
    "embed_weight = None\n",
    "for st_file in st_files:\n",
    "    with safe_open(st_file, framework=\"pt\") as f:\n",
    "        if \"model.embed_tokens.weight\" in f.keys():\n",
    "            embed_weight = f.get_tensor(\"model.embed_tokens.weight\")\n",
    "            print(f\"Loaded embed_tokens.weight from {os.path.basename(st_file)}\")\n",
    "            break\n",
    "\n",
    "assert embed_weight is not None, \"Could not find embedding weights\"\n",
    "\n",
    "embed_layer = torch.nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embed_layer.weight = torch.nn.Parameter(embed_weight)\n",
    "print(f\"Embedding matrix: {embed_weight.shape} ({embed_weight.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:34.858193Z",
     "iopub.status.busy": "2026-02-25T04:44:34.857998Z",
     "iopub.status.idle": "2026-02-25T04:44:35.057196Z",
     "shell.execute_reply": "2026-02-25T04:44:35.056402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([6, 4096])\n",
      "Embeddings dtype: torch.bfloat16\n",
      "First 5 values:   [0.0002651214599609375, -0.000499725341796875, -0.000583648681640625, 0.00133514404296875, 9.393692016601562e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "with torch.no_grad():\n",
    "    # Cast to bfloat16 to match the Neuron model's computation dtype\n",
    "    embeddings = embed_layer(input_ids).to(torch.bfloat16)\n",
    "\n",
    "# vLLM API expects shape [seq_len, hidden_dim] (no batch dim)\n",
    "prompt_embeds = embeddings.squeeze(0)\n",
    "\n",
    "print(f\"Embeddings shape: {prompt_embeds.shape}\")\n",
    "print(f\"Embeddings dtype: {prompt_embeds.dtype}\")\n",
    "print(f\"First 5 values:   {prompt_embeds[0, :5].tolist()}\")\n",
    "\n",
    "del embed_layer, embed_weight\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Serialize Embeddings for the API\n",
    "\n",
    "The vLLM `prompt_embeds` API expects the tensor serialized as **base64-encoded `torch.save()` bytes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:35.058925Z",
     "iopub.status.busy": "2026-02-25T04:44:35.058742Z",
     "iopub.status.idle": "2026-02-25T04:44:35.066950Z",
     "shell.execute_reply": "2026-02-25T04:44:35.065879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized size: 67,640 characters (base64)\n",
      "Raw tensor size: 49,152 bytes\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "\n",
    "buf = io.BytesIO()\n",
    "torch.save(prompt_embeds, buf)\n",
    "buf.seek(0)\n",
    "b64_data = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "print(f\"Serialized size: {len(b64_data):,} characters (base64)\")\n",
    "print(f\"Raw tensor size: {prompt_embeds.nelement() * prompt_embeds.element_size():,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Send Prompt Embeddings to the Server\n",
    "\n",
    "Send the pre-computed embeddings via the `prompt_embeds` field. The server skips the internal embedding lookup and uses our embeddings directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:35.069179Z",
     "iopub.status.busy": "2026-02-25T04:44:35.069015Z",
     "iopub.status.idle": "2026-02-25T04:44:36.194094Z",
     "shell.execute_reply": "2026-02-25T04:44:36.192827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Results ===\n",
      "Baseline (text):      \" a city of romance, art, fashion, and cuisine. Paris is a must-visit destination for anyone who loves history, architecture, and culture.\"\n",
      "Prompt embeds:        \" a city of romance, art, fashion, and cuisine. Paris is a must-visit destination for anyone who loves history, architecture, and culture.\"\n",
      "\n",
      "PERFECT MATCH -- prompt_embeds produces identical output to text tokens!\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    f\"{SERVER_URL}/v1/completions\",\n",
    "    json={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt_embeds\": b64_data,\n",
    "        \"max_tokens\": 30,\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "assert response.status_code == 200, f\"Server error: {response.text}\"\n",
    "embeds_text = response.json()[\"choices\"][0][\"text\"]\n",
    "\n",
    "print(f\"=== Results ===\")\n",
    "print(f\"Baseline (text):      \\\"{baseline_text}\\\"\")\n",
    "print(f\"Prompt embeds:        \\\"{embeds_text}\\\"\")\n",
    "print()\n",
    "if baseline_text == embeds_text:\n",
    "    print(\"PERFECT MATCH -- prompt_embeds produces identical output to text tokens!\")\n",
    "else:\n",
    "    print(\"OUTPUTS DIFFER -- see notes below for possible causes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Embedding Manipulation Example\n",
    "\n",
    "The real power of `prompt_embeds` is the ability to manipulate embeddings before sending them. Here we blend two prompts by averaging their embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:36.195691Z",
     "iopub.status.busy": "2026-02-25T04:44:36.195526Z",
     "iopub.status.idle": "2026-02-25T04:44:40.529121Z",
     "shell.execute_reply": "2026-02-25T04:44:40.528215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt A: \"The weather in Paris is\" -> shape torch.Size([6, 4096])\n",
      "Prompt B: \"The weather in Tokyo is\" -> shape torch.Size([6, 4096])\n",
      "Blended:  shape torch.Size([6, 4096])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blended output: \" mild and pleasant in the spring, making it an ideal time to visit the city. The average temperature in April is around 12°C (54°F\"\n"
     ]
    }
   ],
   "source": [
    "# Reload embedding layer briefly\n",
    "embed_weight_2 = None\n",
    "for st_file in st_files:\n",
    "    with safe_open(st_file, framework=\"pt\") as f:\n",
    "        if \"model.embed_tokens.weight\" in f.keys():\n",
    "            embed_weight_2 = f.get_tensor(\"model.embed_tokens.weight\")\n",
    "            break\n",
    "\n",
    "embed_layer_2 = torch.nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "embed_layer_2.weight = torch.nn.Parameter(embed_weight_2)\n",
    "\n",
    "prompt_a = \"The weather in Paris is\"\n",
    "prompt_b = \"The weather in Tokyo is\"\n",
    "\n",
    "tokens_a = tokenizer(prompt_a, return_tensors=\"pt\")[\"input_ids\"]\n",
    "tokens_b = tokenizer(prompt_b, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeds_a = embed_layer_2(tokens_a).to(torch.bfloat16).squeeze(0)\n",
    "    embeds_b = embed_layer_2(tokens_b).to(torch.bfloat16).squeeze(0)\n",
    "\n",
    "del embed_layer_2, embed_weight_2\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Prompt A: \\\"{prompt_a}\\\" -> shape {embeds_a.shape}\")\n",
    "print(f\"Prompt B: \\\"{prompt_b}\\\" -> shape {embeds_b.shape}\")\n",
    "\n",
    "if embeds_a.shape == embeds_b.shape:\n",
    "    blended = 0.5 * embeds_a + 0.5 * embeds_b\n",
    "    print(f\"Blended:  shape {blended.shape}\")\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    torch.save(blended, buf)\n",
    "    buf.seek(0)\n",
    "    b64_blended = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "    resp = requests.post(\n",
    "        f\"{SERVER_URL}/v1/completions\",\n",
    "        json={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"prompt_embeds\": b64_blended,\n",
    "            \"max_tokens\": 30,\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        timeout=60,\n",
    "    )\n",
    "    assert resp.status_code == 200, f\"Server error: {resp.text}\"\n",
    "    print(f\"\\nBlended output: \\\"{resp.json()['choices'][0]['text']}\\\"\")\n",
    "else:\n",
    "    print(f\"\\nPrompts have different token lengths ({embeds_a.shape[0]} vs {embeds_b.shape[0]}).\")\n",
    "    print(\"Pad the shorter embedding to blend them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T04:44:40.530913Z",
     "iopub.status.busy": "2026-02-25T04:44:40.530733Z",
     "iopub.status.idle": "2026-02-25T04:44:40.564636Z",
     "shell.execute_reply": "2026-02-25T04:44:40.563649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pkill -f \"vllm.entrypoints.openai.api_server\" 2>/dev/null && echo \"Server stopped.\" || echo \"No server running.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "The Neuron compiler traces a static computation graph, so it cannot use Python-level `if/else` branching to choose between token embeddings and pre-computed embeddings at runtime. Instead, the model **always computes token embeddings** from `input_ids`, then uses `torch.where` to select between the computed embeddings and the provided `inputs_embeds` based on whether the input is non-zero.\n",
    "\n",
    "This means both regular text prompts and prompt embeddings work on the same compiled model with no recompilation.\n",
    "\n",
    "### Performance Note on `torch.where`\n",
    "\n",
    "The `torch.where` approach always runs the token embedding lookup even when pre-computed embeddings are provided -- the embedding table lookup still executes on every request, and its result is simply discarded when `inputs_embeds` is non-zero. For most use cases this overhead is negligible (the embedding lookup is a small fraction of total inference time). However, if your production workload **exclusively** sends pre-computed embeddings and never uses token IDs, a more efficient approach would be to compile a separate model that skips the embedding lookup entirely. That would require a dedicated traced model variant and is not covered in this notebook.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Prefix caching not supported**: Must use `--no-enable-prefix-caching`. Prefix caching is on by default in vLLM v0.13.\n",
    "- **Sequence length**: `prompt_embeds` length must fit within `--max-model-len` and the compiled bucket sizes.\n",
    "- **dtype**: Embeddings should be `torch.bfloat16` to match the model's computation dtype.\n",
    "- **Serialization format**: Must use `torch.save()` + base64 encoding (not JSON lists).\n",
    "- **Tested with**: Llama-3.1-8B-Instruct on inf2.8xlarge. Other model architectures supported by NxD Inference should also work but are untested.\n",
    "\n",
    "### Modified Repositories\n",
    "\n",
    "- **vllm-neuron** `embeddings` branch: [github.com/jimburtoft/vllm-neuron](https://github.com/jimburtoft/vllm-neuron/tree/embeddings)\n",
    "- **neuronx-distributed-inference** `embeddings` branch: [github.com/jimburtoft/neuronx-distributed-inference](https://github.com/jimburtoft/neuronx-distributed-inference/tree/embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
