INFO 02-04 01:53:22 [__init__.py:43] Available plugins for group vllm.platform_plugins:
INFO 02-04 01:53:22 [__init__.py:45] - neuron -> vllm_neuron:register
INFO 02-04 01:53:22 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 02-04 01:53:22 [__init__.py:217] Platform plugin neuron is activated
INFO 02-04 01:53:24 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 02-04 01:53:24 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
Testing Llama 3.3 70B with:
  Context length: 65536
  Batch size: 3
  Prefix caching: DISABLED

Initializing LLM...
INFO 02-04 01:53:25 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 65536, 'tensor_parallel_size': 64, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_seqs': 3, 'disable_log_stats': True, 'model': 'models/Llama-3.3-70B-Instruct/'}
[2026-02-04 01:53:25] INFO platform.py:100: Applying Neuron config overrides
[2026-02-04 01:53:25] INFO platform.py:116: Neuron config overrides applied successfully
INFO 02-04 01:53:25 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 02-04 01:53:26 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-02-04 01:53:26] INFO platform_overrides.py:22: Skipping attention head divisibility check for Neuron platform
[2026-02-04 01:53:26] INFO platform.py:149: Neuron OpenAI serving overrides applied successfully
[2026-02-04 01:53:26] INFO platform.py:241: The custom Neuron scheduler will disable chunked prefill and schedule requests using the continuous batching mechanism, prioritizing prefill over decode.
[2026-02-04 01:53:26] INFO platform.py:254: Neuron custom scheduler default: max_num_batched_tokens set to 131072. Override with --max-num-batched-tokens if needed.
[2026-02-04 01:53:26] WARNING platform.py:280: Pin memory is not supported on Neuron.
[0;36m(EngineCore_DP0 pid=108046)[0;0m INFO 02-04 01:53:26 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='models/Llama-3.3-70B-Instruct/', speculative_config=None, tokenizer='models/Llama-3.3-70B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=64, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=models/Llama-3.3-70B-Instruct/, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:27] INFO initializer.py:82: PJRT_DEVICE not set, defaulting to NEURON
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronxcc/nki/_pre_prod_kernels/bwmm_mxfp4.py:564: SyntaxWarning: assertion is always true, perhaps remove parentheses?
[0;36m(EngineCore_DP0 pid=108046)[0;0m   assert(token_indices_2D.shape==(128, 1), f"Expect token_indices_2D to have shape (128, 1), got {token_indices_2D.shape}")
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:76: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private.blockwise_matmul_while'
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(f"Warning: {error}")
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/attention/utils.py:13: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=108046)[0;0m WARNING 02-04 01:53:29 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:1: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.models.dbrx.modeling_dbrx import NeuronDbrxForCausalLM
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/llama4/modeling_llama4_vision.py:62: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.models.mllama.modeling_mllama_vision import (
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:5: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.models.mixtral.modeling_mixtral import NeuronMixtralForCausalLM
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   from neuronx_distributed_inference.models.qwen3_moe.modeling_qwen3_moe import NeuronQwen3MoeForCausalLM
[0;36m(EngineCore_DP0 pid=108046)[0;0m INFO 02-04 01:53:29 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.31.23.210:56989 backend=gloo
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=108046)[0;0m INFO 02-04 01:53:29 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=108046)[0;0m WARNING 02-04 01:53:29 [vllm.py:1403] Current vLLM config is not set.
[0;36m(EngineCore_DP0 pid=108046)[0;0m INFO 02-04 01:53:29 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO neuronx_distributed_model_loader.py:797: No neuron overrides are passed via additional_config: {}. Proceeding with defaults.
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] WARNING neuronx_distributed_model_loader.py:210: Exception: [Errno 2] No such file or directory: 'models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/6361a5e018bf93a55a3ae0ba9f00fe04/neuron_config.json'
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] WARNING neuronx_distributed_model_loader.py:211: Unable to find precompiled artifacts from models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/6361a5e018bf93a55a3ae0ba9f00fe04. Recompiling...
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO model_wrapper.py:168: neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true' 
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO model_wrapper.py:168: neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true' 
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO application_base.py:300: Saving the neuron_config to models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/6361a5e018bf93a55a3ae0ba9f00fe04/
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO model_builder.py:549: Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.959: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 64
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.959: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.959: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.959: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.959: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 64
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.960: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7e5c12910b80>, 'Ascending Ring PG Group')>
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29.962: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO model_builder.py:575: Generating 10 hlos for key: context_encoding_model
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:29] INFO model_builder.py:929: Minimal metadata will be added to HLO
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:30] INFO model_builder.py:858: Started loading module context_encoding_model
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:30] INFO model_builder.py:861: Finished loading module context_encoding_model in 0.4333522319793701 seconds
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:30] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 128])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/attention/attention_base.py:1353: UserWarning: Flash attention disabled. For flash attn to be performant, LNC2 requires context_len >= 1024 to be divisible by 512, or context_len < 1024 to be divisible by 256
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 128]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:36] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 5.878339767456055 seconds, input example shape = torch.Size([1, 128])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:36] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 256])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 256]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:39] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 2.7173011302948 seconds, input example shape = torch.Size([1, 256])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:39] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:41] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 2.848973512649536 seconds, input example shape = torch.Size([1, 512])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:41] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 1024])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 1024]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:44] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.045419931411743 seconds, input example shape = torch.Size([1, 1024])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:44] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 2048])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 2048]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:48] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.308795213699341 seconds, input example shape = torch.Size([1, 2048])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:48] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 4096])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 4096]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:52] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.9796957969665527 seconds, input example shape = torch.Size([1, 4096])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:52] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 8192])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 8192]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:57] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 5.655371904373169 seconds, input example shape = torch.Size([1, 8192])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:53:57] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 16384])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 16384]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:05] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 7.408066749572754 seconds, input example shape = torch.Size([1, 16384])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:05] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 32768])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 32768]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:16] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 11.246586799621582 seconds, input example shape = torch.Size([1, 32768])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:16] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 65536])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 65536]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:37] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 21.02203059196472 seconds, input example shape = torch.Size([1, 65536])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:37] INFO model_builder.py:575: Generating 10 hlos for key: token_generation_model
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:37] INFO model_builder.py:929: Minimal metadata will be added to HLO
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:37] INFO model_builder.py:858: Started loading module token_generation_model
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:38] INFO model_builder.py:861: Finished loading module token_generation_model in 0.42769527435302734 seconds
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:38] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:42] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 4.134905576705933 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:42] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:45] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.170523166656494 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:45] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:48] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.216888427734375 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:48] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:51] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.184825897216797 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:51] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:55] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.2005090713500977 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:55] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:58] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.1935839653015137 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:54:58] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:01] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.2084131240844727 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:01] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:04] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.199291706085205 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:04] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:07] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.209658145904541 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:07] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([3]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:11] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.239764928817749 seconds, input example shape = torch.Size([3, 1])
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:11] INFO model_builder.py:600: Generated all HLOs in 101.71174049377441 seconds
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:11] INFO model_builder.py:632: Starting compilation for the priority HLO
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:55:11] INFO model_builder.py:635: 'token_generation_model' is the priority model with bucket rank 0
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:284: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
...........Completed run_backend_driver.
.
Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:58:52.000846:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_6489f5a3c4552048a8c0+2a404f8d.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:52] INFO model_builder.py:678: Done compilation for the priority HLO in 221.21896743774414 seconds
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:53] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:53] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:54] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:54] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:55] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:55] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:56] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:57] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:58:58] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:00] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:01] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:02] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:02] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:03] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:03] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:03] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:04] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:04] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:05] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:05] INFO model_builder.py:1199: Done optimizing weight layout for all HLOs in 12.449440479278564 seconds
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:05] INFO model_builder.py:708: Starting compilation for all HLOs
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:05] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:05] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk1/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:06] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk2/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
.[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:06] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk3/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:07] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk4/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
.[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:08] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk5/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
.[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:09] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk6/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
.[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:12] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk7/log-neuron-cc.txt
Completed run_backend_driver.
..
Compiler status PASS
Completed run_backend_driver.

Compiler status PASS
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:15.000716:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_75717678c213d5f11b19+390b6f80.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:15.000717:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_5ff9395d9ec50756a1fa+768db271.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:15.000718:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_d76f503847556eaeb671+16a01a30.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:16] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk8/log-neuron-cc.txt
.Completed run_backend_driver.

Compiler status PASS
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:22.000841:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_17cb6dad982f9940ddd6+a5d45b9d.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:23.000123:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_cd60411cbb9fe08e1540+724a9fcd.hlo_module.pb
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:24] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk9/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:24] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk1/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:24] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk2/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=108046)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:24] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk3/log-neuron-cc.txt
.[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:25] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk4/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:25] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk5/log-neuron-cc.txt
Completed run_backend_driver.
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:25] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk6/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:26] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk7/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:26] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk8/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=108046)[0;0m [2026-02-04 01:59:26] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --internal-disable-fma-on-ios  --disable-mixed-precision-accumulation  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 --hbm-scratchpad-page-size=1024  -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk9/log-neuron-cc.txt
.........
Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:31.000746:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_2d451ecb0988d42d254a+18ac7767.hlo_module.pb
...Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 01:59:42.000076:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_8e08b5378a41cb17490a+c4011b55.hlo_module.pb
........................Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 02:00:19.000871:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_c71efe8af4693b10bd22+68f7cbc2.hlo_module.pb
.............................................Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=108046)[0;0m 2026-02-04 02:01:49.000854:  108046  [INFO]: Compilation Successfully Completed for model.MODULE_a54d96fc45fa8539a87e+ae4dfeb0.hlo_module.pb
.................................................