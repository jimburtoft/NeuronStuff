INFO 02-04 00:09:08 [__init__.py:43] Available plugins for group vllm.platform_plugins:
INFO 02-04 00:09:08 [__init__.py:45] - neuron -> vllm_neuron:register
INFO 02-04 00:09:08 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 02-04 00:09:08 [__init__.py:217] Platform plugin neuron is activated
INFO 02-04 00:09:09 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 02-04 00:09:09 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 02-04 00:09:10 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 16384, 'tensor_parallel_size': 64, 'block_size': 16384, 'enable_prefix_caching': False, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_chunked_prefill': False, 'additional_config': {'override_neuron_config': {'batch_size': 1, 'tp_degree': 64, 'enable_bucketing': True, 'is_continuous_batching': True, 'logical_nc_config': 2, 'seq_len': 16384, 'torch_dtype': 'bfloat16'}}, 'model': 'models/Llama-3.3-70B-Instruct/'}
[2026-02-04 00:09:10] INFO platform.py:100: Applying Neuron config overrides
[2026-02-04 00:09:10] INFO platform.py:116: Neuron config overrides applied successfully
INFO 02-04 00:09:10 [model.py:514] Resolved architecture: LlamaForCausalLM
WARNING 02-04 00:09:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-02-04 00:09:11] INFO platform_overrides.py:22: Skipping attention head divisibility check for Neuron platform
[2026-02-04 00:09:11] INFO platform.py:149: Neuron OpenAI serving overrides applied successfully
[2026-02-04 00:09:11] INFO platform.py:241: The custom Neuron scheduler will disable chunked prefill and schedule requests using the continuous batching mechanism, prioritizing prefill over decode.
[2026-02-04 00:09:11] INFO platform.py:254: Neuron custom scheduler default: max_num_batched_tokens set to 131072. Override with --max-num-batched-tokens if needed.
[2026-02-04 00:09:12] WARNING platform.py:280: Pin memory is not supported on Neuron.
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:09:12 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='models/Llama-3.3-70B-Instruct/', speculative_config=None, tokenizer='models/Llama-3.3-70B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=64, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=models/Llama-3.3-70B-Instruct/, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:10:41] INFO initializer.py:82: PJRT_DEVICE not set, defaulting to NEURON
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from .mappings import (
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronxcc/nki/_pre_prod_kernels/bwmm_mxfp4.py:564: SyntaxWarning: assertion is always true, perhaps remove parentheses?
[0;36m(EngineCore_DP0 pid=23695)[0;0m   assert(token_indices_2D.shape==(128, 1), f"Expect token_indices_2D to have shape (128, 1), got {token_indices_2D.shape}")
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:76: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private.blockwise_matmul_while'
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(f"Warning: {error}")
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   component, error = import_nki(config)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/attention/utils.py:13: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
[0;36m(EngineCore_DP0 pid=23695)[0;0m WARNING 02-04 00:10:59 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/dbrx/modeling_dbrx.py:38: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:1: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.models.dbrx.modeling_dbrx import NeuronDbrxForCausalLM
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/models/llama4/modeling_llama4_vision.py:62: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.models.mllama.modeling_mllama_vision import (
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:5: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.models.mixtral.modeling_mixtral import NeuronMixtralForCausalLM
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/utils/constants.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   from neuronx_distributed_inference.models.qwen3_moe.modeling_qwen3_moe import NeuronQwen3MoeForCausalLM
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:11:03 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.31.23.210:39445 backend=gloo
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:11:03 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=23695)[0;0m WARNING 02-04 00:11:03 [vllm.py:1403] Current vLLM config is not set.
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:11:03 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO neuronx_distributed_model_loader.py:792: Retrieved override_neuron_config from additional_config: {'batch_size': 1, 'tp_degree': 64, 'enable_bucketing': True, 'is_continuous_batching': True, 'logical_nc_config': 2, 'seq_len': 16384, 'torch_dtype': 'bfloat16'}
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] WARNING neuronx_distributed_model_loader.py:210: Exception: [Errno 2] No such file or directory: 'models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/061d4f2155e774f31703b998fef7ff27/neuron_config.json'
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] WARNING neuronx_distributed_model_loader.py:211: Unable to find precompiled artifacts from models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/061d4f2155e774f31703b998fef7ff27. Recompiling...
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_wrapper.py:168: neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true' 
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_wrapper.py:168: neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true' 
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO application_base.py:300: Saving the neuron_config to models/Llama-3.3-70B-Instruct/neuron-compiled-artifacts/061d4f2155e774f31703b998fef7ff27/
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_builder.py:549: Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 64
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 64
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.867: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7cc7ee130900>, 'Ascending Ring PG Group')>
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03.868: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_builder.py:575: Generating 8 hlos for key: context_encoding_model
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_builder.py:929: Minimal metadata will be added to HLO
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:03] INFO model_builder.py:858: Started loading module context_encoding_model
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:04] INFO model_builder.py:861: Finished loading module context_encoding_model in 0.20157742500305176 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:04] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 128])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/attention/attention_base.py:1353: UserWarning: Flash attention disabled. For flash attn to be performant, LNC2 requires context_len >= 1024 to be divisible by 512, or context_len < 1024 to be divisible by 256
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 128]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:15] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 11.176246166229248 seconds, input example shape = torch.Size([1, 128])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:15] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 256])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 256]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:17] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 2.7106661796569824 seconds, input example shape = torch.Size([1, 256])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:17] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:20] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 2.83345890045166 seconds, input example shape = torch.Size([1, 512])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:20] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 1024])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 1024]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:23] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.0205085277557373 seconds, input example shape = torch.Size([1, 1024])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:23] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 2048])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 2048]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:27] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.3304567337036133 seconds, input example shape = torch.Size([1, 2048])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:27] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 4096])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 4096]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:31] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 3.9467883110046387 seconds, input example shape = torch.Size([1, 4096])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:31] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 8192])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 8192]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:36] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 5.595075607299805 seconds, input example shape = torch.Size([1, 8192])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:36] INFO model_builder.py:886: generating HLO: context_encoding_model, input example shape = torch.Size([1, 16384])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 16384]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:900: Finished generating HLO for context_encoding_model in 7.437129259109497 seconds, input example shape = torch.Size([1, 16384])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:575: Generating 8 hlos for key: token_generation_model
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:929: Minimal metadata will be added to HLO
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:858: Started loading module token_generation_model
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:861: Finished loading module token_generation_model in 0.18459367752075195 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:44] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:47] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.167741060256958 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:47] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:50] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.1151649951934814 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:50] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:53] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.114339828491211 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:53] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:57] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.125929117202759 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:11:57] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:00] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.112422466278076 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:00] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:03] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.544651746749878 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:03] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:06] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.1476714611053467 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:06] INFO model_builder.py:886: generating HLO: token_generation_model, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   with torch.cuda.amp.autocast(enabled=False):
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:374: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:327: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:10] INFO model_builder.py:900: Finished generating HLO for token_generation_model in 3.1683008670806885 seconds, input example shape = torch.Size([1, 1])
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:10] INFO model_builder.py:600: Generated all HLOs in 66.27762365341187 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:10] INFO model_builder.py:632: Starting compilation for the priority HLO
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:12:10] INFO model_builder.py:635: 'token_generation_model' is the priority model with bucket rank 0
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:284: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
..........Completed run_backend_driver.
.
Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:15:35.000361:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_607ca23f898f27552164+8f34053f.hlo_module.pb
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:35] INFO model_builder.py:678: Done compilation for the priority HLO in 205.2515480518341 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:43] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:44] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:44] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:44] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:45] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:46] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:46] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:47] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:48] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:48] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:49] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:49] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:49] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:50] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:50] INFO model_builder.py:1190: Updating the hlo module with optimized layout
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:50] INFO model_builder.py:1199: Done optimizing weight layout for all HLOs in 15.457515239715576 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:50] INFO model_builder.py:708: Starting compilation for all HLOs
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:51] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:51] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk1/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:51] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk2/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:52] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk3/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
.[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:52] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk4/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
.[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:53] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk5/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
.[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:55] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk6/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:57] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=2 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk7/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
..[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:57] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk1/log-neuron-cc.txt
.[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:58] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk2/log-neuron-cc.txt
Completed run_backend_driver.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:58] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk3/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:58] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk4/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:59] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk5/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:59] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk6/log-neuron-cc.txt
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(SyntaxWarning(
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:15:59] INFO model_builder.py:733: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=2 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true'  --verbose=35 --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk7/log-neuron-cc.txt
..
Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:02.000250:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_9c39b7fde5188d38e2f3+b35fee7e.hlo_module.pb
.Completed run_backend_driver.
.....
Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:03.000683:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_e769b72805dc5a155d0a+4e79e89b.hlo_module.pb
.Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:05.000159:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_7b3d7a0aa3156116a871+3d4a12eb.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:09.000183:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_7c62b296d03edecd9291+8dd23d0c.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:10.000615:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_362e1c05ca4a350af439+42a8ac25.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:13.000913:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_4f0a031c28d139c2ffca+0ba9c0cf.hlo_module.pb
........Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:23.000803:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_8784d96c55edcd2748c2+1e783417.hlo_module.pb
.........Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:16:49.000586:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_09a144203578077be03d+aef0bcf9.hlo_module.pb
...............................................................Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:19:45.000541:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_b42233f1baa93281c614+610f5891.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:19:48.000207:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_07ddbe71ca94d50e93a3+769d2bfd.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:19:54.000071:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_353ed8eb260ac402b959+ed5433ae.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:19:54.000624:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_a67bb66c9330159082e6+d2076f45.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:19:59.000354:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_a5d9ca729b9ec2b15e45+2dad96a5.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:20:02.000034:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_f18075b369dfad77a95c+42fbac26.hlo_module.pb
.Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m 2026-02-04 00:20:10.000849:  23695  [INFO]: Compilation Successfully Completed for model.MODULE_102ea3f9f9c73dea7c2f+d13e65b6.hlo_module.pb
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:20:11] INFO model_builder.py:760: Finished Compilation for all HLOs in 260.1803171634674 seconds
.....Completed run_backend_driver.

Compiler status PASS
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:38] INFO model_builder.py:1314: Done preparing weight layout transformation
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:38] INFO model_builder.py:783: Finished building model in 634.6468484401703 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:39] INFO application_base.py:249: SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40] INFO application_base.py:404: Sharding weights on load...
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40] INFO model_builder.py:808: Sharding weights for ranks: 0...63
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 64
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 64
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.665: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7cc7ee130900>, 'Ascending Ring PG Group')>
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:40.666: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63]]
[0;36m(EngineCore_DP0 pid=23695)[0;0m /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/lib/python3.12/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: ['layers.72.self_attn.k_proj.weight', 'layers.72.self_attn.o_proj.weight', 'layers.72.self_attn.q_proj.weight', 'layers.72.self_attn.v_proj.weight', 'layers.73.self_attn.k_proj.weight', 'layers.73.self_attn.o_proj.weight', 'layers.73.self_attn.q_proj.weight', 'layers.73.self_attn.v_proj.weight', 'layers.74.self_attn.k_proj.weight', 'layers.74.self_attn.o_proj.weight', 'layers.74.self_attn.q_proj.weight', 'layers.74.self_attn.v_proj.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.self_attn.k_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.33.self_attn.k_proj.weight', 'layers.33.self_attn.o_proj.weight', 'layers.33.self_attn.q_proj.weight', 'layers.33.self_attn.v_proj.weight', 'layers.34.self_attn.k_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.34.self_attn.q_proj.weight', 'layers.34.self_attn.v_proj.weight', 'layers.35.self_attn.k_proj.weight', 'layers.35.self_attn.q_proj.weight', 'layers.35.self_attn.v_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.63.self_attn.o_proj.weight', 'layers.64.self_attn.k_proj.weight', 'layers.64.self_attn.o_proj.weight', 'layers.64.self_attn.q_proj.weight', 'layers.64.self_attn.v_proj.weight', 'layers.65.self_attn.k_proj.weight', 'layers.65.self_attn.o_proj.weight', 'layers.65.self_attn.q_proj.weight', 'layers.65.self_attn.v_proj.weight', 'layers.52.self_attn.k_proj.weight', 'layers.52.self_attn.o_proj.weight', 'layers.52.self_attn.q_proj.weight', 'layers.52.self_attn.v_proj.weight', 'layers.53.self_attn.k_proj.weight', 'layers.53.self_attn.o_proj.weight', 'layers.53.self_attn.q_proj.weight', 'layers.53.self_attn.v_proj.weight', 'layers.54.self_attn.k_proj.weight', 'layers.54.self_attn.o_proj.weight', 'layers.54.self_attn.q_proj.weight', 'layers.54.self_attn.v_proj.weight', 'layers.75.self_attn.k_proj.weight', 'layers.75.self_attn.o_proj.weight', 'layers.75.self_attn.q_proj.weight', 'layers.75.self_attn.v_proj.weight', 'layers.76.self_attn.k_proj.weight', 'layers.76.self_attn.o_proj.weight', 'layers.76.self_attn.q_proj.weight', 'layers.76.self_attn.v_proj.weight', 'layers.77.self_attn.k_proj.weight', 'layers.77.self_attn.q_proj.weight', 'layers.77.self_attn.v_proj.weight', 'layers.77.self_attn.o_proj.weight', 'layers.78.self_attn.k_proj.weight', 'layers.78.self_attn.o_proj.weight', 'layers.78.self_attn.q_proj.weight', 'layers.78.self_attn.v_proj.weight', 'layers.79.self_attn.k_proj.weight', 'layers.79.self_attn.o_proj.weight', 'layers.79.self_attn.q_proj.weight', 'layers.79.self_attn.v_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.49.self_attn.o_proj.weight', 'layers.50.self_attn.k_proj.weight', 'layers.50.self_attn.o_proj.weight', 'layers.50.self_attn.q_proj.weight', 'layers.50.self_attn.v_proj.weight', 'layers.51.self_attn.k_proj.weight', 'layers.51.self_attn.o_proj.weight', 'layers.51.self_attn.q_proj.weight', 'layers.51.self_attn.v_proj.weight', 'layers.61.self_attn.k_proj.weight', 'layers.61.self_attn.o_proj.weight', 'layers.61.self_attn.q_proj.weight', 'layers.61.self_attn.v_proj.weight', 'layers.62.self_attn.k_proj.weight', 'layers.62.self_attn.o_proj.weight', 'layers.62.self_attn.q_proj.weight', 'layers.62.self_attn.v_proj.weight', 'layers.63.self_attn.k_proj.weight', 'layers.63.self_attn.q_proj.weight', 'layers.63.self_attn.v_proj.weight', 'layers.38.self_attn.k_proj.weight', 'layers.38.self_attn.o_proj.weight', 'layers.38.self_attn.q_proj.weight', 'layers.38.self_attn.v_proj.weight', 'layers.39.self_attn.k_proj.weight', 'layers.39.self_attn.o_proj.weight', 'layers.39.self_attn.q_proj.weight', 'layers.39.self_attn.v_proj.weight', 'layers.40.self_attn.k_proj.weight', 'layers.40.self_attn.o_proj.weight', 'layers.40.self_attn.q_proj.weight', 'layers.40.self_attn.v_proj.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.66.self_attn.k_proj.weight', 'layers.66.self_attn.o_proj.weight', 'layers.66.self_attn.q_proj.weight', 'layers.66.self_attn.v_proj.weight', 'layers.67.self_attn.k_proj.weight', 'layers.67.self_attn.o_proj.weight', 'layers.67.self_attn.q_proj.weight', 'layers.67.self_attn.v_proj.weight', 'layers.68.self_attn.k_proj.weight', 'layers.68.self_attn.o_proj.weight', 'layers.68.self_attn.q_proj.weight', 'layers.68.self_attn.v_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.41.self_attn.k_proj.weight', 'layers.41.self_attn.o_proj.weight', 'layers.41.self_attn.q_proj.weight', 'layers.41.self_attn.v_proj.weight', 'layers.42.self_attn.k_proj.weight', 'layers.42.self_attn.o_proj.weight', 'layers.42.self_attn.q_proj.weight', 'layers.42.self_attn.v_proj.weight', 'layers.43.self_attn.k_proj.weight', 'layers.43.self_attn.o_proj.weight', 'layers.43.self_attn.q_proj.weight', 'layers.43.self_attn.v_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.32.self_attn.k_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.32.self_attn.q_proj.weight', 'layers.32.self_attn.v_proj.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.47.self_attn.k_proj.weight', 'layers.47.self_attn.o_proj.weight', 'layers.47.self_attn.q_proj.weight', 'layers.47.self_attn.v_proj.weight', 'layers.48.self_attn.k_proj.weight', 'layers.48.self_attn.o_proj.weight', 'layers.48.self_attn.q_proj.weight', 'layers.48.self_attn.v_proj.weight', 'layers.49.self_attn.k_proj.weight', 'layers.49.self_attn.q_proj.weight', 'layers.49.self_attn.v_proj.weight', 'layers.35.self_attn.o_proj.weight', 'layers.36.self_attn.k_proj.weight', 'layers.36.self_attn.o_proj.weight', 'layers.36.self_attn.q_proj.weight', 'layers.36.self_attn.v_proj.weight', 'layers.37.self_attn.k_proj.weight', 'layers.37.self_attn.o_proj.weight', 'layers.37.self_attn.q_proj.weight', 'layers.37.self_attn.v_proj.weight', 'layers.44.self_attn.k_proj.weight', 'layers.44.self_attn.o_proj.weight', 'layers.44.self_attn.q_proj.weight', 'layers.44.self_attn.v_proj.weight', 'layers.45.self_attn.k_proj.weight', 'layers.45.self_attn.o_proj.weight', 'layers.45.self_attn.q_proj.weight', 'layers.45.self_attn.v_proj.weight', 'layers.46.self_attn.k_proj.weight', 'layers.46.self_attn.o_proj.weight', 'layers.46.self_attn.q_proj.weight', 'layers.46.self_attn.v_proj.weight', 'layers.55.self_attn.k_proj.weight', 'layers.55.self_attn.o_proj.weight', 'layers.55.self_attn.q_proj.weight', 'layers.55.self_attn.v_proj.weight', 'layers.56.self_attn.k_proj.weight', 'layers.56.self_attn.o_proj.weight', 'layers.56.self_attn.q_proj.weight', 'layers.56.self_attn.v_proj.weight', 'layers.57.self_attn.k_proj.weight', 'layers.57.self_attn.o_proj.weight', 'layers.57.self_attn.q_proj.weight', 'layers.57.self_attn.v_proj.weight', 'layers.58.self_attn.k_proj.weight', 'layers.58.self_attn.o_proj.weight', 'layers.58.self_attn.q_proj.weight', 'layers.58.self_attn.v_proj.weight', 'layers.59.self_attn.k_proj.weight', 'layers.59.self_attn.o_proj.weight', 'layers.59.self_attn.q_proj.weight', 'layers.59.self_attn.v_proj.weight', 'layers.60.self_attn.k_proj.weight', 'layers.60.self_attn.o_proj.weight', 'layers.60.self_attn.q_proj.weight', 'layers.60.self_attn.v_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.69.self_attn.k_proj.weight', 'layers.69.self_attn.o_proj.weight', 'layers.69.self_attn.q_proj.weight', 'layers.69.self_attn.v_proj.weight', 'layers.70.self_attn.k_proj.weight', 'layers.70.self_attn.o_proj.weight', 'layers.70.self_attn.q_proj.weight', 'layers.70.self_attn.v_proj.weight', 'layers.71.self_attn.k_proj.weight', 'layers.71.self_attn.o_proj.weight', 'layers.71.self_attn.q_proj.weight', 'layers.71.self_attn.v_proj.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight']
[0;36m(EngineCore_DP0 pid=23695)[0;0m   warnings.warn(f"Removing redundant keys from checkpoint: {keys_to_delete}")
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:21:52] INFO model_builder.py:842: Done Sharding weights in 11.824388592000105
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:24:34] INFO application_base.py:419: Finished weights loading in 173.60283024700038 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:24:40] INFO application_base.py:350: Warming up the model.
2026-Feb-04 00:24:41.0061 23695:35929 [54] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):219 CCOM WARN NET/OFI Failed to initialize rdma protocol
2026-Feb-04 00:24:41.0063 23695:35929 [54] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed
2026-Feb-04 00:24:41.0065 23695:35929 [54] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed
2026-Feb-04 00:24:41.0067 23695:35929 [54] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:25:18] INFO application_base.py:372: Warmup completed in 37.85233950614929 seconds.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:25:18] INFO neuronx_distributed_model_runner.py:518: Hardware sampling enabled: config=<neuronx_distributed_inference.models.config.OnDeviceSamplingConfig object at 0x7cc7ed2eeae0>
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:25:18 [kv_cache_utils.py:1291] GPU KV cache size: 393,216 tokens
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:25:18 [kv_cache_utils.py:1296] Maximum concurrency for 16,384 tokens per request: 24.00x
[0;36m(EngineCore_DP0 pid=23695)[0;0m INFO 02-04 00:25:18 [core.py:259] init engine (profile, create kv cache, warmup model) took 0.00 seconds
[0;36m(EngineCore_DP0 pid=23695)[0;0m WARNING 02-04 00:25:18 [scheduler.py:170] Using custom scheduler class vllm_neuron.core.scheduler.ContinuousBatchingNeuronScheduler. This scheduler interface is not public and compatibility may not be maintained.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:25:19] INFO platform_overrides.py:22: Skipping attention head divisibility check for Neuron platform
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:25:19] INFO platform.py:241: The custom Neuron scheduler will disable chunked prefill and schedule requests using the continuous batching mechanism, prioritizing prefill over decode.
[0;36m(EngineCore_DP0 pid=23695)[0;0m [2026-02-04 00:25:19] INFO platform.py:254: Neuron custom scheduler default: max_num_batched_tokens set to 131072. Override with --max-num-batched-tokens if needed.
INFO 02-04 00:25:19 [llm.py:360] Supported tasks: ['generate']
Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]Adding requests: 100%|██████████| 3/3 [00:00<00:00, 222.27it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|███▎      | 1/3 [00:00<00:00,  3.99it/s, est. speed input: 31.92 toks/s, output: 63.84 toks/s]Processed prompts:  67%|██████▋   | 2/3 [00:00<00:00,  4.33it/s, est. speed input: 29.90 toks/s, output: 68.35 toks/s]Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  4.45it/s, est. speed input: 29.17 toks/s, output: 70.01 toks/s]Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  4.45it/s, est. speed input: 29.17 toks/s, output: 70.01 toks/s]Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  4.37it/s, est. speed input: 29.17 toks/s, output: 70.01 toks/s]
Prompt: 'The president of the United States is', Generated text: ' the head of state and head of government of the United States. The president is'
Prompt: 'The capital of France is', Generated text: ' a city that is steeped in history, art, fashion, and culture.'
Prompt: 'The future of AI is', Generated text: ' not just about machines, but about humans\nThe future of AI is not just'
