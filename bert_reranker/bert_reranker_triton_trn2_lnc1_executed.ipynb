{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Reranker Inference on AWS Trainium2 with Triton Inference Server (LNC=1)\n",
    "\n",
    "This notebook deploys the [Alibaba-NLP/gte-multilingual-reranker-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base)\n",
    "BERT cross-encoder reranker (~306M parameters) on an AWS **trn2.3xlarge** instance using\n",
    "NVIDIA Triton Inference Server with the AWS Neuron SDK, then benchmarks throughput and latency.\n",
    "\n",
    "**This is the LNC=1 variant.** With LNC=1, each physical NeuronCore becomes one logical core,\n",
    "giving **8 logical cores** on trn2.3xlarge. Each core has 12 GB of memory, which is sufficient\n",
    "for this model (~718 MB compiled).\n",
    "\n",
    "Everything is self-contained: the notebook writes all required files (Dockerfile, Triton config,\n",
    "Python backend, compile script, benchmark client), builds the Docker image, compiles the models,\n",
    "starts the server, and runs the benchmark.\n",
    "\n",
    "**Instance**: trn2.3xlarge (1 Neuron device, 8 logical NeuronCores at LNC=1)  \n",
    "**Time to complete**: ~70-90 minutes (compilation dominates on first run)  \n",
    "**Prerequisites**: Deep Learning AMI Neuron (Ubuntu 24.04), Docker installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We measure throughput (inferences/second) and latency (P50, P95, P99) under multiple load patterns,\n",
    "varying both client-side batch size and request concurrency to exercise Triton's dynamic batching.\n",
    "\n",
    "- **Input**: Query-passage pairs, tokenized and padded to 1024 tokens (`input_ids` + `attention_mask`)\n",
    "- **Output**: Single float reranking score per sample (logit from cross-encoder head)\n",
    "- **Duration**: 10 seconds per test configuration\n",
    "- **Warmup**: 5 requests before timing\n",
    "- **Concurrency**: Python threads, one HTTP client per thread\n",
    "\n",
    "### Why LNC=1?\n",
    "\n",
    "LNC=1 was proven optimal for this model in Task 4 benchmarks:\n",
    "\n",
    "| Property | LNC=2 | LNC=1 |\n",
    "|----------|-------|-------|\n",
    "| Logical cores | 4 | **8** |\n",
    "| Physical cores per logical core | 2 | **1** |\n",
    "| Memory per logical core | 24 GB | **12 GB** |\n",
    "| Triton model instances | 4 | **8** |\n",
    "| Single-core throughput | 51.31 qps | **83.63 qps** |\n",
    "| Total throughput (DataParallel) | 102.45 qps | **166.87 qps** |\n",
    "| Per-physical-core efficiency | 1x | **3.3x** |\n",
    "\n",
    "### Neuron-Specific: Per-Batch-Size Compilation\n",
    "\n",
    "Unlike GPUs, Neuron models are compiled for a **fixed batch size**. Running a BS=1 request through\n",
    "a BS=16 compiled model wastes 15/16 of the compute. We compile separate models for BS=1, 2, 4, 8, 16\n",
    "and the Triton Python backend dispatches each request to the smallest model that fits.\n",
    "\n",
    "**Critical**: Models compiled with `--lnc 1` are **not interchangeable** with `--lnc 2` models.\n",
    "The `--lnc` compiler flag and `NEURON_LOGICAL_NC_CONFIG` environment variable must always match.\n",
    "\n",
    "### Triton Configuration\n",
    "\n",
    "- **8 model instances** (one per logical NeuronCore), each pinned via `NEURON_RT_VISIBLE_CORES`\n",
    "- **Dynamic batching**: preferred sizes [4, 8, 16], max queue delay 5ms\n",
    "- **Python backend**: Triton has no native Neuron backend, so we use the Python backend with `torch_neuronx`\n",
    "\n",
    "### Test Matrix\n",
    "\n",
    "| Test | Concurrency | Client Batch Sizes |\n",
    "|------|-------------|--------------------|\n",
    "| Baseline | 1 (no batching) | 1 |\n",
    "| Dynamic batching | 16, 32, 64 | 1, 2, 4, 8, 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Instance Specifications\n",
    "\n",
    "| Spec | Value |\n",
    "|------|-------|\n",
    "| Model | Alibaba-NLP/gte-multilingual-reranker-base (~306M params) |\n",
    "| Architecture | 12-layer NewBERT cross-encoder, hidden size 768 |\n",
    "| Task | Cross-encoder reranking (query + passage -> relevance score) |\n",
    "| Input | `input_ids`, `attention_mask` (1024 tokens, padded) |\n",
    "| Output | Single float logit score per sample |\n",
    "| Compiled model size | ~718 MB per batch size variant (LNC=1) |\n",
    "| Instance type | trn2.3xlarge |\n",
    "| Neuron device | 1 device, 8 physical NeuronCores |\n",
    "| LNC config | **1 (= 8 logical cores, 12 GB each)** |\n",
    "| vCPUs / RAM | 12 / 128 GB |\n",
    "\n",
    "### Software Stack\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|--------|\n",
    "| Neuron SDK | 2.27.1 |\n",
    "| PyTorch / torch-neuronx | 2.9.0 / 2.9.0.2.11 |\n",
    "| neuronx-cc (compiler) | 2.22.12471 |\n",
    "| Transformers | 4.48.0 |\n",
    "| Triton Inference Server | 2.65.0 (r26.01, built from source) |\n",
    "| Python | 3.12 |\n",
    "| OS / AMI | Ubuntu 24.04 / Deep Learning AMI Neuron 20260126 |\n",
    "\n",
    "> **Important**: Transformers versions 4.54.0+ have a confirmed ~20% performance regression for\n",
    "> this model family on Neuron. Use versions 4.48.0 through 4.53.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Setup & Environment Check\n",
    "\n",
    "After you deploy the instance using the Ubuntu Neuron Deep Learning AMI (with all the Neuron drivers installed), run this notebook inside the pre-installed PyTorch 2.9 Neuron virtual environment:\n",
    "\n",
    "```bash\n",
    "source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser\n",
    "```\n",
    "\n",
    "Alternatively, if you are running from within a remote VSCode instance, you can use `ln -s /opt/aws_neuronx_venv_pytorch_2_9/bin/activate ~/.venv` to help VSCode find your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:22:35.002711Z",
     "iopub.status.busy": "2026-02-26T04:22:35.002587Z",
     "iopub.status.idle": "2026-02-26T04:22:40.279796Z",
     "shell.execute_reply": "2026-02-26T04:22:40.279139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu128\n",
      "torch-neuronx: 2.9.0.2.11.19912+e48cd891\n",
      "transformers: 4.48.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instance-type: trn2.3xlarge\n",
      "instance-id: i-0d18965e738b3336d\n",
      "logical-neuroncore-config: 1\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 8      | 0-7      | 96 GB  | 0000:33:00.0 | 0-11     | 0    |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os, time, shutil\n",
    "\n",
    "# Verify Neuron environment\n",
    "import torch, torch_neuronx\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'torch-neuronx: {torch_neuronx.__version__}')\n",
    "\n",
    "# Ensure correct transformers version\n",
    "try:\n",
    "    import transformers\n",
    "    ver = transformers.__version__\n",
    "    print(f'transformers: {ver}')\n",
    "    if ver >= '4.54.0':\n",
    "        print('WARNING: transformers >= 4.54.0 has ~20% regression for this model. Downgrading...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                               'transformers==4.48.0', '-q'])\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           'transformers==4.48.0', '-q'])\n",
    "\n",
    "# Install Triton client\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                       'tritonclient[http]', '-q'])\n",
    "\n",
    "# Show Neuron devices\n",
    "r = subprocess.run(['neuron-ls'], capture_output=True, text=True)\n",
    "print(f'\\n{r.stdout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Compile BERT Reranker for All Batch Sizes (LNC=1)\n",
    "\n",
    "Compile 5 model variants (BS=1, 2, 4, 8, 16) with sequence length 1024 and **LNC=1**.\n",
    "Takes ~8 minutes per variant (~40 min total). Skips already-compiled models, so if you are\n",
    "running this multiple times you will see faster compilation. For production, you would deploy\n",
    "with pre-compiled models.\n",
    "\n",
    "**Critical**: The `--lnc 1` compiler flag produces models that are **incompatible** with LNC=2.\n",
    "Never mix LNC=1 and LNC=2 compiled models.\n",
    "\n",
    "The `--auto-cast matmult` flag casts matrix multiplications to BF16, yielding 57-120% speedup\n",
    "with negligible accuracy impact (<0.0004). This was validated in Task 3 benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:22:40.281532Z",
     "iopub.status.busy": "2026-02-26T04:22:40.281249Z",
     "iopub.status.idle": "2026-02-26T04:39:01.758277Z",
     "shell.execute_reply": "2026-02-26T04:39:01.757368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Alibaba-NLP/gte-multilingual-reranker-base\n",
      "Parameters: 305,959,681\n",
      "Compiling for batch sizes: [1, 2, 4, 8, 16]\n",
      "Sequence length: 1024, LNC: 1\n",
      "\n",
      "  Compiling BS=1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs1.pt (676.4 MB, 113s)\n",
      "  Compiling BS=2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs2.pt (679.3 MB, 95s)\n",
      "  Compiling BS=4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs4.pt (684.2 MB, 136s)\n",
      "  Compiling BS=8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs8.pt (695.2 MB, 231s)\n",
      "  Compiling BS=16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs16.pt (713.5 MB, 393s)\n",
      "\n",
      "Compiled models:\n",
      "  model_bs1.pt: 676.4 MB\n",
      "  model_bs16.pt: 713.5 MB\n",
      "  model_bs2.pt: 679.3 MB\n",
      "  model_bs4.pt: 684.2 MB\n",
      "  model_bs8.pt: 695.2 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "os.environ['NEURON_RT_LOG_LEVEL'] = 'ERROR'\n",
    "\n",
    "MODEL_ID = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "SEQ_LENGTH = 1024\n",
    "LNC = 1\n",
    "MODEL_DIR = os.path.expanduser('~/triton_repo/bert_reranker/1')\n",
    "BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, torchscript=True, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model: {MODEL_ID}')\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Compiling for batch sizes: {BATCH_SIZES}')\n",
    "print(f'Sequence length: {SEQ_LENGTH}, LNC: {LNC}\\n')\n",
    "\n",
    "for bs in BATCH_SIZES:\n",
    "    output_path = os.path.join(MODEL_DIR, f'model_bs{bs}.pt')\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'  BS={bs}: already compiled, skipping')\n",
    "        continue\n",
    "    print(f'  Compiling BS={bs}...')\n",
    "    queries = ['What is machine learning?'] * bs\n",
    "    docs = ['Machine learning is a subset of artificial intelligence.'] * bs\n",
    "    inputs = tokenizer(queries, docs, return_tensors='pt',\n",
    "                       max_length=SEQ_LENGTH, padding='max_length', truncation=True)\n",
    "    t0 = time.time()\n",
    "    model_neuron = torch_neuronx.trace(\n",
    "        model,\n",
    "        (inputs['input_ids'], inputs['attention_mask']),\n",
    "        compiler_args=['--model-type', 'transformer', '--optlevel', '2',\n",
    "                       '--auto-cast', 'matmult', '--lnc', str(LNC)])\n",
    "    compile_time = time.time() - t0\n",
    "    torch.jit.save(model_neuron, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f'    Saved: {output_path} ({size_mb:.1f} MB, {compile_time:.0f}s)')\n",
    "\n",
    "print('\\nCompiled models:')\n",
    "for f in sorted(os.listdir(MODEL_DIR)):\n",
    "    if f.endswith('.pt'):\n",
    "        size = os.path.getsize(os.path.join(MODEL_DIR, f)) / (1024 * 1024)\n",
    "        print(f'  {f}: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Write Triton Model Repository Files (LNC=1)\n",
    "\n",
    "Write `config.pbtxt` and `model.py` into the model repository alongside the compiled `.pt` files.\n",
    "\n",
    "Key differences from DistilBERT:\n",
    "- **Output**: single float score (`score`) instead of 768-dim embedding\n",
    "- **Sequence length**: 1024 instead of 128\n",
    "- **Model class**: `AutoModelForSequenceClassification` instead of `DistilBertModel`\n",
    "- **Output extraction**: `outputs.logits[:, 0]` instead of CLS hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:39:01.761034Z",
     "iopub.status.busy": "2026-02-26T04:39:01.760671Z",
     "iopub.status.idle": "2026-02-26T04:39:01.766623Z",
     "shell.execute_reply": "2026-02-26T04:39:01.766170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config.pbtxt (instance_group count=8 for LNC=1)\n",
      "Wrote model.py (NEURON_LOGICAL_NC_CONFIG=1, cores 0-7, 2 input tensors)\n",
      "\n",
      "Model repository:\n",
      "triton_repo/\n",
      "  bert_reranker/\n",
      "    config.pbtxt\n",
      "    1/\n",
      "      model.py\n",
      "      model_bs1.pt (676.4 MB)\n",
      "      model_bs16.pt (713.5 MB)\n",
      "      model_bs2.pt (679.3 MB)\n",
      "      model_bs4.pt (684.2 MB)\n",
      "      model_bs8.pt (695.2 MB)\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = os.path.expanduser('~/triton_repo/bert_reranker')\n",
    "\n",
    "# -- config.pbtxt (LNC=1: 8 instances) -----------------------------------------\n",
    "config_pbtxt = r\"\"\"name: \"bert_reranker\"\n",
    "platform: \"python\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [1024]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [1024]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"score\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 8\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 5000\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"model_dir\"\n",
    "  value: { string_value: \"/models/bert_reranker/1\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"tokenizer_name\"\n",
    "  value: { string_value: \"Alibaba-NLP/gte-multilingual-reranker-base\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"max_seq_length\"\n",
    "  value: { string_value: \"1024\" }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(REPO_DIR, 'config.pbtxt'), 'w') as f:\n",
    "    f.write(config_pbtxt)\n",
    "print('Wrote config.pbtxt (instance_group count=8 for LNC=1)')\n",
    "\n",
    "# -- model.py (Triton Python backend, LNC=1) ----------------------------------\n",
    "model_py = r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Triton Python Backend for BERT Reranker on AWS Neuron - Multi-Model (LNC=1)\n",
    "\n",
    "Loads compiled models for each batch size (1, 2, 4, 8, 16) and\n",
    "dispatches to the best-fit model to avoid padding waste.\n",
    "Each Triton instance is pinned to a separate Neuron core (cores 0-7).\n",
    "\n",
    "Key differences from DistilBERT:\n",
    "- Output is a single float score (logit) per sample, not an embedding\n",
    "- Uses AutoModelForSequenceClassification\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import triton_python_backend_utils as pb_utils\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\"model_config\"])\n",
    "        params = self.model_config.get(\"parameters\", {})\n",
    "        model_dir = params.get(\"model_dir\", {}).get(\n",
    "            \"string_value\", \"/models/bert_reranker/1\")\n",
    "        self.max_seq_length = int(\n",
    "            params.get(\"max_seq_length\", {}).get(\"string_value\", \"1024\"))\n",
    "\n",
    "        # Pin this instance to a specific Neuron core (0-7 for LNC=1)\n",
    "        instance_name = args.get(\"model_instance_name\", \"bert_reranker_0_0\")\n",
    "        core_id = int(instance_name.split(\"_\")[-1])\n",
    "        os.environ[\"NEURON_RT_VISIBLE_CORES\"] = str(core_id)\n",
    "        os.environ[\"NEURON_LOGICAL_NC_CONFIG\"] = \"1\"\n",
    "        print(f\"Instance {instance_name}: pinned to Neuron core {core_id}\")\n",
    "\n",
    "        # Load all compiled models\n",
    "        self.models = {}\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            model_path = os.path.join(model_dir, f\"model_bs{bs}.pt\")\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[bs] = torch.jit.load(model_path)\n",
    "                self.models[bs].eval()\n",
    "                print(f\"  Loaded model for batch_size={bs}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Model not found: {model_path}\")\n",
    "\n",
    "        if not self.models:\n",
    "            raise RuntimeError(\"No compiled models found!\")\n",
    "\n",
    "        # Warmup all models\n",
    "        print(\"Warming up models...\")\n",
    "        for bs, mdl in self.models.items():\n",
    "            dummy_ids = torch.zeros((bs, self.max_seq_length), dtype=torch.long)\n",
    "            dummy_mask = torch.ones((bs, self.max_seq_length), dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = mdl(dummy_ids, dummy_mask)\n",
    "            print(f\"  Warmed up BS={bs}\")\n",
    "        print(f\"Model initialization complete! \"\n",
    "              f\"Available batch sizes: {sorted(self.models.keys())}\")\n",
    "\n",
    "    def _get_best_model(self, actual_batch_size):\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            if bs >= actual_batch_size and bs in self.models:\n",
    "                return bs, self.models[bs]\n",
    "        largest = max(self.models.keys())\n",
    "        return largest, self.models[largest]\n",
    "\n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        for request in requests:\n",
    "            input_ids = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            ).long()\n",
    "            attention_mask = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            ).long()\n",
    "\n",
    "            actual_bs = input_ids.shape[0]\n",
    "            target_bs, model = self._get_best_model(actual_bs)\n",
    "\n",
    "            # Pad input tensors to compiled batch size if needed\n",
    "            if actual_bs < target_bs:\n",
    "                pad = target_bs - actual_bs\n",
    "                input_ids = torch.cat([input_ids,\n",
    "                    torch.zeros((pad, input_ids.shape[1]), dtype=torch.long)], dim=0)\n",
    "                attention_mask = torch.cat([attention_mask,\n",
    "                    torch.zeros((pad, attention_mask.shape[1]), dtype=torch.long)], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # Extract logit score: outputs is a tuple, first element is logits\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "            scores = logits[:actual_bs, 0:1].cpu().numpy()\n",
    "\n",
    "            output_tensor = pb_utils.Tensor(\"score\",\n",
    "                                            scores.astype(np.float32))\n",
    "            responses.append(\n",
    "                pb_utils.InferenceResponse(output_tensors=[output_tensor]))\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(\"Finalizing BERT Reranker model...\")\n",
    "        if hasattr(self, \"models\"):\n",
    "            for bs, mdl in self.models.items():\n",
    "                del mdl\n",
    "            self.models.clear()\n",
    "'''\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model.py'), 'w') as f:\n",
    "    f.write(model_py)\n",
    "print('Wrote model.py (NEURON_LOGICAL_NC_CONFIG=1, cores 0-7, 2 input tensors)')\n",
    "\n",
    "# Verify\n",
    "print('\\nModel repository:')\n",
    "for root, dirs, files in os.walk(os.path.expanduser('~/triton_repo')):\n",
    "    level = root.replace(os.path.expanduser('~/triton_repo'), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, f)) / (1024 * 1024)\n",
    "        label = f'{f} ({size:.1f} MB)' if size > 1 else f\n",
    "        print(f'{indent}  {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Build Triton + Neuron Docker Image\n",
    "\n",
    "Triton has no native Neuron backend, so we build Triton from source inside the AWS Neuron\n",
    "PyTorch inference base image. This takes **15-20 minutes** and produces a ~15.8 GB image.\n",
    "\n",
    "The cell skips the build if the image already exists.\n",
    "\n",
    "**Note**: We pin `transformers==4.48.0` inside the Docker image to avoid the performance\n",
    "regression with versions >= 4.54.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:39:01.768606Z",
     "iopub.status.busy": "2026-02-26T04:39:01.768485Z",
     "iopub.status.idle": "2026-02-26T04:54:09.658659Z",
     "shell.execute_reply": "2026-02-26T04:54:09.657876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Triton + Neuron Docker image (15-20 minutes)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build complete!\n",
      "IMAGE                                ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
      "triton-neuron-bert-reranker:latest   4b37207b7048       24.1GB         7.83GB        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOCKER_IMAGE = 'triton-neuron-bert-reranker:latest'\n",
    "\n",
    "# Check if already built\n",
    "r = subprocess.run(['docker', 'images', '-q', DOCKER_IMAGE],\n",
    "                   capture_output=True, text=True)\n",
    "if r.stdout.strip():\n",
    "    print(f'Docker image already exists: {r.stdout.strip()}')\n",
    "    print('Delete with: docker rmi ' + DOCKER_IMAGE)\n",
    "else:\n",
    "    # Write Dockerfile\n",
    "    dockerfile = r\"\"\"ARG BASE_IMAGE=public.ecr.aws/neuron/pytorch-inference-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04\n",
    "FROM $BASE_IMAGE\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PJRT_DEVICE=NEURON \\\n",
    "    LD_LIBRARY_PATH=\"/opt/conda/lib:/opt/aws/neuron/lib:/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}\" \\\n",
    "    PATH=\"/opt/program:/opt/aws/neuron/bin:/opt/tritonserver/bin:${PATH}\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    wget gnupg2 build-essential git nginx pkg-config unzip \\\n",
    "    libssl-dev libcurl4-openssl-dev libgoogle-perftools-dev \\\n",
    "    libnuma-dev libarchive-dev libxml2-dev zlib1g-dev \\\n",
    "    autoconf automake libtool gperf scons patchelf \\\n",
    "    libre2-dev libb64-dev rapidjson-dev libboost-dev \\\n",
    "    cmake cmake-data \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel virtualenv build cmake==3.31.10\n",
    "RUN pip3 install transformers==4.48.0\n",
    "\n",
    "RUN git clone --depth=1 --branch=r26.01 https://github.com/triton-inference-server/server.git /server && \\\n",
    "    cd /server && \\\n",
    "    Python3_ROOT_DIR=/opt/conda \\\n",
    "    Python3_EXECUTABLE=/opt/conda/bin/python3 \\\n",
    "    Python3_INCLUDE_DIR=/opt/conda/include/python3.12 \\\n",
    "    Python3_LIBRARY=/opt/conda/lib/libpython3.12.so \\\n",
    "    ./build.py -v --no-container-build --build-dir=/server/build --backend=python \\\n",
    "    --enable-metrics --enable-logging --enable-stats --endpoint=\"http\" --endpoint=\"grpc\" && \\\n",
    "    cp -r /server/build/opt/* /opt/ && \\\n",
    "    cd / && rm -rf /server\n",
    "\n",
    "EXPOSE 8000 8001 8002\n",
    "CMD [\"tritonserver\", \"--model-repository=/models\"]\n",
    "\"\"\"\n",
    "    dockerfile_path = os.path.expanduser('~/Dockerfile.triton-neuron')\n",
    "    with open(dockerfile_path, 'w') as f:\n",
    "        f.write(dockerfile)\n",
    "\n",
    "    print('Building Triton + Neuron Docker image (15-20 minutes)...')\n",
    "    r = subprocess.run(\n",
    "        ['docker', 'build', '-f', dockerfile_path, '-t', DOCKER_IMAGE, '.'],\n",
    "        cwd=os.path.expanduser('~'),\n",
    "        capture_output=True, text=True, timeout=2400)\n",
    "    if r.returncode == 0:\n",
    "        print('Build complete!')\n",
    "    else:\n",
    "        print(f'Build FAILED (rc={r.returncode})')\n",
    "        print(r.stderr[-3000:])\n",
    "\n",
    "# Show image\n",
    "r = subprocess.run(['docker', 'images', DOCKER_IMAGE], capture_output=True, text=True)\n",
    "print(r.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Start Triton Server\n",
    "\n",
    "Launch the Docker container with the Neuron device mounted and the model repository bind-mounted.\n",
    "With 8 instances (LNC=1) each loading 5 batch-size variants of a ~718 MB model, loading takes\n",
    "~2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:54:09.660175Z",
     "iopub.status.busy": "2026-02-26T04:54:09.660021Z",
     "iopub.status.idle": "2026-02-26T04:56:20.649655Z",
     "shell.execute_reply": "2026-02-26T04:56:20.649092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container started. Waiting for 8 model instances to load (~2-3 min)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server ready after ~122s\n",
      "Model instances initialized: 8/8\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "NUM_INSTANCES = 8  # LNC=1: 8 logical cores\n",
    "\n",
    "# Stop any previous run\n",
    "subprocess.run(['docker', 'rm', '-f', 'triton-bert-reranker'],\n",
    "               capture_output=True)\n",
    "time.sleep(3)\n",
    "\n",
    "cmd = [\n",
    "    'docker', 'run', '-d',\n",
    "    '--name', 'triton-bert-reranker',\n",
    "    '--device=/dev/neuron0',\n",
    "    '-v', os.path.expanduser('~/triton_repo') + ':/models:ro',\n",
    "    '-p', '8000:8000', '-p', '8001:8001', '-p', '8002:8002',\n",
    "    DOCKER_IMAGE,\n",
    "    'tritonserver', '--model-repository=/models', '--log-verbose=0',\n",
    "]\n",
    "r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "if r.returncode != 0:\n",
    "    print(f'Failed to start container: {r.stderr}')\n",
    "else:\n",
    "    print(f'Container started. Waiting for {NUM_INSTANCES} model instances to load (~2-3 min)...')\n",
    "\n",
    "# Poll for readiness (up to 6 minutes)\n",
    "for i in range(180):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen('http://localhost:8000/v2/health/ready', timeout=2)\n",
    "        if resp.status == 200:\n",
    "            elapsed = (i + 1) * 2\n",
    "            print(f'Server ready after ~{elapsed}s')\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('Timeout waiting for server!')\n",
    "    r = subprocess.run(['docker', 'logs', '--tail', '30', 'triton-bert-reranker'],\n",
    "                       capture_output=True, text=True)\n",
    "    print(r.stdout)\n",
    "\n",
    "# Verify instances loaded\n",
    "r = subprocess.run(['docker', 'logs', 'triton-bert-reranker'],\n",
    "                   capture_output=True, text=True)\n",
    "n = r.stdout.count('Model initialization complete')\n",
    "print(f'Model instances initialized: {n}/{NUM_INSTANCES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Run Benchmark\n",
    "\n",
    "Runs the full test matrix: 1 baseline + 15 dynamic-batching configurations (3 concurrency levels\n",
    "x 5 batch sizes), 10 seconds each. Total runtime ~3 minutes.\n",
    "\n",
    "Concurrency levels are 16/32/64 to maintain 2 workers per core as baseline.\n",
    "\n",
    "**Note**: The reranker uses seq_len=1024 (8x longer than DistilBERT's 128), so per-inference\n",
    "latency is significantly higher (~191ms vs ~0.37ms per sample). Throughput numbers will be\n",
    "correspondingly lower.\n",
    "\n",
    "**Note**: One worker per test will print a harmless greenlet thread-switch error. This is a\n",
    "known cosmetic issue in `tritonclient` and does not affect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:56:20.651287Z",
     "iopub.status.busy": "2026-02-26T04:56:20.651160Z",
     "iopub.status.idle": "2026-02-26T04:59:15.228528Z",
     "shell.execute_reply": "2026-02-26T04:59:15.227958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT RERANKER TRITON BENCHMARK - Neuron (trn2.3xlarge, LNC=1)\n",
      "==========================================================================================\n",
      "\n",
      "Baseline: single request, no concurrency...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  P50: 20.73ms  Throughput: 48.3 inf/sec\n",
      "\n",
      "Concurrency=16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   58.45ms  P95=   59.40ms  P99=   60.13ms  Throughput=   265.2 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   57.95ms  P95=   58.39ms  P99=   59.16ms  Throughput=   539.0 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  115.51ms  P95=  231.70ms  P99=  273.92ms  Throughput=   553.9 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  245.45ms  P95=  348.78ms  P99=  357.53ms  Throughput=   517.2 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  469.56ms  P95=  478.48ms  P99=  482.38ms  Throughput=   525.4 inf/sec\n",
      "\n",
      "Concurrency=32:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   58.53ms  P95=   59.52ms  P99=   62.73ms  Throughput=   534.2 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=  116.18ms  P95=  231.47ms  P99=  263.38ms  Throughput=   554.1 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  233.47ms  P95=  281.10ms  P99=  288.67ms  Throughput=   552.7 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  485.58ms  P95=  487.23ms  P99=  560.93ms  Throughput=   523.7 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  947.81ms  P95=  959.88ms  P99=  969.52ms  Throughput=   526.9 inf/sec\n",
      "\n",
      "Concurrency=64:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=  117.38ms  P95=  236.26ms  P99=  258.73ms  Throughput=   551.1 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=  235.14ms  P95=  291.14ms  P99=  308.74ms  Throughput=   553.7 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  457.54ms  P95=  459.32ms  P99=  460.57ms  Throughput=   556.9 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  968.93ms  P95=  975.58ms  P99=  980.95ms  Throughput=   524.7 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50= 1903.44ms  P95= 1927.22ms  P99= 1936.59ms  Throughput=   529.1 inf/sec\n",
      "\n",
      "==========================================================================================\n",
      "Batch   Workers   Requests    P50 (ms)    P95 (ms)    P99 (ms)    Throughput     \n",
      "------------------------------------------------------------------------------------------\n",
      "1       1         483         20.73       20.93       21.02       48.3           \n",
      "1       16        2667        58.45       59.40       60.13       265.2          \n",
      "2       16        2711        57.95       58.39       59.16       539.0          \n",
      "4       16        1404        115.51      231.70      273.92      553.9          \n",
      "8       16        667         245.45      348.78      357.53      517.2          \n",
      "16      16        344         469.56      478.48      482.38      525.4          \n",
      "1       32        5394        58.53       59.52       62.73       534.2          \n",
      "2       32        2801        116.18      231.47      263.38      554.1          \n",
      "4       32        1416        233.47      281.10      288.67      552.7          \n",
      "8       32        678         485.58      487.23      560.93      523.7          \n",
      "16      32        361         947.81      959.88      969.52      526.9          \n",
      "1       64        5600        117.38      236.26      258.73      551.1          \n",
      "2       64        2840        235.14      291.14      308.74      553.7          \n",
      "4       64        1446        457.54      459.32      460.57      556.9          \n",
      "8       64        711         968.93      975.58      980.95      524.7          \n",
      "16      64        390         1903.44     1927.22     1936.59     529.1          \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'bert_reranker'\n",
    "DURATION = 10.0  # seconds per test\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-multilingual-reranker-base',\n",
    "                                    trust_remote_code=True)\n",
    "\n",
    "\n",
    "def _make_inputs(batch_size):\n",
    "    \"\"\"Create tokenized inputs for the reranker (query + passage pairs).\"\"\"\n",
    "    queries = ['What is machine learning?'] * batch_size\n",
    "    docs = ['Machine learning is a subset of artificial intelligence that '\n",
    "            'focuses on building systems that learn from data.'] * batch_size\n",
    "    tokens = tok(queries, docs, max_length=1024, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    return ids_np, mask_np\n",
    "\n",
    "\n",
    "def _worker(client, batch_size, duration_s, latencies_q, stop_evt):\n",
    "    ids_np, mask_np = _make_inputs(batch_size)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    t_end = time.time() + duration_s\n",
    "    while time.time() < t_end and not stop_evt.is_set():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            client.infer(model_name=MODEL_NAME,\n",
    "                         inputs=[inp_ids, inp_mask])\n",
    "            latencies_q.put((time.time() - t0) * 1000)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "\n",
    "def run_concurrent(batch_size, num_workers, duration_s=DURATION):\n",
    "    clients = [httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "               for _ in range(num_workers)]\n",
    "    # Warmup\n",
    "    ids_np, mask_np = _make_inputs(batch_size)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(5):\n",
    "        clients[0].infer(model_name=MODEL_NAME,\n",
    "                         inputs=[inp_ids, inp_mask])\n",
    "\n",
    "    q = Queue()\n",
    "    stop = threading.Event()\n",
    "    threads = []\n",
    "    t_start = time.time()\n",
    "    for i in range(num_workers):\n",
    "        t = threading.Thread(target=_worker,\n",
    "                             args=(clients[i], batch_size, duration_s, q, stop))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    total_time = time.time() - t_start\n",
    "\n",
    "    latencies = []\n",
    "    while not q.empty():\n",
    "        latencies.append(q.get())\n",
    "    if not latencies:\n",
    "        return None\n",
    "    return {\n",
    "        'batch_size': batch_size, 'num_workers': num_workers,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': (len(latencies) * batch_size) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_baseline(duration_s=DURATION):\n",
    "    client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    ids_np, mask_np = _make_inputs(1)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(10):\n",
    "        client.infer(model_name=MODEL_NAME,\n",
    "                     inputs=[inp_ids, inp_mask])\n",
    "    latencies = []\n",
    "    t_start = time.time()\n",
    "    while time.time() - t_start < duration_s:\n",
    "        t0 = time.time()\n",
    "        client.infer(model_name=MODEL_NAME,\n",
    "                     inputs=[inp_ids, inp_mask])\n",
    "        latencies.append((time.time() - t0) * 1000)\n",
    "    total_time = time.time() - t_start\n",
    "    return {\n",
    "        'batch_size': 1, 'num_workers': 1,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': len(latencies) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# -- Run all tests -------------------------------------------------------------\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "assert client.is_server_ready(), 'Triton server not ready!'\n",
    "\n",
    "print('BERT RERANKER TRITON BENCHMARK - Neuron (trn2.3xlarge, LNC=1)')\n",
    "print('=' * 90)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline\n",
    "print('\\nBaseline: single request, no concurrency...')\n",
    "r = run_baseline()\n",
    "results.append(r)\n",
    "print(f'  P50: {r[\"p50\"]:.2f}ms  Throughput: {r[\"throughput\"]:.1f} inf/sec')\n",
    "\n",
    "# Dynamic batching -- concurrency levels scaled for 8 cores\n",
    "for conc in [16, 32, 64]:\n",
    "    print(f'\\nConcurrency={conc}:')\n",
    "    for bs in [1, 2, 4, 8, 16]:\n",
    "        r = run_concurrent(bs, conc)\n",
    "        if r:\n",
    "            results.append(r)\n",
    "            print(f'  BS={bs:<3} P50={r[\"p50\"]:>8.2f}ms  '\n",
    "                  f'P95={r[\"p95\"]:>8.2f}ms  '\n",
    "                  f'P99={r[\"p99\"]:>8.2f}ms  '\n",
    "                  f'Throughput={r[\"throughput\"]:>8.1f} inf/sec')\n",
    "\n",
    "# Summary table\n",
    "print(f'\\n{\"=\" * 90}')\n",
    "print(f'{\"Batch\":<8}{\"Workers\":<10}{\"Requests\":<12}'\n",
    "      f'{\"P50 (ms)\":<12}{\"P95 (ms)\":<12}{\"P99 (ms)\":<12}{\"Throughput\":<15}')\n",
    "print('-' * 90)\n",
    "for r in results:\n",
    "    print(f'{r[\"batch_size\"]:<8}{r[\"num_workers\"]:<10}{r[\"total_requests\"]:<12}'\n",
    "          f'{r[\"p50\"]:<12.2f}{r[\"p95\"]:<12.2f}{r[\"p99\"]:<12.2f}'\n",
    "          f'{r[\"throughput\"]:<15.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Sample Reranking Results\n",
    "\n",
    "Send a real query with multiple passages to the Triton server and display the reranking scores.\n",
    "This demonstrates the model's ability to rank passages by relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:59:15.229950Z",
     "iopub.status.busy": "2026-02-26T04:59:15.229800Z",
     "iopub.status.idle": "2026-02-26T04:59:16.215797Z",
     "shell.execute_reply": "2026-02-26T04:59:16.215154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"What are the benefits of renewable energy?\"\n",
      "Passages: 6\n",
      "\n",
      "Scoring...\n",
      "\n",
      "================================================================================\n",
      "Reranking Results for: \"What are the benefits of renewable energy?\"\n",
      "================================================================================\n",
      "\n",
      "  Rank 1 (score=+1.9195, prob=0.8721):\n",
      "    [0] Renewable energy sources like solar and wind power produce electricity without g...\n",
      "\n",
      "  Rank 2 (score=+1.5806, prob=0.8293):\n",
      "    [4] Wind energy creates more jobs per megawatt than coal or natural gas power plants...\n",
      "\n",
      "  Rank 3 (score=+0.7861, prob=0.6870):\n",
      "    [2] Solar panels have become 90% cheaper over the past decade, making renewable ener...\n",
      "\n",
      "  Rank 4 (score=-3.3127, prob=0.0351):\n",
      "    [1] The stock market experienced significant volatility in Q4 2025, with tech stocks...\n",
      "\n",
      "  Rank 5 (score=-3.3220, prob=0.0348):\n",
      "    [3] The history of ancient Rome spans over a thousand years, from its founding in 75...\n",
      "\n",
      "  Rank 6 (score=-3.3224, prob=0.0348):\n",
      "    [5] Python is a popular programming language known for its readability and extensive...\n",
      "\n",
      "================================================================================\n",
      "Top-3 passages are about renewable energy, as expected.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "tok = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-multilingual-reranker-base',\n",
    "                                    trust_remote_code=True)\n",
    "\n",
    "query = 'What are the benefits of renewable energy?'\n",
    "passages = [\n",
    "    'Renewable energy sources like solar and wind power produce electricity without '\n",
    "    'greenhouse gas emissions, reducing climate change impacts and improving air quality.',\n",
    "    'The stock market experienced significant volatility in Q4 2025, with tech stocks '\n",
    "    'leading the decline amid rising interest rates.',\n",
    "    'Solar panels have become 90% cheaper over the past decade, making renewable '\n",
    "    'energy cost-competitive with fossil fuels in most markets.',\n",
    "    'The history of ancient Rome spans over a thousand years, from its founding '\n",
    "    'in 753 BC to the fall of the Western Roman Empire in 476 AD.',\n",
    "    'Wind energy creates more jobs per megawatt than coal or natural gas power plants, '\n",
    "    'boosting local economies in rural areas.',\n",
    "    'Python is a popular programming language known for its readability and '\n",
    "    'extensive library ecosystem.',\n",
    "]\n",
    "\n",
    "print(f'Query: \"{query}\"')\n",
    "print(f'Passages: {len(passages)}')\n",
    "print('\\nScoring...')\n",
    "\n",
    "scores = []\n",
    "for i, passage in enumerate(passages):\n",
    "    tokens = tok([query], [passage], max_length=1024, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    inp_ids = httpclient.InferInput('input_ids',\n",
    "                                    tokens['input_ids'].shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask',\n",
    "                                     tokens['attention_mask'].shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(tokens['input_ids'].astype(np.int64))\n",
    "    inp_mask.set_data_from_numpy(tokens['attention_mask'].astype(np.int64))\n",
    "    result = client.infer(model_name=MODEL_NAME,\n",
    "                          inputs=[inp_ids, inp_mask])\n",
    "    score = result.as_numpy('score')[0][0]\n",
    "    scores.append((score, i, passage))\n",
    "\n",
    "# Sort by score (highest = most relevant)\n",
    "scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(f'\\n{\"=\" * 80}')\n",
    "print(f'Reranking Results for: \"{query}\"')\n",
    "print(f'{\"=\" * 80}')\n",
    "for rank, (score, idx, passage) in enumerate(scores, 1):\n",
    "    sigmoid_score = 1 / (1 + np.exp(-score))\n",
    "    short = passage[:80] + '...' if len(passage) > 80 else passage\n",
    "    print(f'\\n  Rank {rank} (score={score:+.4f}, prob={sigmoid_score:.4f}):')\n",
    "    print(f'    [{idx}] {short}')\n",
    "\n",
    "print(f'\\n{\"=\" * 80}')\n",
    "print('Top-3 passages are about renewable energy, as expected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Cleanup\n",
    "\n",
    "Stop and remove the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T04:59:16.217464Z",
     "iopub.status.busy": "2026-02-26T04:59:16.217335Z",
     "iopub.status.idle": "2026-02-26T04:59:17.153697Z",
     "shell.execute_reply": "2026-02-26T04:59:17.153075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton server stopped and removed.\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(['docker', 'rm', '-f', 'triton-bert-reranker'], capture_output=True)\n",
    "print('Triton server stopped and removed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (aws_neuronx_venv_pytorch_2_9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
