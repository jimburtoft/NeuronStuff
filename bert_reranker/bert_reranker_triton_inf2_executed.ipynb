{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Reranker Inference on AWS Inferentia2 with Triton Inference Server\n",
    "\n",
    "This notebook deploys the [Alibaba-NLP/gte-multilingual-reranker-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base)\n",
    "BERT cross-encoder reranker (~306M parameters) on an AWS **inf2.8xlarge** instance using\n",
    "NVIDIA Triton Inference Server with the AWS Neuron SDK, then benchmarks throughput and latency.\n",
    "\n",
    "Everything is self-contained: the notebook writes all required files (Dockerfile, Triton config,\n",
    "Python backend, compile script, benchmark client), builds the Docker image, compiles the models,\n",
    "starts the server, and runs the benchmark.\n",
    "\n",
    "**Instance**: inf2.8xlarge (1 Neuron device, 2 NeuronCores, 32 GB device memory)  \n",
    "**Time to complete**: ~60-80 minutes (compilation dominates on first run)  \n",
    "**Prerequisites**: Deep Learning AMI Neuron (Ubuntu 24.04), Docker installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We measure throughput (inferences/second) and latency (P50, P95, P99) under multiple load patterns,\n",
    "varying both client-side batch size and request concurrency to exercise Triton's dynamic batching.\n",
    "\n",
    "- **Input**: Query-passage pairs, tokenized and padded to 1024 tokens (`input_ids` + `attention_mask`)\n",
    "- **Output**: Single float reranking score per sample (logit from cross-encoder head)\n",
    "- **Duration**: 10 seconds per test configuration\n",
    "- **Warmup**: 5 requests before timing\n",
    "- **Concurrency**: Python threads, one HTTP client per thread\n",
    "\n",
    "### Neuron-Specific: Per-Batch-Size Compilation\n",
    "\n",
    "Unlike GPUs, Neuron models are compiled for a **fixed batch size**. Running a BS=1 request through\n",
    "a BS=16 compiled model wastes 15/16 of the compute. We compile separate models for BS=1, 2, 4, 8, 16\n",
    "and the Triton Python backend dispatches each request to the smallest model that fits.\n",
    "\n",
    "### Triton Configuration\n",
    "\n",
    "- **2 model instances** (one per NeuronCore), each pinned via `NEURON_RT_VISIBLE_CORES`\n",
    "- **Dynamic batching**: preferred sizes [4, 8, 16], max queue delay 5ms\n",
    "- **Python backend**: Triton has no native Neuron backend, so we use the Python backend with `torch_neuronx`\n",
    "\n",
    "### Test Matrix\n",
    "\n",
    "| Test | Concurrency | Client Batch Sizes |\n",
    "|------|-------------|--------------------|\n",
    "| Baseline | 1 (no batching) | 1 |\n",
    "| Dynamic batching | 4, 8, 16 | 1, 2, 4, 8, 16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Instance Specifications\n",
    "\n",
    "| Spec | Value |\n",
    "|------|-------|\n",
    "| Model | Alibaba-NLP/gte-multilingual-reranker-base (~306M params) |\n",
    "| Architecture | 12-layer NewBERT cross-encoder, hidden size 768 |\n",
    "| Task | Cross-encoder reranking (query + passage -> relevance score) |\n",
    "| Input | `input_ids`, `attention_mask` (1024 tokens, padded) |\n",
    "| Output | Single float logit score per sample |\n",
    "| Compiled model size | ~720 MB per batch size variant |\n",
    "| Instance type | inf2.8xlarge |\n",
    "| Neuron device | 1 device, 2 NeuronCores |\n",
    "| Device memory | 32 GB (16 GB per core) |\n",
    "| vCPUs / RAM | 32 / 128 GB |\n",
    "\n",
    "### Software Stack\n",
    "\n",
    "| Component | Version |\n",
    "|-----------|--------|\n",
    "| Neuron SDK | 2.27.1 |\n",
    "| PyTorch / torch-neuronx | 2.9.0 / 2.9.0.2.11 |\n",
    "| neuronx-cc (compiler) | 2.22.12471 |\n",
    "| Transformers | 4.48.0 |\n",
    "| Triton Inference Server | 2.65.0 (r26.01, built from source) |\n",
    "| Python | 3.12 |\n",
    "| OS / AMI | Ubuntu 24.04 / Deep Learning AMI Neuron 20260126 |\n",
    "\n",
    "> **Important**: Transformers versions 4.54.0+ have a confirmed ~20% performance regression for\n",
    "> this model family on Neuron. Use versions 4.48.0 through 4.53.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Setup & Environment Check\n",
    "\n",
    "After you deploy the instance using the Ubuntu Neuron Deep Learning AMI (with all the Neuron drivers installed), run this notebook inside the pre-installed PyTorch 2.9 Neuron virtual environment:\n",
    "\n",
    "```bash\n",
    "source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --no-browser\n",
    "```\n",
    "\n",
    "Alternatively, if you are running from within a remote VSCode instance, you can use `ln -s /opt/aws_neuronx_venv_pytorch_2_9/bin/activate ~/.venv` to help VSCode find your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:13:19.513385Z",
     "iopub.status.busy": "2026-02-26T17:13:19.513247Z",
     "iopub.status.idle": "2026-02-26T17:13:30.742539Z",
     "shell.execute_reply": "2026-02-26T17:13:30.741752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu128\n",
      "torch-neuronx: 2.9.0.2.11.19912+e48cd891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.53.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instance-type: inf2.8xlarge\n",
      "instance-id: i-06e983546fee50417\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 2      | 0-1      | 32 GB  | 0000:00:1f.0 | 0-31     | -1   |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os, time, shutil\n",
    "\n",
    "# Verify Neuron environment\n",
    "import torch, torch_neuronx\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'torch-neuronx: {torch_neuronx.__version__}')\n",
    "\n",
    "# Ensure correct transformers version\n",
    "try:\n",
    "    import transformers\n",
    "    ver = transformers.__version__\n",
    "    print(f'transformers: {ver}')\n",
    "    if ver >= '4.54.0':\n",
    "        print('WARNING: transformers >= 4.54.0 has ~20% regression for this model. Downgrading...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                               'transformers==4.48.0', '-q'])\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           'transformers==4.48.0', '-q'])\n",
    "\n",
    "# Install Triton client\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                       'tritonclient[http]', '-q'])\n",
    "\n",
    "# Show Neuron devices\n",
    "r = subprocess.run(['neuron-ls'], capture_output=True, text=True)\n",
    "print(f'\\n{r.stdout}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Compile BERT Reranker for All Batch Sizes\n",
    "\n",
    "Compile 5 model variants (BS=1, 2, 4, 8, 16) with sequence length 1024.\n",
    "Takes ~10 minutes per variant (~50 min total on inf2). Skips already-compiled models.\n",
    "\n",
    "The `--auto-cast matmult` flag casts matrix multiplications to BF16, yielding 57-120% speedup\n",
    "with negligible accuracy impact (<0.0004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:13:30.745067Z",
     "iopub.status.busy": "2026-02-26T17:13:30.744706Z",
     "iopub.status.idle": "2026-02-26T17:32:27.858989Z",
     "shell.execute_reply": "2026-02-26T17:32:27.857659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Alibaba-NLP/gte-multilingual-reranker-base\n",
      "Parameters: 305,959,681\n",
      "Compiling for batch sizes: [1, 2, 4, 8, 16]\n",
      "Sequence length: 1024\n",
      "\n",
      "  Compiling BS=1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs1.pt (676.4 MB, 91s)\n",
      "  Compiling BS=2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs2.pt (679.1 MB, 120s)\n",
      "  Compiling BS=4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs4.pt (684.3 MB, 164s)\n",
      "  Compiling BS=8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs8.pt (695.8 MB, 273s)\n",
      "  Compiling BS=16...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: /home/ubuntu/triton_repo/bert_reranker/1/model_bs16.pt (715.2 MB, 472s)\n",
      "\n",
      "Compiled models:\n",
      "  model_bs1.pt: 676.4 MB\n",
      "  model_bs16.pt: 715.2 MB\n",
      "  model_bs2.pt: 679.1 MB\n",
      "  model_bs4.pt: 684.3 MB\n",
      "  model_bs8.pt: 695.8 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "os.environ['NEURON_RT_LOG_LEVEL'] = 'ERROR'\n",
    "\n",
    "MODEL_ID = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "SEQ_LENGTH = 1024\n",
    "MODEL_DIR = os.path.expanduser('~/triton_repo/bert_reranker/1')\n",
    "BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, torchscript=True, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "print(f'Model: {MODEL_ID}')\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Compiling for batch sizes: {BATCH_SIZES}')\n",
    "print(f'Sequence length: {SEQ_LENGTH}\\n')\n",
    "\n",
    "for bs in BATCH_SIZES:\n",
    "    output_path = os.path.join(MODEL_DIR, f'model_bs{bs}.pt')\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'  BS={bs}: already compiled, skipping')\n",
    "        continue\n",
    "    print(f'  Compiling BS={bs}...')\n",
    "    queries = ['What is machine learning?'] * bs\n",
    "    docs = ['Machine learning is a subset of artificial intelligence.'] * bs\n",
    "    inputs = tokenizer(queries, docs, return_tensors='pt',\n",
    "                       max_length=SEQ_LENGTH, padding='max_length', truncation=True)\n",
    "    t0 = time.time()\n",
    "    model_neuron = torch_neuronx.trace(\n",
    "        model,\n",
    "        (inputs['input_ids'], inputs['attention_mask']),\n",
    "        compiler_args=['--model-type', 'transformer', '--optlevel', '2',\n",
    "                       '--auto-cast', 'matmult'])\n",
    "    compile_time = time.time() - t0\n",
    "    torch.jit.save(model_neuron, output_path)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f'    Saved: {output_path} ({size_mb:.1f} MB, {compile_time:.0f}s)')\n",
    "\n",
    "print('\\nCompiled models:')\n",
    "for f in sorted(os.listdir(MODEL_DIR)):\n",
    "    if f.endswith('.pt'):\n",
    "        size = os.path.getsize(os.path.join(MODEL_DIR, f)) / (1024 * 1024)\n",
    "        print(f'  {f}: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Write Triton Model Repository Files\n",
    "\n",
    "Write `config.pbtxt` and `model.py` into the model repository alongside the compiled `.pt` files.\n",
    "\n",
    "Key configuration for inf2.8xlarge:\n",
    "- **2 model instances** (one per NeuronCore)\n",
    "- **No LNC configuration** needed (LNC is a trn2-only feature)\n",
    "- **Output**: single float score (`score`)\n",
    "- **Sequence length**: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:32:27.863054Z",
     "iopub.status.busy": "2026-02-26T17:32:27.862019Z",
     "iopub.status.idle": "2026-02-26T17:32:27.870665Z",
     "shell.execute_reply": "2026-02-26T17:32:27.869884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config.pbtxt (instance_group count=2 for inf2.8xlarge)\n",
      "Wrote model.py (cores 0-1, 2 input tensors)\n",
      "\n",
      "Model repository:\n",
      "triton_repo/\n",
      "  bert_reranker/\n",
      "    config.pbtxt\n",
      "    1/\n",
      "      model.py\n",
      "      model_bs1.pt (676.4 MB)\n",
      "      model_bs16.pt (715.2 MB)\n",
      "      model_bs2.pt (679.1 MB)\n",
      "      model_bs4.pt (684.3 MB)\n",
      "      model_bs8.pt (695.8 MB)\n"
     ]
    }
   ],
   "source": [
    "REPO_DIR = os.path.expanduser('~/triton_repo/bert_reranker')\n",
    "\n",
    "# -- config.pbtxt (inf2.8xlarge: 2 instances) -----------------------------------\n",
    "config_pbtxt = r\"\"\"name: \"bert_reranker\"\n",
    "platform: \"python\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [1024]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [1024]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"score\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_CPU\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 5000\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"model_dir\"\n",
    "  value: { string_value: \"/models/bert_reranker/1\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"tokenizer_name\"\n",
    "  value: { string_value: \"Alibaba-NLP/gte-multilingual-reranker-base\" }\n",
    "}\n",
    "\n",
    "parameters: {\n",
    "  key: \"max_seq_length\"\n",
    "  value: { string_value: \"1024\" }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(REPO_DIR, 'config.pbtxt'), 'w') as f:\n",
    "    f.write(config_pbtxt)\n",
    "print('Wrote config.pbtxt (instance_group count=2 for inf2.8xlarge)')\n",
    "\n",
    "# -- model.py (Triton Python backend, inf2) ------------------------------------\n",
    "model_py = r'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Triton Python Backend for BERT Reranker on AWS Neuron (inf2)\n",
    "\n",
    "Loads compiled models for each batch size (1, 2, 4, 8, 16) and\n",
    "dispatches to the best-fit model to avoid padding waste.\n",
    "Each Triton instance is pinned to a separate NeuronCore (cores 0-1).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import triton_python_backend_utils as pb_utils\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    BATCH_SIZES = [1, 2, 4, 8, 16]\n",
    "\n",
    "    def initialize(self, args):\n",
    "        self.model_config = json.loads(args[\"model_config\"])\n",
    "        params = self.model_config.get(\"parameters\", {})\n",
    "        model_dir = params.get(\"model_dir\", {}).get(\n",
    "            \"string_value\", \"/models/bert_reranker/1\")\n",
    "        self.max_seq_length = int(\n",
    "            params.get(\"max_seq_length\", {}).get(\"string_value\", \"1024\"))\n",
    "\n",
    "        # Pin this instance to a specific NeuronCore (0-1 for inf2.8xlarge)\n",
    "        instance_name = args.get(\"model_instance_name\", \"bert_reranker_0_0\")\n",
    "        core_id = int(instance_name.split(\"_\")[-1])\n",
    "        os.environ[\"NEURON_RT_VISIBLE_CORES\"] = str(core_id)\n",
    "        print(f\"Instance {instance_name}: pinned to NeuronCore {core_id}\")\n",
    "\n",
    "        # Load all compiled models\n",
    "        self.models = {}\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            model_path = os.path.join(model_dir, f\"model_bs{bs}.pt\")\n",
    "            if os.path.exists(model_path):\n",
    "                self.models[bs] = torch.jit.load(model_path)\n",
    "                self.models[bs].eval()\n",
    "                print(f\"  Loaded model for batch_size={bs}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Model not found: {model_path}\")\n",
    "\n",
    "        if not self.models:\n",
    "            raise RuntimeError(\"No compiled models found!\")\n",
    "\n",
    "        # Warmup all models\n",
    "        print(\"Warming up models...\")\n",
    "        for bs, mdl in self.models.items():\n",
    "            dummy_ids = torch.zeros((bs, self.max_seq_length), dtype=torch.long)\n",
    "            dummy_mask = torch.ones((bs, self.max_seq_length), dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = mdl(dummy_ids, dummy_mask)\n",
    "            print(f\"  Warmed up BS={bs}\")\n",
    "        print(f\"Model initialization complete! \"\n",
    "              f\"Available batch sizes: {sorted(self.models.keys())}\")\n",
    "\n",
    "    def _get_best_model(self, actual_batch_size):\n",
    "        for bs in self.BATCH_SIZES:\n",
    "            if bs >= actual_batch_size and bs in self.models:\n",
    "                return bs, self.models[bs]\n",
    "        largest = max(self.models.keys())\n",
    "        return largest, self.models[largest]\n",
    "\n",
    "    def execute(self, requests):\n",
    "        responses = []\n",
    "        for request in requests:\n",
    "            input_ids = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"input_ids\").as_numpy()\n",
    "            ).long()\n",
    "            attention_mask = torch.from_numpy(\n",
    "                pb_utils.get_input_tensor_by_name(request, \"attention_mask\").as_numpy()\n",
    "            ).long()\n",
    "\n",
    "            actual_bs = input_ids.shape[0]\n",
    "            target_bs, model = self._get_best_model(actual_bs)\n",
    "\n",
    "            # Pad input tensors to compiled batch size if needed\n",
    "            if actual_bs < target_bs:\n",
    "                pad = target_bs - actual_bs\n",
    "                input_ids = torch.cat([input_ids,\n",
    "                    torch.zeros((pad, input_ids.shape[1]), dtype=torch.long)], dim=0)\n",
    "                attention_mask = torch.cat([attention_mask,\n",
    "                    torch.zeros((pad, attention_mask.shape[1]), dtype=torch.long)], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # Extract logit score: outputs is a tuple, first element is logits\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            else:\n",
    "                logits = outputs\n",
    "            scores = logits[:actual_bs, 0:1].cpu().numpy()\n",
    "\n",
    "            output_tensor = pb_utils.Tensor(\"score\",\n",
    "                                            scores.astype(np.float32))\n",
    "            responses.append(\n",
    "                pb_utils.InferenceResponse(output_tensors=[output_tensor]))\n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(\"Finalizing BERT Reranker model...\")\n",
    "        if hasattr(self, \"models\"):\n",
    "            for bs, mdl in self.models.items():\n",
    "                del mdl\n",
    "            self.models.clear()\n",
    "'''\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model.py'), 'w') as f:\n",
    "    f.write(model_py)\n",
    "print('Wrote model.py (cores 0-1, 2 input tensors)')\n",
    "\n",
    "# Verify\n",
    "print('\\nModel repository:')\n",
    "for root, dirs, files in os.walk(os.path.expanduser('~/triton_repo')):\n",
    "    level = root.replace(os.path.expanduser('~/triton_repo'), '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for f in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, f)) / (1024 * 1024)\n",
    "        label = f'{f} ({size:.1f} MB)' if size > 1 else f\n",
    "        print(f'{indent}  {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Build Triton + Neuron Docker Image\n",
    "\n",
    "Triton has no native Neuron backend, so we build Triton from source inside the AWS Neuron\n",
    "PyTorch inference base image. This takes **15-20 minutes** and produces a ~15.8 GB image.\n",
    "\n",
    "The cell skips the build if the image already exists.\n",
    "\n",
    "**Note**: We pin `transformers==4.48.0` inside the Docker image to avoid the performance\n",
    "regression with versions >= 4.54.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:32:27.873072Z",
     "iopub.status.busy": "2026-02-26T17:32:27.872740Z",
     "iopub.status.idle": "2026-02-26T17:45:34.048007Z",
     "shell.execute_reply": "2026-02-26T17:45:34.047103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Triton + Neuron Docker image (15-20 minutes)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build complete!\n",
      "IMAGE                                ID             DISK USAGE   CONTENT SIZE   EXTRA\n",
      "triton-neuron-bert-reranker:latest   3e169a9bfbe4       24.1GB         7.83GB        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DOCKER_IMAGE = 'triton-neuron-bert-reranker:latest'\n",
    "\n",
    "# Check if already built\n",
    "r = subprocess.run(['docker', 'images', '-q', DOCKER_IMAGE],\n",
    "                   capture_output=True, text=True)\n",
    "if r.stdout.strip():\n",
    "    print(f'Docker image already exists: {r.stdout.strip()}')\n",
    "    print('Delete with: docker rmi ' + DOCKER_IMAGE)\n",
    "else:\n",
    "    # Write Dockerfile\n",
    "    dockerfile = r\"\"\"ARG BASE_IMAGE=public.ecr.aws/neuron/pytorch-inference-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04\n",
    "FROM $BASE_IMAGE\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PJRT_DEVICE=NEURON \\\n",
    "    LD_LIBRARY_PATH=\"/opt/conda/lib:/opt/aws/neuron/lib:/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}\" \\\n",
    "    PATH=\"/opt/program:/opt/aws/neuron/bin:/opt/tritonserver/bin:${PATH}\"\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    wget gnupg2 build-essential git nginx pkg-config unzip \\\n",
    "    libssl-dev libcurl4-openssl-dev libgoogle-perftools-dev \\\n",
    "    libnuma-dev libarchive-dev libxml2-dev zlib1g-dev \\\n",
    "    autoconf automake libtool gperf scons patchelf \\\n",
    "    libre2-dev libb64-dev rapidjson-dev libboost-dev \\\n",
    "    cmake cmake-data \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel virtualenv build cmake==3.31.10\n",
    "RUN pip3 install transformers==4.48.0\n",
    "\n",
    "RUN git clone --depth=1 --branch=r26.01 https://github.com/triton-inference-server/server.git /server && \\\n",
    "    cd /server && \\\n",
    "    Python3_ROOT_DIR=/opt/conda \\\n",
    "    Python3_EXECUTABLE=/opt/conda/bin/python3 \\\n",
    "    Python3_INCLUDE_DIR=/opt/conda/include/python3.12 \\\n",
    "    Python3_LIBRARY=/opt/conda/lib/libpython3.12.so \\\n",
    "    ./build.py -v --no-container-build --build-dir=/server/build --backend=python \\\n",
    "    --enable-metrics --enable-logging --enable-stats --endpoint=\"http\" --endpoint=\"grpc\" && \\\n",
    "    cp -r /server/build/opt/* /opt/ && \\\n",
    "    cd / && rm -rf /server\n",
    "\n",
    "EXPOSE 8000 8001 8002\n",
    "CMD [\"tritonserver\", \"--model-repository=/models\"]\n",
    "\"\"\"\n",
    "    dockerfile_path = os.path.expanduser('~/Dockerfile.triton-neuron')\n",
    "    with open(dockerfile_path, 'w') as f:\n",
    "        f.write(dockerfile)\n",
    "\n",
    "    print('Building Triton + Neuron Docker image (15-20 minutes)...')\n",
    "    r = subprocess.run(\n",
    "        ['docker', 'build', '-f', dockerfile_path, '-t', DOCKER_IMAGE, '.'],\n",
    "        cwd=os.path.expanduser('~'),\n",
    "        capture_output=True, text=True, timeout=2400)\n",
    "    if r.returncode == 0:\n",
    "        print('Build complete!')\n",
    "    else:\n",
    "        print(f'Build FAILED (rc={r.returncode})')\n",
    "        print(r.stderr[-3000:])\n",
    "\n",
    "# Show image\n",
    "r = subprocess.run(['docker', 'images', DOCKER_IMAGE], capture_output=True, text=True)\n",
    "print(r.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Start Triton Server\n",
    "\n",
    "Launch the Docker container with the Neuron device mounted and the model repository bind-mounted.\n",
    "With 2 instances each loading 5 batch-size variants of a ~720 MB model, loading takes\n",
    "~1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:45:34.050561Z",
     "iopub.status.busy": "2026-02-26T17:45:34.050371Z",
     "iopub.status.idle": "2026-02-26T17:47:10.402601Z",
     "shell.execute_reply": "2026-02-26T17:47:10.401838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container started. Waiting for 2 model instances to load (~1-2 min)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server ready after ~86s\n",
      "Model instances initialized: 2/2\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "NUM_INSTANCES = 2  # inf2.8xlarge: 2 NeuronCores\n",
    "\n",
    "# Stop any previous run\n",
    "subprocess.run(['docker', 'rm', '-f', 'triton-bert-reranker'],\n",
    "               capture_output=True)\n",
    "time.sleep(3)\n",
    "\n",
    "cmd = [\n",
    "    'docker', 'run', '-d',\n",
    "    '--name', 'triton-bert-reranker',\n",
    "    '--device=/dev/neuron0',\n",
    "    '-v', os.path.expanduser('~/triton_repo') + ':/models:ro',\n",
    "    '-p', '8000:8000', '-p', '8001:8001', '-p', '8002:8002',\n",
    "    DOCKER_IMAGE,\n",
    "    'tritonserver', '--model-repository=/models', '--log-verbose=0',\n",
    "]\n",
    "r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "if r.returncode != 0:\n",
    "    print(f'Failed to start container: {r.stderr}')\n",
    "else:\n",
    "    print(f'Container started. Waiting for {NUM_INSTANCES} model instances to load (~1-2 min)...')\n",
    "\n",
    "# Poll for readiness (up to 6 minutes)\n",
    "for i in range(180):\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        resp = urllib.request.urlopen('http://localhost:8000/v2/health/ready', timeout=2)\n",
    "        if resp.status == 200:\n",
    "            elapsed = (i + 1) * 2\n",
    "            print(f'Server ready after ~{elapsed}s')\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('Timeout waiting for server!')\n",
    "    r = subprocess.run(['docker', 'logs', '--tail', '30', 'triton-bert-reranker'],\n",
    "                       capture_output=True, text=True)\n",
    "    print(r.stdout)\n",
    "\n",
    "# Verify instances loaded\n",
    "r = subprocess.run(['docker', 'logs', 'triton-bert-reranker'],\n",
    "                   capture_output=True, text=True)\n",
    "n = r.stdout.count('Model initialization complete')\n",
    "print(f'Model instances initialized: {n}/{NUM_INSTANCES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Run Benchmark\n",
    "\n",
    "Runs the full test matrix: 1 baseline + 15 dynamic-batching configurations (3 concurrency levels\n",
    "x 5 batch sizes), 10 seconds each. Total runtime ~3 minutes.\n",
    "\n",
    "Concurrency levels are 4/8/16, scaled for 2 NeuronCores (2x, 4x, 8x per core).\n",
    "\n",
    "**Note**: The reranker uses seq_len=1024, so per-inference\n",
    "latency is significantly higher than short-sequence models (~215ms vs ~0.37ms per sample).\n",
    "\n",
    "**Note**: One worker per test will print a harmless greenlet thread-switch error. This is a\n",
    "known cosmetic issue in `tritonclient` and does not affect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:47:10.404900Z",
     "iopub.status.busy": "2026-02-26T17:47:10.404732Z",
     "iopub.status.idle": "2026-02-26T17:50:03.968770Z",
     "shell.execute_reply": "2026-02-26T17:50:03.967847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT RERANKER TRITON BENCHMARK - Neuron (inf2.8xlarge)\n",
      "==========================================================================================\n",
      "\n",
      "Baseline: single request, no concurrency...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  P50: 18.35ms  Throughput: 54.4 inf/sec\n",
      "\n",
      "Concurrency=4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   43.18ms  P95=   43.85ms  P99=   44.21ms  Throughput=    69.3 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   50.52ms  P95=   51.73ms  P99=   52.08ms  Throughput=   143.0 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=   59.18ms  P95=  104.65ms  P99=  104.95ms  Throughput=   146.5 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  129.24ms  P95=  256.40ms  P99=  307.18ms  Throughput=   123.7 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  445.37ms  P95=  451.51ms  P99=  452.35ms  Throughput=   138.5 inf/sec\n",
      "\n",
      "Concurrency=8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=   51.20ms  P95=   53.61ms  P99=   56.10ms  Throughput=   144.5 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=   96.35ms  P95=   98.90ms  P99=   99.06ms  Throughput=   161.2 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  162.54ms  P95=  270.46ms  P99=  271.27ms  Throughput=   147.8 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  410.24ms  P95=  507.46ms  P99=  550.55ms  Throughput=   126.2 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50=  674.69ms  P95=  894.27ms  P99=  894.47ms  Throughput=   139.8 inf/sec\n",
      "\n",
      "Concurrency=16:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=1   P50=  100.88ms  P95=  138.16ms  P99=  140.64ms  Throughput=   156.6 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=2   P50=  151.76ms  P95=  273.53ms  P99=  278.05ms  Throughput=   160.1 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=4   P50=  389.66ms  P95=  434.89ms  P99=  438.58ms  Throughput=   146.4 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=8   P50=  990.37ms  P95= 1017.46ms  P99= 1048.21ms  Throughput=   126.1 inf/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BS=16  P50= 1598.51ms  P95= 1786.68ms  P99= 1789.90ms  Throughput=   140.2 inf/sec\n",
      "\n",
      "==========================================================================================\n",
      "Batch   Workers   Requests    P50 (ms)    P95 (ms)    P99 (ms)    Throughput     \n",
      "------------------------------------------------------------------------------------------\n",
      "1       1         545         18.35       18.54       18.60       54.4           \n",
      "1       4         696         43.18       43.85       44.21       69.3           \n",
      "2       4         717         50.52       51.73       52.08       143.0          \n",
      "4       4         370         59.18       104.65      104.95      146.5          \n",
      "8       4         158         129.24      256.40      307.18      123.7          \n",
      "16      4         90          445.37      451.51      452.35      138.5          \n",
      "1       8         1450        51.20       53.61       56.10       144.5          \n",
      "2       8         814         96.35       98.90       99.06       161.2          \n",
      "4       8         379         162.54      270.46      271.27      147.8          \n",
      "8       8         164         410.24      507.46      550.55      126.2          \n",
      "16      8         94          674.69      894.27      894.47      139.8          \n",
      "1       16        1583        100.88      138.16      140.64      156.6          \n",
      "2       16        813         151.76      273.53      278.05      160.1          \n",
      "4       16        381         389.66      434.89      438.58      146.4          \n",
      "8       16        172         990.37      1017.46     1048.21     126.1          \n",
      "16      16        102         1598.51     1786.68     1789.90     140.2          \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import tritonclient.http as httpclient\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "TRITON_URL = 'localhost:8000'\n",
    "MODEL_NAME = 'bert_reranker'\n",
    "DURATION = 10.0  # seconds per test\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-multilingual-reranker-base',\n",
    "                                    trust_remote_code=True)\n",
    "\n",
    "\n",
    "def _make_inputs(batch_size):\n",
    "    \"\"\"Create tokenized inputs for the reranker (query + passage pairs).\"\"\"\n",
    "    queries = ['What is machine learning?'] * batch_size\n",
    "    docs = ['Machine learning is a subset of artificial intelligence that '\n",
    "            'focuses on building systems that learn from data.'] * batch_size\n",
    "    tokens = tok(queries, docs, max_length=1024, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    ids_np = tokens['input_ids'].astype(np.int64)\n",
    "    mask_np = tokens['attention_mask'].astype(np.int64)\n",
    "    return ids_np, mask_np\n",
    "\n",
    "\n",
    "def _worker(client, batch_size, duration_s, latencies_q, stop_evt):\n",
    "    ids_np, mask_np = _make_inputs(batch_size)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    t_end = time.time() + duration_s\n",
    "    while time.time() < t_end and not stop_evt.is_set():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            client.infer(model_name=MODEL_NAME,\n",
    "                         inputs=[inp_ids, inp_mask])\n",
    "            latencies_q.put((time.time() - t0) * 1000)\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "\n",
    "def run_concurrent(batch_size, num_workers, duration_s=DURATION):\n",
    "    clients = [httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "               for _ in range(num_workers)]\n",
    "    # Warmup\n",
    "    ids_np, mask_np = _make_inputs(batch_size)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(5):\n",
    "        clients[0].infer(model_name=MODEL_NAME,\n",
    "                         inputs=[inp_ids, inp_mask])\n",
    "\n",
    "    q = Queue()\n",
    "    stop = threading.Event()\n",
    "    threads = []\n",
    "    t_start = time.time()\n",
    "    for i in range(num_workers):\n",
    "        t = threading.Thread(target=_worker,\n",
    "                             args=(clients[i], batch_size, duration_s, q, stop))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    total_time = time.time() - t_start\n",
    "\n",
    "    latencies = []\n",
    "    while not q.empty():\n",
    "        latencies.append(q.get())\n",
    "    if not latencies:\n",
    "        return None\n",
    "    return {\n",
    "        'batch_size': batch_size, 'num_workers': num_workers,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': (len(latencies) * batch_size) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_baseline(duration_s=DURATION):\n",
    "    client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "    ids_np, mask_np = _make_inputs(1)\n",
    "    inp_ids = httpclient.InferInput('input_ids', ids_np.shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask', mask_np.shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(ids_np)\n",
    "    inp_mask.set_data_from_numpy(mask_np)\n",
    "    for _ in range(10):\n",
    "        client.infer(model_name=MODEL_NAME,\n",
    "                     inputs=[inp_ids, inp_mask])\n",
    "    latencies = []\n",
    "    t_start = time.time()\n",
    "    while time.time() - t_start < duration_s:\n",
    "        t0 = time.time()\n",
    "        client.infer(model_name=MODEL_NAME,\n",
    "                     inputs=[inp_ids, inp_mask])\n",
    "        latencies.append((time.time() - t0) * 1000)\n",
    "    total_time = time.time() - t_start\n",
    "    return {\n",
    "        'batch_size': 1, 'num_workers': 1,\n",
    "        'total_requests': len(latencies),\n",
    "        'p50': np.percentile(latencies, 50),\n",
    "        'p95': np.percentile(latencies, 95),\n",
    "        'p99': np.percentile(latencies, 99),\n",
    "        'throughput': len(latencies) / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# -- Run all tests -------------------------------------------------------------\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "assert client.is_server_ready(), 'Triton server not ready!'\n",
    "\n",
    "print('BERT RERANKER TRITON BENCHMARK - Neuron (inf2.8xlarge)')\n",
    "print('=' * 90)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline\n",
    "print('\\nBaseline: single request, no concurrency...')\n",
    "r = run_baseline()\n",
    "results.append(r)\n",
    "print(f'  P50: {r[\"p50\"]:.2f}ms  Throughput: {r[\"throughput\"]:.1f} inf/sec')\n",
    "\n",
    "# Dynamic batching -- concurrency levels scaled for 2 cores\n",
    "for conc in [4, 8, 16]:\n",
    "    print(f'\\nConcurrency={conc}:')\n",
    "    for bs in [1, 2, 4, 8, 16]:\n",
    "        r = run_concurrent(bs, conc)\n",
    "        if r:\n",
    "            results.append(r)\n",
    "            print(f'  BS={bs:<3} P50={r[\"p50\"]:>8.2f}ms  '\n",
    "                  f'P95={r[\"p95\"]:>8.2f}ms  '\n",
    "                  f'P99={r[\"p99\"]:>8.2f}ms  '\n",
    "                  f'Throughput={r[\"throughput\"]:>8.1f} inf/sec')\n",
    "\n",
    "# Summary table\n",
    "print(f'\\n{\"=\" * 90}')\n",
    "print(f'{\"Batch\":<8}{\"Workers\":<10}{\"Requests\":<12}'\n",
    "      f'{\"P50 (ms)\":<12}{\"P95 (ms)\":<12}{\"P99 (ms)\":<12}{\"Throughput\":<15}')\n",
    "print('-' * 90)\n",
    "for r in results:\n",
    "    print(f'{r[\"batch_size\"]:<8}{r[\"num_workers\"]:<10}{r[\"total_requests\"]:<12}'\n",
    "          f'{r[\"p50\"]:<12.2f}{r[\"p95\"]:<12.2f}{r[\"p99\"]:<12.2f}'\n",
    "          f'{r[\"throughput\"]:<15.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Sample Reranking Results\n",
    "\n",
    "Send a real query with multiple passages to the Triton server and display the reranking scores.\n",
    "This demonstrates the model's ability to rank passages by relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:50:03.970923Z",
     "iopub.status.busy": "2026-02-26T17:50:03.970742Z",
     "iopub.status.idle": "2026-02-26T17:50:05.060862Z",
     "shell.execute_reply": "2026-02-26T17:50:05.059717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \"What are the benefits of renewable energy?\"\n",
      "Passages: 6\n",
      "\n",
      "Scoring...\n",
      "\n",
      "================================================================================\n",
      "Reranking Results for: \"What are the benefits of renewable energy?\"\n",
      "================================================================================\n",
      "\n",
      "  Rank 1 (score=+1.9195, prob=0.8721):\n",
      "    [0] Renewable energy sources like solar and wind power produce electricity without g...\n",
      "\n",
      "  Rank 2 (score=+1.5806, prob=0.8293):\n",
      "    [4] Wind energy creates more jobs per megawatt than coal or natural gas power plants...\n",
      "\n",
      "  Rank 3 (score=+0.7861, prob=0.6870):\n",
      "    [2] Solar panels have become 90% cheaper over the past decade, making renewable ener...\n",
      "\n",
      "  Rank 4 (score=-3.3127, prob=0.0351):\n",
      "    [1] The stock market experienced significant volatility in Q4 2025, with tech stocks...\n",
      "\n",
      "  Rank 5 (score=-3.3220, prob=0.0348):\n",
      "    [3] The history of ancient Rome spans over a thousand years, from its founding in 75...\n",
      "\n",
      "  Rank 6 (score=-3.3224, prob=0.0348):\n",
      "    [5] Python is a popular programming language known for its readability and extensive...\n",
      "\n",
      "================================================================================\n",
      "Top-3 passages are about renewable energy, as expected.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "client = httpclient.InferenceServerClient(url=TRITON_URL)\n",
    "tok = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-multilingual-reranker-base',\n",
    "                                    trust_remote_code=True)\n",
    "\n",
    "query = 'What are the benefits of renewable energy?'\n",
    "passages = [\n",
    "    'Renewable energy sources like solar and wind power produce electricity without '\n",
    "    'greenhouse gas emissions, reducing climate change impacts and improving air quality.',\n",
    "    'The stock market experienced significant volatility in Q4 2025, with tech stocks '\n",
    "    'leading the decline amid rising interest rates.',\n",
    "    'Solar panels have become 90% cheaper over the past decade, making renewable '\n",
    "    'energy cost-competitive with fossil fuels in most markets.',\n",
    "    'The history of ancient Rome spans over a thousand years, from its founding '\n",
    "    'in 753 BC to the fall of the Western Roman Empire in 476 AD.',\n",
    "    'Wind energy creates more jobs per megawatt than coal or natural gas power plants, '\n",
    "    'boosting local economies in rural areas.',\n",
    "    'Python is a popular programming language known for its readability and '\n",
    "    'extensive library ecosystem.',\n",
    "]\n",
    "\n",
    "print(f'Query: \"{query}\"')\n",
    "print(f'Passages: {len(passages)}')\n",
    "print('\\nScoring...')\n",
    "\n",
    "scores = []\n",
    "for i, passage in enumerate(passages):\n",
    "    tokens = tok([query], [passage], max_length=1024, padding='max_length',\n",
    "                 truncation=True, return_tensors='np')\n",
    "    inp_ids = httpclient.InferInput('input_ids',\n",
    "                                    tokens['input_ids'].shape, 'INT64')\n",
    "    inp_mask = httpclient.InferInput('attention_mask',\n",
    "                                     tokens['attention_mask'].shape, 'INT64')\n",
    "    inp_ids.set_data_from_numpy(tokens['input_ids'].astype(np.int64))\n",
    "    inp_mask.set_data_from_numpy(tokens['attention_mask'].astype(np.int64))\n",
    "    result = client.infer(model_name=MODEL_NAME,\n",
    "                          inputs=[inp_ids, inp_mask])\n",
    "    score = result.as_numpy('score')[0][0]\n",
    "    scores.append((score, i, passage))\n",
    "\n",
    "# Sort by score (highest = most relevant)\n",
    "scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(f'\\n{\"=\" * 80}')\n",
    "print(f'Reranking Results for: \"{query}\"')\n",
    "print(f'{\"=\" * 80}')\n",
    "for rank, (score, idx, passage) in enumerate(scores, 1):\n",
    "    sigmoid_score = 1 / (1 + np.exp(-score))\n",
    "    short = passage[:80] + '...' if len(passage) > 80 else passage\n",
    "    print(f'\\n  Rank {rank} (score={score:+.4f}, prob={sigmoid_score:.4f}):')\n",
    "    print(f'    [{idx}] {short}')\n",
    "\n",
    "print(f'\\n{\"=\" * 80}')\n",
    "print('Top-3 passages are about renewable energy, as expected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Cleanup\n",
    "\n",
    "Stop and remove the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T17:50:05.062617Z",
     "iopub.status.busy": "2026-02-26T17:50:05.062419Z",
     "iopub.status.idle": "2026-02-26T17:50:06.073162Z",
     "shell.execute_reply": "2026-02-26T17:50:06.071789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton server stopped and removed.\n"
     ]
    }
   ],
   "source": [
    "subprocess.run(['docker', 'rm', '-f', 'triton-bert-reranker'], capture_output=True)\n",
    "print('Triton server stopped and removed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (aws_neuronx_venv_pytorch_2_9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
