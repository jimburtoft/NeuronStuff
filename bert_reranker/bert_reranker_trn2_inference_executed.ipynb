{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58e7fcb",
   "metadata": {},
   "source": [
    "# BERT Reranker Inference on AWS Trainium2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970996f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to compile and run the [Alibaba-NLP/gte-multilingual-reranker-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base) BERT reranker model for accelerated inference on AWS Trainium2.\n",
    "\n",
    "This notebook was tested on a **trn2.3xlarge** instance using **LNC=1** (Logical NeuronCore=1),\n",
    "which provides 8 logical cores from 8 physical NeuronCores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ea77",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook requires a Trainium2 instance with the following Neuron SDK packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `transformers` (4.48 - 4.53)\n",
    "\n",
    "For a step-by-step guide on launching an instance, see [Getting Started with Inferentia or Trainium](https://repost.aws/articles/ARgiH8VXXuQ22iSUmwX7ffiQ/getting-started-with-inferentia-or-trainium).\n",
    "\n",
    "**Important**: The version of `transformers` affects compilation performance. Versions 4.48 through 4.53\n",
    "produce an optimal TorchScript graph for `torch_neuronx.trace()`. Other versions (including the 4.56\n",
    "shipped with the Neuron DLAMI) produce a ~20% slower compiled model. Pin the version before compiling:\n",
    "\n",
    "```bash\n",
    "pip install \"transformers>=4.48,<=4.53.3\"\n",
    "```\n",
    "\n",
    "**LNC Configuration**: This notebook uses LNC=1. Before running, set the runtime environment variable:\n",
    "\n",
    "```bash\n",
    "export NEURON_LOGICAL_NC_CONFIG=1\n",
    "```\n",
    "\n",
    "If you are using VS Code with Remote SSH on the AWS Neuron DLAMI, you can make the pre-installed environment available as a kernel:\n",
    "\n",
    "```bash\n",
    "ln -s /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13 ~/.venv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802916a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:44:29.509734Z",
     "iopub.status.busy": "2026-02-26T15:44:29.509588Z",
     "iopub.status.idle": "2026-02-26T15:44:34.046632Z",
     "shell.execute_reply": "2026-02-26T15:44:34.045364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "torch-neuronx version: 2.9.0.2.11.19912+e48cd891\n",
      "NEURON_LOGICAL_NC_CONFIG: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NEURON_LOGICAL_NC_CONFIG\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"torch-neuronx version: {torch_neuronx.__version__}\")\n",
    "print(f\"NEURON_LOGICAL_NC_CONFIG: {os.environ.get('NEURON_LOGICAL_NC_CONFIG')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0aefc",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the model parameters. For best performance on Trainium2, we recommend:\n",
    "\n",
    "- `--target trn2`: Targets the Trainium2 hardware\n",
    "- `--lnc 1`: Uses LNC=1, giving 8 logical cores on trn2.3xlarge (1 physical core per logical core)\n",
    "- `--auto-cast matmult`: Enables BF16 for matrix multiplications (~60% faster, negligible accuracy impact)\n",
    "- `--optlevel 2`: Standard compiler optimizations\n",
    "\n",
    "### Why LNC=1?\n",
    "\n",
    "Trainium2 supports two LNC modes:\n",
    "- **LNC=1**: Each physical NeuronCore is one logical core (8 logical cores on trn2.3xlarge)\n",
    "- **LNC=2**: Two physical NeuronCores form one logical core (4 logical cores on trn2.3xlarge)\n",
    "\n",
    "For this model (~306M parameters), LNC=1 is significantly more efficient. LNC=2 doubles the\n",
    "hardware per core without a proportional performance gain, resulting in lower per-physical-core\n",
    "efficiency.\n",
    "\n",
    "**DataParallel**: With LNC=1 on trn2.3xlarge, `torch_neuronx.DataParallel` loads the compiled model\n",
    "onto all 8 cores. Each core runs its own full batch=16 independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5ba453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:44:34.049039Z",
     "iopub.status.busy": "2026-02-26T15:44:34.048797Z",
     "iopub.status.idle": "2026-02-26T15:44:34.052237Z",
     "shell.execute_reply": "2026-02-26T15:44:34.051504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Alibaba-NLP/gte-multilingual-reranker-base\n",
      "Sequence Length: 1024\n",
      "Batch Size: 16\n",
      "Autocast: matmult\n",
      "Optlevel: 2\n",
      "LNC: 1\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_ID = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
    "SEQUENCE_LENGTH = 1024\n",
    "BATCH_SIZE = 16\n",
    "AUTOCAST = \"matmult\"  # Use 'matmult' for best performance, 'none' for full precision\n",
    "OPTLEVEL = 2\n",
    "LNC = 1\n",
    "\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Sequence Length: {SEQUENCE_LENGTH}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Autocast: {AUTOCAST}\")\n",
    "print(f\"Optlevel: {OPTLEVEL}\")\n",
    "print(f\"LNC: {LNC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac6215",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6e6c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:44:34.053896Z",
     "iopub.status.busy": "2026-02-26T15:44:34.053711Z",
     "iopub.status.idle": "2026-02-26T15:44:36.019852Z",
     "shell.execute_reply": "2026-02-26T15:44:36.019237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 305,959,681\n",
      "Hidden size: 768\n",
      "Layers: 12\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torchscript=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Model info\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9429798",
   "metadata": {},
   "source": [
    "## Prepare Example Input\n",
    "\n",
    "Create example inputs for tracing. The input shape during compilation must match inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366904e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:44:36.021926Z",
     "iopub.status.busy": "2026-02-26T15:44:36.021796Z",
     "iopub.status.idle": "2026-02-26T15:44:36.029773Z",
     "shell.execute_reply": "2026-02-26T15:44:36.029210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Create example input\n",
    "queries = [\"What is machine learning?\"] * BATCH_SIZE\n",
    "docs = [\"Machine learning is a subset of artificial intelligence.\"] * BATCH_SIZE\n",
    "\n",
    "encoded = tokenizer(\n",
    "    queries,\n",
    "    docs,\n",
    "    padding=\"max_length\",\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example_inputs = (encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
    "print(f\"Input shape: {example_inputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf94da",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "\n",
    "Compile the model using `torch_neuronx.trace()` with Trainium2-specific settings.\n",
    "\n",
    "The `--target trn2` and `--lnc 1` flags are required for Trainium2 compilation.\n",
    "A model compiled with a given LNC value must be run with the matching\n",
    "`NEURON_LOGICAL_NC_CONFIG` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3ea99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:44:36.030960Z",
     "iopub.status.busy": "2026-02-26T15:44:36.030844Z",
     "iopub.status.idle": "2026-02-26T15:51:57.212781Z",
     "shell.execute_reply": "2026-02-26T15:51:57.212056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiler args: ['--optlevel=2', '--target', 'trn2', '--lnc', '1', '--auto-cast', 'matmult']\n",
      "\n",
      "Compiling model (this takes ~8 minutes)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time: 434.2s\n",
      "Model size: 717.7 MB\n",
      "Saved to: bert_reranker_trn2_lnc1_seq1024_batch16.pt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Build compiler arguments\n",
    "compiler_args = [\n",
    "    f\"--optlevel={OPTLEVEL}\",\n",
    "    \"--target\", \"trn2\",\n",
    "    \"--lnc\", str(LNC),\n",
    "]\n",
    "if AUTOCAST == \"matmult\":\n",
    "    compiler_args.extend([\"--auto-cast\", \"matmult\"])\n",
    "\n",
    "print(f\"Compiler args: {compiler_args}\")\n",
    "\n",
    "# Compile\n",
    "print(\"\\nCompiling model (this takes ~8 minutes)...\")\n",
    "start = time.time()\n",
    "model_neuron = torch_neuronx.trace(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    compiler_args=compiler_args\n",
    ")\n",
    "compile_time = time.time() - start\n",
    "\n",
    "# Save\n",
    "output_path = f\"bert_reranker_trn2_lnc{LNC}_seq{SEQUENCE_LENGTH}_batch{BATCH_SIZE}.pt\"\n",
    "torch.jit.save(model_neuron, output_path)\n",
    "\n",
    "import os\n",
    "model_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Compilation time: {compile_time:.1f}s\")\n",
    "print(f\"Model size: {model_size_mb:.1f} MB\")\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53df6b3",
   "metadata": {},
   "source": [
    "## Verify Compilation\n",
    "\n",
    "Run a quick test to verify the compiled model produces valid output.\n",
    "The model is saved to disk so it can be loaded in future sessions without recompiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35db1555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:51:57.215275Z",
     "iopub.status.busy": "2026-02-26T15:51:57.215141Z",
     "iopub.status.idle": "2026-02-26T15:52:11.895731Z",
     "shell.execute_reply": "2026-02-26T15:52:11.895041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample score: 0.9479\n",
      "Model compiled and running successfully on Trainium2\n"
     ]
    }
   ],
   "source": [
    "# Test inference with the compiled model\n",
    "with torch.no_grad():\n",
    "    outputs = model_neuron(*example_inputs)\n",
    "\n",
    "# Get score\n",
    "if isinstance(outputs, tuple):\n",
    "    logits = outputs[0]\n",
    "else:\n",
    "    logits = outputs\n",
    "\n",
    "score = torch.sigmoid(logits[0]).item()\n",
    "print(f\"Sample score: {score:.4f}\")\n",
    "print(\"Model compiled and running successfully on Trainium2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3890465",
   "metadata": {},
   "source": [
    "## Benchmark Single-Core Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f367c914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:52:11.897930Z",
     "iopub.status.busy": "2026-02-26T15:52:11.897801Z",
     "iopub.status.idle": "2026-02-26T15:52:23.380236Z",
     "shell.execute_reply": "2026-02-26T15:52:23.379540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking (50 iterations)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single-Core Results ===\n",
      "Latency p50: 191.28 ms\n",
      "Latency p90: 191.34 ms\n",
      "Latency p99: 191.38 ms\n",
      "Throughput: 83.64 queries/second\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "WARMUP_ITERATIONS = 10\n",
    "BENCHMARK_ITERATIONS = 50\n",
    "\n",
    "print(\"Warming up...\")\n",
    "for _ in range(WARMUP_ITERATIONS):\n",
    "    with torch.no_grad():\n",
    "        _ = model_neuron(*example_inputs)\n",
    "\n",
    "print(f\"Benchmarking ({BENCHMARK_ITERATIONS} iterations)...\")\n",
    "latencies = []\n",
    "for _ in range(BENCHMARK_ITERATIONS):\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        _ = model_neuron(*example_inputs)\n",
    "    end = time.perf_counter()\n",
    "    latencies.append((end - start) * 1000)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p90 = np.percentile(latencies, 90)\n",
    "p99 = np.percentile(latencies, 99)\n",
    "throughput = (BATCH_SIZE * 1000) / np.mean(latencies)\n",
    "\n",
    "print(\"\\n=== Single-Core Results ===\")\n",
    "print(f\"Latency p50: {p50:.2f} ms\")\n",
    "print(f\"Latency p90: {p90:.2f} ms\")\n",
    "print(f\"Latency p99: {p99:.2f} ms\")\n",
    "print(f\"Throughput: {throughput:.2f} queries/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13af8e",
   "metadata": {},
   "source": [
    "## Benchmark with DataParallel\n",
    "\n",
    "`torch_neuronx.DataParallel` loads the same compiled model onto each available logical NeuronCore\n",
    "and runs them in parallel. Each core processes its own full batch independently, multiplying\n",
    "total throughput by the number of cores.\n",
    "\n",
    "On trn2.3xlarge with LNC=1 (8 logical cores), each core gets its own batch=16.\n",
    "\n",
    "**Important**: When consecutive device IDs are used (the default), `DataParallel` loads all cores\n",
    "via a single batch call and sets `num_workers=2`. This limits the thread pool to 2 concurrent\n",
    "dispatches, serializing execution across the remaining cores. Set `model_dp.num_workers` to at\n",
    "least the number of cores for full parallel throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725b394a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T15:52:23.382104Z",
     "iopub.status.busy": "2026-02-26T15:52:23.381975Z",
     "iopub.status.idle": "2026-02-26T15:53:34.958361Z",
     "shell.execute_reply": "2026-02-26T15:53:34.957777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron logical cores detected: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel num_workers set to: 8\n",
      "DataParallel input shape: torch.Size([128, 1024])  (16 per core x 8 cores)\n",
      "\n",
      "Warming up DataParallel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking DataParallel (50 iterations)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DataParallel Results (8 cores) ===\n",
      "Latency p50: 201.98 ms\n",
      "Latency p90: 202.22 ms\n",
      "Latency p99: 202.32 ms\n",
      "Throughput: 633.83 queries/second\n",
      "\n",
      "Speedup vs single-core: 7.58x\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_neuron_core_count():\n",
    "    \"\"\"Detect the number of Neuron logical cores using neuron-ls.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"neuron-ls\", \"--json-output\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            devices = json.loads(result.stdout)\n",
    "            return sum(d[\"nc_count\"] for d in devices)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 1\n",
    "\n",
    "num_cores = get_neuron_core_count()\n",
    "print(f\"Neuron logical cores detected: {num_cores}\")\n",
    "\n",
    "if num_cores > 1:\n",
    "    # Load the same compiled model onto all cores\n",
    "    model_dp = torch_neuronx.DataParallel(model_neuron)\n",
    "    \n",
    "    # Fix: default num_workers=2 serializes across cores; set to num_cores for full parallelism\n",
    "    model_dp.num_workers = num_cores\n",
    "    print(f\"DataParallel num_workers set to: {model_dp.num_workers}\")\n",
    "    \n",
    "    # Each core gets a full batch, so total input is BATCH_SIZE * num_cores\n",
    "    dp_total_batch = BATCH_SIZE * num_cores\n",
    "    dp_queries = [\"What is machine learning?\"] * dp_total_batch\n",
    "    dp_docs = [\"Machine learning is a subset of artificial intelligence.\"] * dp_total_batch\n",
    "    dp_encoded = tokenizer(\n",
    "        dp_queries, dp_docs,\n",
    "        padding=\"max_length\", max_length=SEQUENCE_LENGTH,\n",
    "        truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    dp_inputs = (dp_encoded[\"input_ids\"], dp_encoded[\"attention_mask\"])\n",
    "    print(f\"DataParallel input shape: {dp_inputs[0].shape}  ({BATCH_SIZE} per core x {num_cores} cores)\")\n",
    "    \n",
    "    print(\"\\nWarming up DataParallel...\")\n",
    "    for _ in range(WARMUP_ITERATIONS):\n",
    "        with torch.no_grad():\n",
    "            _ = model_dp(*dp_inputs)\n",
    "    \n",
    "    print(f\"Benchmarking DataParallel ({BENCHMARK_ITERATIONS} iterations)...\")\n",
    "    latencies_dp = []\n",
    "    for _ in range(BENCHMARK_ITERATIONS):\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = model_dp(*dp_inputs)\n",
    "        end = time.perf_counter()\n",
    "        latencies_dp.append((end - start) * 1000)\n",
    "    \n",
    "    latencies_dp = np.array(latencies_dp)\n",
    "    p50_dp = np.percentile(latencies_dp, 50)\n",
    "    p90_dp = np.percentile(latencies_dp, 90)\n",
    "    p99_dp = np.percentile(latencies_dp, 99)\n",
    "    throughput_dp = (dp_total_batch * 1000) / np.mean(latencies_dp)\n",
    "    \n",
    "    print(f\"\\n=== DataParallel Results ({num_cores} cores) ===\")\n",
    "    print(f\"Latency p50: {p50_dp:.2f} ms\")\n",
    "    print(f\"Latency p90: {p90_dp:.2f} ms\")\n",
    "    print(f\"Latency p99: {p99_dp:.2f} ms\")\n",
    "    print(f\"Throughput: {throughput_dp:.2f} queries/second\")\n",
    "    print(f\"\\nSpeedup vs single-core: {throughput_dp/throughput:.2f}x\")\n",
    "else:\n",
    "    print(\"Only 1 Neuron core available - skipping DataParallel benchmark.\")\n",
    "    print(\"Use an instance with multiple cores for DataParallel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff47604",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Measured on **trn2.3xlarge** with LNC=1 (torch-neuronx 2.9.0, neuronx-cc 2.22, transformers 4.53.3):\n",
    "\n",
    "| Configuration | Batch Size | Seq Length | Latency (p50) | Throughput |\n",
    "|--------------|------------|------------|---------------|------------|\n",
    "| Single-Core | 16 | 1024 | 191.2 ms | 83.7 qps |\n",
    "| DataParallel (8 cores) | 16 per core | 1024 | 202.2 ms | 633.4 qps |\n",
    "\n",
    "**Compiler flags explained**:\n",
    "- `--target trn2`: Generates code optimized for Trainium2 hardware\n",
    "- `--lnc 1`: Maps one physical NeuronCore to one logical core (8 cores on trn2.3xlarge)\n",
    "- `--auto-cast matmult`: Casts matrix multiplications to BF16, providing ~60% speedup with negligible accuracy impact (<0.0004)\n",
    "- `--optlevel 2`: Enables standard compiler optimizations (optlevel 3 provides no additional benefit for this model)\n",
    "\n",
    "**Key Findings**:\n",
    "- LNC=1 is significantly more efficient than LNC=2 for this model size (~306M parameters)\n",
    "- DataParallel provides 7.6x throughput scaling across 8 cores (requires setting `num_workers`)\n",
    "- Default `DataParallel` `num_workers=2` limits scaling to ~2x; set `num_workers >= num_cores`\n",
    "- Model size with autocast=matmult: ~718 MB\n",
    "- The `transformers` version used at compile time significantly affects performance (see Setup section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
